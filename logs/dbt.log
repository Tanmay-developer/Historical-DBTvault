

============================== 2023-04-24 09:25:28.661836 | 6c38388e-d78d-4ed9-9fbd-ddfa94f098c8 ==============================
[0m09:25:28.661836 [info ] [MainThread]: Running with dbt=1.4.6
[0m09:25:28.662942 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m09:25:28.663101 [debug] [MainThread]: Tracking: tracking
[0m09:25:28.670963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc614520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc614ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc614e50>]}
[0m09:25:28.770927 [debug] [MainThread]: Executing "git --help"
[0m09:25:28.775312 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:25:28.775714 [debug] [MainThread]: STDERR: "b''"
[0m09:25:28.779245 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m09:25:28.779670 [debug] [MainThread]: Using spark connection "debug"
[0m09:25:28.779855 [debug] [MainThread]: On debug: select 1 as id
[0m09:25:28.780043 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:25:28.866366 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m09:25:28.866759 [debug] [MainThread]: SQL status: OK in 0 seconds
[0m09:25:28.867889 [debug] [MainThread]: On debug: Close
[0m09:25:28.870275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc430b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc430dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcfc3bfd00>]}
[0m09:25:28.870774 [debug] [MainThread]: Flushing usage events
[0m09:25:28.921863 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.
[0m09:25:28.922229 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-04-24 09:31:09.235989 | d5b45f80-c447-48bc-ab5b-9fad31fb5482 ==============================
[0m09:31:09.235989 [info ] [MainThread]: Running with dbt=1.4.6
[0m09:31:09.237298 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m09:31:09.237463 [debug] [MainThread]: Tracking: tracking
[0m09:31:09.245422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6358f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6365070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6365310>]}
[0m09:31:09.261586 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m09:31:09.261994 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:31:09.262282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa63771f0>]}
[0m09:31:09.883655 [debug] [MainThread]: 1699: static parser successfully parsed example/test.sql
[0m09:31:09.894819 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m09:31:09.897383 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m09:31:09.899864 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m09:31:09.951191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa62a87c0>]}
[0m09:31:09.956723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa51cbf10>]}
[0m09:31:09.957049 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m09:31:09.957311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6358f70>]}
[0m09:31:09.958527 [info ] [MainThread]: 
[0m09:31:09.959840 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:31:09.960970 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:31:09.970291 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:31:09.970553 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:31:09.970740 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:31:10.066005 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:31:10.066481 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:31:10.069938 [debug] [ThreadPool]: On list_schemas: Close
[0m09:31:10.073650 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_public'
[0m09:31:10.078847 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:10.079078 [debug] [ThreadPool]: Using spark connection "list_None_public"
[0m09:31:10.079239 [debug] [ThreadPool]: On list_None_public: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_public"} */
show table extended in public like '*'
  
[0m09:31:10.079398 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:31:10.238380 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:31:10.238695 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:31:10.242727 [debug] [ThreadPool]: On list_None_public: ROLLBACK
[0m09:31:10.242976 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:31:10.243141 [debug] [ThreadPool]: On list_None_public: Close
[0m09:31:10.246778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa62a8d90>]}
[0m09:31:10.247196 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:10.247382 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:31:10.247789 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:31:10.248071 [info ] [MainThread]: 
[0m09:31:10.258367 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m09:31:10.258842 [info ] [Thread-1  ]: 1 of 4 START sql table model public.my_first_dbt_model ......................... [RUN]
[0m09:31:10.259704 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m09:31:10.260118 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m09:31:10.263474 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m09:31:10.264072 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-24 09:31:10.260336 => 2023-04-24 09:31:10.263971
[0m09:31:10.264420 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m09:31:10.337022 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m09:31:10.337561 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:10.337765 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m09:31:10.337919 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table public.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m09:31:10.338073 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:31:10.997710 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:31:10.998078 [debug] [Thread-1  ]: SQL status: OK in 1 seconds
[0m09:31:11.011577 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-24 09:31:10.264628 => 2023-04-24 09:31:11.011517
[0m09:31:11.011829 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m09:31:11.011990 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:31:11.012128 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m09:31:11.016971 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa5165670>]}
[0m09:31:11.017520 [info ] [Thread-1  ]: 1 of 4 OK created sql table model public.my_first_dbt_model .................... [[32mOK[0m in 0.76s]
[0m09:31:11.018770 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m09:31:11.019095 [debug] [Thread-1  ]: Began running node model.poc_demo.table3
[0m09:31:11.019364 [info ] [Thread-1  ]: 2 of 4 START sql view model public.new_table ................................... [RUN]
[0m09:31:11.019901 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m09:31:11.020109 [debug] [Thread-1  ]: Began compiling node model.poc_demo.table3
[0m09:31:11.022599 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m09:31:11.023125 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-24 09:31:11.020333 => 2023-04-24 09:31:11.023055
[0m09:31:11.023353 [debug] [Thread-1  ]: Began executing node model.poc_demo.table3
[0m09:31:11.037793 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m09:31:11.038258 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:11.038449 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.table3"
[0m09:31:11.038591 [debug] [Thread-1  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
create or replace view public.new_table
  
  as
    

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking) as 'rnk'
from public.Event_details
where booking_status = 'success'
)
select * from cte

[0m09:31:11.038736 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:31:11.121088 [debug] [Thread-1  ]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input \'\'rnk\'\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 9, pos 61)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */\ncreate or replace view public.new_table\n  \n  as\n    \n\nwith cte as(\nselect user_id,event_id,name,artist,city,date_of_booking,booking_status,\nrank() over(partition by artist order by date_of_booking) as \'rnk\'\n-------------------------------------------------------------^^^\nfrom public.Event_details\nwhere booking_status = \'success\'\n)\nselect * from cte\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input \'\'rnk\'\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 9, pos 61)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */\ncreate or replace view public.new_table\n  \n  as\n    \n\nwith cte as(\nselect user_id,event_id,name,artist,city,date_of_booking,booking_status,\nrank() over(partition by artist order by date_of_booking) as \'rnk\'\n-------------------------------------------------------------^^^\nfrom public.Event_details\nwhere booking_status = \'success\'\n)\nselect * from cte\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m09:31:11.121851 [debug] [Thread-1  ]: Spark adapter: Poll status: 5
[0m09:31:11.122546 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
create or replace view public.new_table
  
  as
    

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking) as 'rnk'
from public.Event_details
where booking_status = 'success'
)
select * from cte

[0m09:31:11.123742 [debug] [Thread-1  ]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
  create or replace view public.new_table
    
    as
      
  
  with cte as(
  select user_id,event_id,name,artist,city,date_of_booking,booking_status,
  rank() over(partition by artist order by date_of_booking) as 'rnk'
  -------------------------------------------------------------^^^
  from public.Event_details
  where booking_status = 'success'
  )
  select * from cte
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
  create or replace view public.new_table
    
    as
      
  
  with cte as(
  select user_id,event_id,name,artist,city,date_of_booking,booking_status,
  rank() over(partition by artist order by date_of_booking) as 'rnk'
  -------------------------------------------------------------^^^
  from public.Event_details
  where booking_status = 'success'
  )
  select * from cte
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m09:31:11.124822 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-24 09:31:11.023484 => 2023-04-24 09:31:11.124653
[0m09:31:11.125364 [debug] [Thread-1  ]: On model.poc_demo.table3: ROLLBACK
[0m09:31:11.125814 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:31:11.126286 [debug] [Thread-1  ]: On model.poc_demo.table3: Close
[0m09:31:11.143469 [debug] [Thread-1  ]: Runtime Error in model table3 (models/example/table3.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
    mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
    create or replace view public.new_table
      
      as
        
    
    with cte as(
    select user_id,event_id,name,artist,city,date_of_booking,booking_status,
    rank() over(partition by artist order by date_of_booking) as 'rnk'
    -------------------------------------------------------------^^^
    from public.Event_details
    where booking_status = 'success'
    )
    select * from cte
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:750)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
    create or replace view public.new_table
      
      as
        
    
    with cte as(
    select user_id,event_id,name,artist,city,date_of_booking,booking_status,
    rank() over(partition by artist order by date_of_booking) as 'rnk'
    -------------------------------------------------------------^^^
    from public.Event_details
    where booking_status = 'success'
    )
    select * from cte
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m09:31:11.144246 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa468e5b0>]}
[0m09:31:11.145045 [error] [Thread-1  ]: 2 of 4 ERROR creating sql view model public.new_table .......................... [[31mERROR[0m in 0.12s]
[0m09:31:11.145670 [debug] [Thread-1  ]: Finished running node model.poc_demo.table3
[0m09:31:11.146148 [debug] [Thread-1  ]: Began running node model.poc_demo.test
[0m09:31:11.146828 [info ] [Thread-1  ]: 3 of 4 START sql table model public.Event_details .............................. [RUN]
[0m09:31:11.147845 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m09:31:11.148260 [debug] [Thread-1  ]: Began compiling node model.poc_demo.test
[0m09:31:11.153146 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.test"
[0m09:31:11.153793 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (compile): 2023-04-24 09:31:11.148551 => 2023-04-24 09:31:11.153685
[0m09:31:11.154215 [debug] [Thread-1  ]: Began executing node model.poc_demo.test
[0m09:31:11.163251 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.test"
[0m09:31:11.163924 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:11.164251 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m09:31:11.164540 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table public.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m09:31:11.164795 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:31:12.345377 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:31:12.345810 [debug] [Thread-1  ]: SQL status: OK in 1 seconds
[0m09:31:12.348442 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (execute): 2023-04-24 09:31:11.154478 => 2023-04-24 09:31:12.348354
[0m09:31:12.348767 [debug] [Thread-1  ]: On model.poc_demo.test: ROLLBACK
[0m09:31:12.349005 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:31:12.349234 [debug] [Thread-1  ]: On model.poc_demo.test: Close
[0m09:31:12.352855 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa629a610>]}
[0m09:31:12.353535 [info ] [Thread-1  ]: 3 of 4 OK created sql table model public.Event_details ......................... [[32mOK[0m in 1.21s]
[0m09:31:12.354052 [debug] [Thread-1  ]: Finished running node model.poc_demo.test
[0m09:31:12.354439 [debug] [Thread-1  ]: Began running node model.poc_demo.my_second_dbt_model
[0m09:31:12.354933 [info ] [Thread-1  ]: 4 of 4 START sql view model public.my_second_dbt_model ......................... [RUN]
[0m09:31:12.355732 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m09:31:12.356043 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_second_dbt_model
[0m09:31:12.359792 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m09:31:12.360375 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (compile): 2023-04-24 09:31:12.356235 => 2023-04-24 09:31:12.360273
[0m09:31:12.360765 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_second_dbt_model
[0m09:31:12.365706 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m09:31:12.366527 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:12.366814 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m09:31:12.367038 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
create or replace view public.my_second_dbt_model
  
  as
    -- Use the `ref` function to select from other models

select *
from public.my_first_dbt_model
where id = 1

[0m09:31:12.367262 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:31:12.553745 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:31:12.554209 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:31:12.556667 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (execute): 2023-04-24 09:31:12.361043 => 2023-04-24 09:31:12.556588
[0m09:31:12.556985 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m09:31:12.557243 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:31:12.557492 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: Close
[0m09:31:12.560739 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5b45f80-c447-48bc-ab5b-9fad31fb5482', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa8694340>]}
[0m09:31:12.561282 [info ] [Thread-1  ]: 4 of 4 OK created sql view model public.my_second_dbt_model .................... [[32mOK[0m in 0.21s]
[0m09:31:12.561676 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m09:31:12.563009 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:31:12.563379 [debug] [MainThread]: On master: ROLLBACK
[0m09:31:12.563663 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:31:12.619667 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:31:12.620123 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:31:12.620415 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:31:12.620694 [debug] [MainThread]: On master: ROLLBACK
[0m09:31:12.620941 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:31:12.621212 [debug] [MainThread]: On master: Close
[0m09:31:12.623197 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:31:12.623540 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m09:31:12.623923 [info ] [MainThread]: 
[0m09:31:12.624417 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 2.66 seconds (2.66s).
[0m09:31:12.625141 [debug] [MainThread]: Command end result
[0m09:31:12.630666 [info ] [MainThread]: 
[0m09:31:12.630984 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:31:12.631214 [info ] [MainThread]: 
[0m09:31:12.631453 [error] [MainThread]: [33mRuntime Error in model table3 (models/example/table3.sql)[0m
[0m09:31:12.631670 [error] [MainThread]:   Database Error
[0m09:31:12.631869 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m09:31:12.632058 [error] [MainThread]:     mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
[0m09:31:12.632237 [error] [MainThread]:     
[0m09:31:12.632423 [error] [MainThread]:     == SQL ==
[0m09:31:12.632600 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
[0m09:31:12.632779 [error] [MainThread]:     create or replace view public.new_table
[0m09:31:12.632958 [error] [MainThread]:       
[0m09:31:12.633130 [error] [MainThread]:       as
[0m09:31:12.633301 [error] [MainThread]:         
[0m09:31:12.633469 [error] [MainThread]:     
[0m09:31:12.633640 [error] [MainThread]:     with cte as(
[0m09:31:12.633809 [error] [MainThread]:     select user_id,event_id,name,artist,city,date_of_booking,booking_status,
[0m09:31:12.633979 [error] [MainThread]:     rank() over(partition by artist order by date_of_booking) as 'rnk'
[0m09:31:12.634148 [error] [MainThread]:     -------------------------------------------------------------^^^
[0m09:31:12.634316 [error] [MainThread]:     from public.Event_details
[0m09:31:12.634486 [error] [MainThread]:     where booking_status = 'success'
[0m09:31:12.634655 [error] [MainThread]:     )
[0m09:31:12.634824 [error] [MainThread]:     select * from cte
[0m09:31:12.634994 [error] [MainThread]:     
[0m09:31:12.635230 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m09:31:12.635485 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m09:31:12.635814 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m09:31:12.636122 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m09:31:12.636392 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m09:31:12.636667 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m09:31:12.636990 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m09:31:12.637454 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m09:31:12.637756 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m09:31:12.638012 [error] [MainThread]:     	at java.security.AccessController.doPrivileged(Native Method)
[0m09:31:12.638263 [error] [MainThread]:     	at javax.security.auth.Subject.doAs(Subject.java:422)
[0m09:31:12.638523 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
[0m09:31:12.638774 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m09:31:12.639026 [error] [MainThread]:     	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[0m09:31:12.639278 [error] [MainThread]:     	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[0m09:31:12.639528 [error] [MainThread]:     	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[0m09:31:12.639777 [error] [MainThread]:     	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[0m09:31:12.640027 [error] [MainThread]:     	at java.lang.Thread.run(Thread.java:750)
[0m09:31:12.640279 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m09:31:12.640655 [error] [MainThread]:     mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 9, pos 61)
[0m09:31:12.640921 [error] [MainThread]:     
[0m09:31:12.641175 [error] [MainThread]:     == SQL ==
[0m09:31:12.641426 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
[0m09:31:12.641676 [error] [MainThread]:     create or replace view public.new_table
[0m09:31:12.641927 [error] [MainThread]:       
[0m09:31:12.642177 [error] [MainThread]:       as
[0m09:31:12.642443 [error] [MainThread]:         
[0m09:31:12.642774 [error] [MainThread]:     
[0m09:31:12.643103 [error] [MainThread]:     with cte as(
[0m09:31:12.643432 [error] [MainThread]:     select user_id,event_id,name,artist,city,date_of_booking,booking_status,
[0m09:31:12.643763 [error] [MainThread]:     rank() over(partition by artist order by date_of_booking) as 'rnk'
[0m09:31:12.644092 [error] [MainThread]:     -------------------------------------------------------------^^^
[0m09:31:12.644444 [error] [MainThread]:     from public.Event_details
[0m09:31:12.644779 [error] [MainThread]:     where booking_status = 'success'
[0m09:31:12.645110 [error] [MainThread]:     )
[0m09:31:12.645439 [error] [MainThread]:     select * from cte
[0m09:31:12.645768 [error] [MainThread]:     
[0m09:31:12.646098 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
[0m09:31:12.646429 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
[0m09:31:12.646759 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
[0m09:31:12.647091 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
[0m09:31:12.647424 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
[0m09:31:12.647756 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m09:31:12.648183 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
[0m09:31:12.648547 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[0m09:31:12.648879 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[0m09:31:12.649211 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m09:31:12.649542 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m09:31:12.649874 [error] [MainThread]:     	... 16 more
[0m09:31:12.650222 [error] [MainThread]:     
[0m09:31:12.650627 [info ] [MainThread]: 
[0m09:31:12.651065 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m09:31:12.651649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6358f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa6377280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fefa62a8ee0>]}
[0m09:31:12.652300 [debug] [MainThread]: Flushing usage events
[0m09:31:12.710160 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.


============================== 2023-04-24 09:34:09.570884 | 9fefe9ce-409e-413c-b416-6ee14d02e4bb ==============================
[0m09:34:09.570884 [info ] [MainThread]: Running with dbt=1.4.6
[0m09:34:09.572223 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m09:34:09.572407 [debug] [MainThread]: Tracking: tracking
[0m09:34:09.580196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce7daf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce7e6070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce7e6310>]}
[0m09:34:09.590194 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m09:34:09.600290 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m09:34:09.600688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce7e6580>]}
[0m09:34:10.203361 [debug] [MainThread]: 1699: static parser successfully parsed example/test.sql
[0m09:34:10.214113 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m09:34:10.216739 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m09:34:10.219258 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m09:34:10.289116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce764a60>]}
[0m09:34:10.294619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce77e6a0>]}
[0m09:34:10.294938 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m09:34:10.295203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd0a8fa60>]}
[0m09:34:10.296439 [info ] [MainThread]: 
[0m09:34:10.297655 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:34:10.298771 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:34:10.307601 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:34:10.307820 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:34:10.307997 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:34:10.396453 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:34:10.397016 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:34:10.401169 [debug] [ThreadPool]: On list_schemas: Close
[0m09:34:10.404673 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_public'
[0m09:34:10.410146 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:10.410379 [debug] [ThreadPool]: Using spark connection "list_None_public"
[0m09:34:10.410546 [debug] [ThreadPool]: On list_None_public: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_public"} */
show table extended in public like '*'
  
[0m09:34:10.410706 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:34:10.556284 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:34:10.556880 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:34:10.561663 [debug] [ThreadPool]: On list_None_public: ROLLBACK
[0m09:34:10.561992 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:34:10.562172 [debug] [ThreadPool]: On list_None_public: Close
[0m09:34:10.565911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd0a567c0>]}
[0m09:34:10.566296 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:10.566470 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:34:10.566861 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:34:10.567119 [info ] [MainThread]: 
[0m09:34:10.570190 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m09:34:10.570554 [info ] [Thread-1  ]: 1 of 4 START sql table model public.my_first_dbt_model ......................... [RUN]
[0m09:34:10.571074 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m09:34:10.571284 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m09:34:10.573773 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m09:34:10.574155 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-24 09:34:10.571427 => 2023-04-24 09:34:10.574084
[0m09:34:10.574377 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m09:34:10.604506 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m09:34:10.604874 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists public.my_first_dbt_model
[0m09:34:10.605116 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:34:10.806620 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:10.806965 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:10.851552 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m09:34:10.852104 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:10.852320 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m09:34:10.852504 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table public.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m09:34:11.167189 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:11.167759 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:11.200520 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-24 09:34:10.574510 => 2023-04-24 09:34:11.200431
[0m09:34:11.200919 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m09:34:11.201186 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:34:11.201415 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m09:34:11.205792 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd0a864c0>]}
[0m09:34:11.206537 [info ] [Thread-1  ]: 1 of 4 OK created sql table model public.my_first_dbt_model .................... [[32mOK[0m in 0.63s]
[0m09:34:11.208135 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m09:34:11.208578 [debug] [Thread-1  ]: Began running node model.poc_demo.table3
[0m09:34:11.209275 [info ] [Thread-1  ]: 2 of 4 START sql table model public.new_table .................................. [RUN]
[0m09:34:11.210109 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m09:34:11.210405 [debug] [Thread-1  ]: Began compiling node model.poc_demo.table3
[0m09:34:11.213619 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m09:34:11.214077 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-24 09:34:11.210585 => 2023-04-24 09:34:11.213999
[0m09:34:11.214335 [debug] [Thread-1  ]: Began executing node model.poc_demo.table3
[0m09:34:11.219777 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m09:34:11.220269 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:11.220503 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.table3"
[0m09:34:11.220673 [debug] [Thread-1  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table public.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking) as 'rnk'
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m09:34:11.220825 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:34:11.288415 [debug] [Thread-1  ]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input \'\'rnk\'\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 18, pos 61)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */\n\n  \n    \n        create table public.new_table\n      \n      \n      \n      \n      \n      \n      \n      as\n      \n\nwith cte as(\nselect user_id,event_id,name,artist,city,date_of_booking,booking_status,\nrank() over(partition by artist order by date_of_booking) as \'rnk\'\n-------------------------------------------------------------^^^\nfrom public.Event_details\nwhere booking_status = \'success\'\n)\nselect * from cte\n  \n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input \'\'rnk\'\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 18, pos 61)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */\n\n  \n    \n        create table public.new_table\n      \n      \n      \n      \n      \n      \n      \n      as\n      \n\nwith cte as(\nselect user_id,event_id,name,artist,city,date_of_booking,booking_status,\nrank() over(partition by artist order by date_of_booking) as \'rnk\'\n-------------------------------------------------------------^^^\nfrom public.Event_details\nwhere booking_status = \'success\'\n)\nselect * from cte\n  \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m09:34:11.288838 [debug] [Thread-1  ]: Spark adapter: Poll status: 5
[0m09:34:11.289184 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table public.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking) as 'rnk'
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m09:34:11.289838 [debug] [Thread-1  ]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
  
    
      
          create table public.new_table
        
        
        
        
        
        
        
        as
        
  
  with cte as(
  select user_id,event_id,name,artist,city,date_of_booking,booking_status,
  rank() over(partition by artist order by date_of_booking) as 'rnk'
  -------------------------------------------------------------^^^
  from public.Event_details
  where booking_status = 'success'
  )
  select * from cte
    
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
  
    
      
          create table public.new_table
        
        
        
        
        
        
        
        as
        
  
  with cte as(
  select user_id,event_id,name,artist,city,date_of_booking,booking_status,
  rank() over(partition by artist order by date_of_booking) as 'rnk'
  -------------------------------------------------------------^^^
  from public.Event_details
  where booking_status = 'success'
  )
  select * from cte
    
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m09:34:11.290324 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-24 09:34:11.214491 => 2023-04-24 09:34:11.290240
[0m09:34:11.290614 [debug] [Thread-1  ]: On model.poc_demo.table3: ROLLBACK
[0m09:34:11.290882 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:34:11.291124 [debug] [Thread-1  ]: On model.poc_demo.table3: Close
[0m09:34:11.297218 [debug] [Thread-1  ]: Runtime Error in model table3 (models/example/table3.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
    mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
    
      
        
            create table public.new_table
          
          
          
          
          
          
          
          as
          
    
    with cte as(
    select user_id,event_id,name,artist,city,date_of_booking,booking_status,
    rank() over(partition by artist order by date_of_booking) as 'rnk'
    -------------------------------------------------------------^^^
    from public.Event_details
    where booking_status = 'success'
    )
    select * from cte
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:750)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
    
      
        
            create table public.new_table
          
          
          
          
          
          
          
          as
          
    
    with cte as(
    select user_id,event_id,name,artist,city,date_of_booking,booking_status,
    rank() over(partition by artist order by date_of_booking) as 'rnk'
    -------------------------------------------------------------^^^
    from public.Event_details
    where booking_status = 'success'
    )
    select * from cte
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m09:34:11.297908 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dcc1aae20>]}
[0m09:34:11.298562 [error] [Thread-1  ]: 2 of 4 ERROR creating sql table model public.new_table ......................... [[31mERROR[0m in 0.09s]
[0m09:34:11.299182 [debug] [Thread-1  ]: Finished running node model.poc_demo.table3
[0m09:34:11.299640 [debug] [Thread-1  ]: Began running node model.poc_demo.test
[0m09:34:11.300249 [info ] [Thread-1  ]: 3 of 4 START sql table model public.Event_details .............................. [RUN]
[0m09:34:11.301258 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m09:34:11.301602 [debug] [Thread-1  ]: Began compiling node model.poc_demo.test
[0m09:34:11.305548 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.test"
[0m09:34:11.306111 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (compile): 2023-04-24 09:34:11.301820 => 2023-04-24 09:34:11.306013
[0m09:34:11.306446 [debug] [Thread-1  ]: Began executing node model.poc_demo.test
[0m09:34:11.312115 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m09:34:11.312417 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */
drop table if exists public.event_details
[0m09:34:11.312630 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:34:11.485420 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:11.485813 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:11.489815 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.test"
[0m09:34:11.490391 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:11.490636 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m09:34:11.490829 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table public.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m09:34:11.979866 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:11.980250 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:11.982485 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (execute): 2023-04-24 09:34:11.306654 => 2023-04-24 09:34:11.982427
[0m09:34:11.982736 [debug] [Thread-1  ]: On model.poc_demo.test: ROLLBACK
[0m09:34:11.982898 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:34:11.983039 [debug] [Thread-1  ]: On model.poc_demo.test: Close
[0m09:34:11.987017 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dcc1539a0>]}
[0m09:34:11.987740 [info ] [Thread-1  ]: 3 of 4 OK created sql table model public.Event_details ......................... [[32mOK[0m in 0.69s]
[0m09:34:11.988284 [debug] [Thread-1  ]: Finished running node model.poc_demo.test
[0m09:34:11.988706 [debug] [Thread-1  ]: Began running node model.poc_demo.my_second_dbt_model
[0m09:34:11.989094 [info ] [Thread-1  ]: 4 of 4 START sql table model public.my_second_dbt_model ........................ [RUN]
[0m09:34:11.989906 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m09:34:11.990305 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_second_dbt_model
[0m09:34:11.993997 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m09:34:11.994753 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (compile): 2023-04-24 09:34:11.990496 => 2023-04-24 09:34:11.994648
[0m09:34:11.995140 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_second_dbt_model
[0m09:34:12.000969 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m09:34:12.001340 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
drop view if exists public.my_second_dbt_model
[0m09:34:12.001620 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:34:12.203284 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:12.203740 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:12.207373 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m09:34:12.207813 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:12.207984 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m09:34:12.208128 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table public.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from public.my_first_dbt_model
where id = 1
  
[0m09:34:12.602574 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:34:12.602904 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:34:12.605066 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (execute): 2023-04-24 09:34:11.995371 => 2023-04-24 09:34:12.605011
[0m09:34:12.605303 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m09:34:12.605465 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:34:12.605611 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: Close
[0m09:34:12.611176 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9fefe9ce-409e-413c-b416-6ee14d02e4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dcc153d30>]}
[0m09:34:12.611712 [info ] [Thread-1  ]: 4 of 4 OK created sql table model public.my_second_dbt_model ................... [[32mOK[0m in 0.62s]
[0m09:34:12.612094 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m09:34:12.613367 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:34:12.613597 [debug] [MainThread]: On master: ROLLBACK
[0m09:34:12.613769 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:34:12.662221 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:34:12.662528 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:34:12.662695 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:34:12.662863 [debug] [MainThread]: On master: ROLLBACK
[0m09:34:12.663015 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:34:12.663163 [debug] [MainThread]: On master: Close
[0m09:34:12.664841 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:34:12.665058 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m09:34:12.665269 [info ] [MainThread]: 
[0m09:34:12.665533 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 2.37 seconds (2.37s).
[0m09:34:12.665945 [debug] [MainThread]: Command end result
[0m09:34:12.671579 [info ] [MainThread]: 
[0m09:34:12.671977 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:34:12.672230 [info ] [MainThread]: 
[0m09:34:12.672503 [error] [MainThread]: [33mRuntime Error in model table3 (models/example/table3.sql)[0m
[0m09:34:12.672729 [error] [MainThread]:   Database Error
[0m09:34:12.672944 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m09:34:12.673171 [error] [MainThread]:     mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
[0m09:34:12.673396 [error] [MainThread]:     
[0m09:34:12.673619 [error] [MainThread]:     == SQL ==
[0m09:34:12.673840 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
[0m09:34:12.674060 [error] [MainThread]:     
[0m09:34:12.674280 [error] [MainThread]:       
[0m09:34:12.674500 [error] [MainThread]:         
[0m09:34:12.674719 [error] [MainThread]:             create table public.new_table
[0m09:34:12.674940 [error] [MainThread]:           
[0m09:34:12.675159 [error] [MainThread]:           
[0m09:34:12.675379 [error] [MainThread]:           
[0m09:34:12.675598 [error] [MainThread]:           
[0m09:34:12.675824 [error] [MainThread]:           
[0m09:34:12.676045 [error] [MainThread]:           
[0m09:34:12.676262 [error] [MainThread]:           
[0m09:34:12.676508 [error] [MainThread]:           as
[0m09:34:12.676730 [error] [MainThread]:           
[0m09:34:12.676949 [error] [MainThread]:     
[0m09:34:12.677170 [error] [MainThread]:     with cte as(
[0m09:34:12.677390 [error] [MainThread]:     select user_id,event_id,name,artist,city,date_of_booking,booking_status,
[0m09:34:12.677610 [error] [MainThread]:     rank() over(partition by artist order by date_of_booking) as 'rnk'
[0m09:34:12.677829 [error] [MainThread]:     -------------------------------------------------------------^^^
[0m09:34:12.678050 [error] [MainThread]:     from public.Event_details
[0m09:34:12.678271 [error] [MainThread]:     where booking_status = 'success'
[0m09:34:12.678494 [error] [MainThread]:     )
[0m09:34:12.678717 [error] [MainThread]:     select * from cte
[0m09:34:12.678937 [error] [MainThread]:       
[0m09:34:12.679159 [error] [MainThread]:     
[0m09:34:12.679378 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m09:34:12.679598 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m09:34:12.679860 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m09:34:12.680233 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m09:34:12.680602 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m09:34:12.680959 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m09:34:12.681313 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m09:34:12.681667 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m09:34:12.682021 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m09:34:12.682375 [error] [MainThread]:     	at java.security.AccessController.doPrivileged(Native Method)
[0m09:34:12.682728 [error] [MainThread]:     	at javax.security.auth.Subject.doAs(Subject.java:422)
[0m09:34:12.683081 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
[0m09:34:12.683435 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m09:34:12.683794 [error] [MainThread]:     	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[0m09:34:12.684147 [error] [MainThread]:     	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[0m09:34:12.684601 [error] [MainThread]:     	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[0m09:34:12.684980 [error] [MainThread]:     	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[0m09:34:12.685340 [error] [MainThread]:     	at java.lang.Thread.run(Thread.java:750)
[0m09:34:12.685697 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m09:34:12.686050 [error] [MainThread]:     mismatched input ''rnk'' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 18, pos 61)
[0m09:34:12.686418 [error] [MainThread]:     
[0m09:34:12.686774 [error] [MainThread]:     == SQL ==
[0m09:34:12.687132 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
[0m09:34:12.687473 [error] [MainThread]:     
[0m09:34:12.687803 [error] [MainThread]:       
[0m09:34:12.688136 [error] [MainThread]:         
[0m09:34:12.688487 [error] [MainThread]:             create table public.new_table
[0m09:34:12.688821 [error] [MainThread]:           
[0m09:34:12.689151 [error] [MainThread]:           
[0m09:34:12.689484 [error] [MainThread]:           
[0m09:34:12.689816 [error] [MainThread]:           
[0m09:34:12.690146 [error] [MainThread]:           
[0m09:34:12.690479 [error] [MainThread]:           
[0m09:34:12.690825 [error] [MainThread]:           
[0m09:34:12.691177 [error] [MainThread]:           as
[0m09:34:12.691530 [error] [MainThread]:           
[0m09:34:12.691883 [error] [MainThread]:     
[0m09:34:12.692238 [error] [MainThread]:     with cte as(
[0m09:34:12.692609 [error] [MainThread]:     select user_id,event_id,name,artist,city,date_of_booking,booking_status,
[0m09:34:12.692965 [error] [MainThread]:     rank() over(partition by artist order by date_of_booking) as 'rnk'
[0m09:34:12.693320 [error] [MainThread]:     -------------------------------------------------------------^^^
[0m09:34:12.693675 [error] [MainThread]:     from public.Event_details
[0m09:34:12.694029 [error] [MainThread]:     where booking_status = 'success'
[0m09:34:12.694383 [error] [MainThread]:     )
[0m09:34:12.694737 [error] [MainThread]:     select * from cte
[0m09:34:12.695091 [error] [MainThread]:       
[0m09:34:12.695450 [error] [MainThread]:     
[0m09:34:12.695805 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
[0m09:34:12.696163 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
[0m09:34:12.696537 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
[0m09:34:12.696894 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
[0m09:34:12.697250 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
[0m09:34:12.697604 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m09:34:12.697960 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
[0m09:34:12.698312 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[0m09:34:12.698666 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[0m09:34:12.699021 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m09:34:12.699376 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m09:34:12.699732 [error] [MainThread]:     	... 16 more
[0m09:34:12.700085 [error] [MainThread]:     
[0m09:34:12.700517 [info ] [MainThread]: 
[0m09:34:12.700952 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m09:34:12.701479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dcca0df10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dcc1a6a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dce7daf70>]}
[0m09:34:12.701903 [debug] [MainThread]: Flushing usage events


============================== 2023-04-24 09:37:14.349845 | 8ec3e701-aa53-48db-a6e7-e1e9e19b2493 ==============================
[0m09:37:14.349845 [info ] [MainThread]: Running with dbt=1.4.6
[0m09:37:14.351137 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m09:37:14.351309 [debug] [MainThread]: Tracking: tracking
[0m09:37:14.360327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cb009f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cb019070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cb019340>]}
[0m09:37:14.370701 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m09:37:14.390878 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:37:14.391325 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models/example/table3.sql
[0m09:37:14.405008 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m09:37:14.422911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cd28e4c0>]}
[0m09:37:14.428400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cd363ac0>]}
[0m09:37:14.428742 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m09:37:14.429014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69caffa9d0>]}
[0m09:37:14.430268 [info ] [MainThread]: 
[0m09:37:14.431476 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:37:14.432667 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:37:14.441906 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:37:14.442131 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:37:14.442317 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:37:14.526781 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:37:14.527121 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:37:14.530576 [debug] [ThreadPool]: On list_schemas: Close
[0m09:37:14.534280 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_public'
[0m09:37:14.539563 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:14.539845 [debug] [ThreadPool]: Using spark connection "list_None_public"
[0m09:37:14.540012 [debug] [ThreadPool]: On list_None_public: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_public"} */
show table extended in public like '*'
  
[0m09:37:14.540171 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:37:14.698838 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:37:14.699224 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m09:37:14.703032 [debug] [ThreadPool]: On list_None_public: ROLLBACK
[0m09:37:14.703274 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:37:14.703450 [debug] [ThreadPool]: On list_None_public: Close
[0m09:37:14.706902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cafb3790>]}
[0m09:37:14.707293 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:14.707482 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:37:14.707897 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:37:14.708165 [info ] [MainThread]: 
[0m09:37:14.711073 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m09:37:14.711411 [info ] [Thread-1  ]: 1 of 4 START sql table model public.my_first_dbt_model ......................... [RUN]
[0m09:37:14.711924 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m09:37:14.712138 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m09:37:14.714614 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m09:37:14.714993 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-24 09:37:14.712285 => 2023-04-24 09:37:14.714920
[0m09:37:14.715208 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m09:37:14.731871 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m09:37:14.732143 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists public.my_first_dbt_model
[0m09:37:14.732312 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:37:14.889808 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:14.890247 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:14.942588 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m09:37:14.943161 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:14.943394 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m09:37:14.943574 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table public.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m09:37:15.190688 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:15.191025 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:15.207232 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-24 09:37:14.715340 => 2023-04-24 09:37:15.207172
[0m09:37:15.207498 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m09:37:15.207653 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:37:15.207790 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m09:37:15.211734 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cd29d2b0>]}
[0m09:37:15.212223 [info ] [Thread-1  ]: 1 of 4 OK created sql table model public.my_first_dbt_model .................... [[32mOK[0m in 0.50s]
[0m09:37:15.213485 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m09:37:15.213796 [debug] [Thread-1  ]: Began running node model.poc_demo.table3
[0m09:37:15.214159 [info ] [Thread-1  ]: 2 of 4 START sql table model public.new_table .................................. [RUN]
[0m09:37:15.214840 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m09:37:15.215049 [debug] [Thread-1  ]: Began compiling node model.poc_demo.table3
[0m09:37:15.217326 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m09:37:15.217701 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-24 09:37:15.215187 => 2023-04-24 09:37:15.217635
[0m09:37:15.217908 [debug] [Thread-1  ]: Began executing node model.poc_demo.table3
[0m09:37:15.221381 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m09:37:15.221706 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:15.221870 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.table3"
[0m09:37:15.222011 [debug] [Thread-1  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table public.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking)
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m09:37:15.222147 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:37:15.821881 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:15.822215 [debug] [Thread-1  ]: SQL status: OK in 1 seconds
[0m09:37:15.824131 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-24 09:37:15.218031 => 2023-04-24 09:37:15.824076
[0m09:37:15.824356 [debug] [Thread-1  ]: On model.poc_demo.table3: ROLLBACK
[0m09:37:15.824615 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:37:15.824760 [debug] [Thread-1  ]: On model.poc_demo.table3: Close
[0m09:37:15.827756 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c8221bb0>]}
[0m09:37:15.828273 [info ] [Thread-1  ]: 2 of 4 OK created sql table model public.new_table ............................. [[32mOK[0m in 0.61s]
[0m09:37:15.828669 [debug] [Thread-1  ]: Finished running node model.poc_demo.table3
[0m09:37:15.828935 [debug] [Thread-1  ]: Began running node model.poc_demo.test
[0m09:37:15.829275 [info ] [Thread-1  ]: 3 of 4 START sql table model public.Event_details .............................. [RUN]
[0m09:37:15.829815 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m09:37:15.830014 [debug] [Thread-1  ]: Began compiling node model.poc_demo.test
[0m09:37:15.833349 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.test"
[0m09:37:15.833776 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (compile): 2023-04-24 09:37:15.830138 => 2023-04-24 09:37:15.833695
[0m09:37:15.833993 [debug] [Thread-1  ]: Began executing node model.poc_demo.test
[0m09:37:15.837670 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m09:37:15.837881 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */
drop table if exists public.event_details
[0m09:37:15.838022 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:37:16.038333 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:16.038808 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:16.041643 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.test"
[0m09:37:16.042067 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:16.042238 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m09:37:16.042377 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table public.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m09:37:16.428411 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:16.428914 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:16.432316 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (execute): 2023-04-24 09:37:15.834124 => 2023-04-24 09:37:16.432231
[0m09:37:16.432671 [debug] [Thread-1  ]: On model.poc_demo.test: ROLLBACK
[0m09:37:16.432934 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:37:16.433183 [debug] [Thread-1  ]: On model.poc_demo.test: Close
[0m09:37:16.437843 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c81825e0>]}
[0m09:37:16.438376 [info ] [Thread-1  ]: 3 of 4 OK created sql table model public.Event_details ......................... [[32mOK[0m in 0.61s]
[0m09:37:16.438778 [debug] [Thread-1  ]: Finished running node model.poc_demo.test
[0m09:37:16.439047 [debug] [Thread-1  ]: Began running node model.poc_demo.my_second_dbt_model
[0m09:37:16.439381 [info ] [Thread-1  ]: 4 of 4 START sql table model public.my_second_dbt_model ........................ [RUN]
[0m09:37:16.439905 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m09:37:16.440096 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_second_dbt_model
[0m09:37:16.442477 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m09:37:16.442891 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (compile): 2023-04-24 09:37:16.440220 => 2023-04-24 09:37:16.442818
[0m09:37:16.443119 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_second_dbt_model
[0m09:37:16.446682 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m09:37:16.446903 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
drop table if exists public.my_second_dbt_model
[0m09:37:16.447076 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m09:37:16.620251 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:16.620609 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:16.623415 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m09:37:16.623832 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:16.623998 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m09:37:16.624137 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table public.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from public.my_first_dbt_model
where id = 1
  
[0m09:37:16.901272 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m09:37:16.901875 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m09:37:16.905594 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (execute): 2023-04-24 09:37:16.443259 => 2023-04-24 09:37:16.905507
[0m09:37:16.905933 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m09:37:16.906206 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m09:37:16.906455 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: Close
[0m09:37:16.909923 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ec3e701-aa53-48db-a6e7-e1e9e19b2493', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c817e9d0>]}
[0m09:37:16.910644 [info ] [Thread-1  ]: 4 of 4 OK created sql table model public.my_second_dbt_model ................... [[32mOK[0m in 0.47s]
[0m09:37:16.911221 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m09:37:16.912480 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:37:16.912722 [debug] [MainThread]: On master: ROLLBACK
[0m09:37:16.912899 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:37:16.948785 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:37:16.949091 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:37:16.949260 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:37:16.949427 [debug] [MainThread]: On master: ROLLBACK
[0m09:37:16.949581 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:37:16.949729 [debug] [MainThread]: On master: Close
[0m09:37:16.951321 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:37:16.951539 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m09:37:16.951747 [info ] [MainThread]: 
[0m09:37:16.952016 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 2.52 seconds (2.52s).
[0m09:37:16.952449 [debug] [MainThread]: Command end result
[0m09:37:16.958069 [info ] [MainThread]: 
[0m09:37:16.958432 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:37:16.958703 [info ] [MainThread]: 
[0m09:37:16.958954 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m09:37:16.959277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c821dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cafb35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69cb009fd0>]}
[0m09:37:16.959536 [debug] [MainThread]: Flushing usage events


============================== 2023-04-24 13:50:13.142713 | 9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971 ==============================
[0m13:50:13.142713 [info ] [MainThread]: Running with dbt=1.4.6
[0m13:50:13.150152 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:50:13.150354 [debug] [MainThread]: Tracking: tracking
[0m13:50:13.158491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf9923d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf9933280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf99333d0>]}
[0m13:50:13.168862 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m13:50:13.179525 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m13:50:13.179934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf99334c0>]}
[0m13:50:13.802506 [debug] [MainThread]: 1699: static parser successfully parsed example/test.sql
[0m13:50:13.813493 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m13:50:13.816236 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m13:50:13.818824 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m13:50:13.872305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf98a3220>]}
[0m13:50:13.878024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf9933100>]}
[0m13:50:13.878372 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:50:13.878650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bfbbe1af0>]}
[0m13:50:13.880283 [info ] [MainThread]: 
[0m13:50:13.881646 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:50:13.882863 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:50:13.892267 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:50:13.892555 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:50:13.892750 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:13.989449 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:50:13.989930 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m13:50:13.993632 [debug] [ThreadPool]: On list_schemas: Close
[0m13:50:13.998848 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test'
[0m13:50:14.004503 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:14.004811 [debug] [ThreadPool]: Using spark connection "list_None_test"
[0m13:50:14.005043 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:50:14.005239 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:50:14.106408 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:50:14.106925 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m13:50:14.123572 [debug] [ThreadPool]: On list_None_test: ROLLBACK
[0m13:50:14.124068 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:50:14.124364 [debug] [ThreadPool]: On list_None_test: Close
[0m13:50:14.129370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf98a3370>]}
[0m13:50:14.130118 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:14.130485 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:50:14.131374 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:50:14.131970 [info ] [MainThread]: 
[0m13:50:14.139749 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m13:50:14.140261 [info ] [Thread-1  ]: 1 of 4 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m13:50:14.140935 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m13:50:14.141212 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m13:50:14.143824 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m13:50:14.144263 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-24 13:50:14.141377 => 2023-04-24 13:50:14.144181
[0m13:50:14.144535 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m13:50:14.193424 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m13:50:14.193932 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:14.194130 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:50:14.194275 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:50:14.194419 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:50:14.581157 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:50:14.581761 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:50:14.606269 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-24 13:50:14.144679 => 2023-04-24 13:50:14.606164
[0m13:50:14.606730 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m13:50:14.607028 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:50:14.607268 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m13:50:14.610995 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf98a4370>]}
[0m13:50:14.611554 [info ] [Thread-1  ]: 1 of 4 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.47s]
[0m13:50:14.612808 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m13:50:14.613143 [debug] [Thread-1  ]: Began running node model.poc_demo.table3
[0m13:50:14.613589 [info ] [Thread-1  ]: 2 of 4 START sql table model test.new_table .................................... [RUN]
[0m13:50:14.614302 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m13:50:14.614535 [debug] [Thread-1  ]: Began compiling node model.poc_demo.table3
[0m13:50:14.616867 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m13:50:14.617330 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-24 13:50:14.614670 => 2023-04-24 13:50:14.617240
[0m13:50:14.617579 [debug] [Thread-1  ]: Began executing node model.poc_demo.table3
[0m13:50:14.621182 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m13:50:14.621712 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:14.621902 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.table3"
[0m13:50:14.622066 [debug] [Thread-1  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table test.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking)
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m13:50:14.622230 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:50:15.118333 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:50:15.118722 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:50:15.120888 [debug] [Thread-1  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-24 13:50:14.617718 => 2023-04-24 13:50:15.120810
[0m13:50:15.121177 [debug] [Thread-1  ]: On model.poc_demo.table3: ROLLBACK
[0m13:50:15.121361 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:50:15.121508 [debug] [Thread-1  ]: On model.poc_demo.table3: Close
[0m13:50:15.125128 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf831c670>]}
[0m13:50:15.125860 [info ] [Thread-1  ]: 2 of 4 OK created sql table model test.new_table ............................... [[32mOK[0m in 0.51s]
[0m13:50:15.126427 [debug] [Thread-1  ]: Finished running node model.poc_demo.table3
[0m13:50:15.126861 [debug] [Thread-1  ]: Began running node model.poc_demo.test
[0m13:50:15.127392 [info ] [Thread-1  ]: 3 of 4 START sql table model test.Event_details ................................ [RUN]
[0m13:50:15.128269 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m13:50:15.128589 [debug] [Thread-1  ]: Began compiling node model.poc_demo.test
[0m13:50:15.132495 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.test"
[0m13:50:15.133056 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (compile): 2023-04-24 13:50:15.128798 => 2023-04-24 13:50:15.132953
[0m13:50:15.133388 [debug] [Thread-1  ]: Began executing node model.poc_demo.test
[0m13:50:15.140733 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.test"
[0m13:50:15.141414 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:15.141723 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m13:50:15.141974 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table test.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m13:50:15.142236 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:50:15.635124 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:50:15.635569 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:50:15.637463 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (execute): 2023-04-24 13:50:15.133609 => 2023-04-24 13:50:15.637411
[0m13:50:15.637682 [debug] [Thread-1  ]: On model.poc_demo.test: ROLLBACK
[0m13:50:15.637886 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:50:15.638051 [debug] [Thread-1  ]: On model.poc_demo.test: Close
[0m13:50:15.640951 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf830d490>]}
[0m13:50:15.641460 [info ] [Thread-1  ]: 3 of 4 OK created sql table model test.Event_details ........................... [[32mOK[0m in 0.51s]
[0m13:50:15.641880 [debug] [Thread-1  ]: Finished running node model.poc_demo.test
[0m13:50:15.642194 [debug] [Thread-1  ]: Began running node model.poc_demo.my_second_dbt_model
[0m13:50:15.642522 [info ] [Thread-1  ]: 4 of 4 START sql table model test.my_second_dbt_model .......................... [RUN]
[0m13:50:15.643173 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m13:50:15.643424 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_second_dbt_model
[0m13:50:15.646337 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m13:50:15.647005 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (compile): 2023-04-24 13:50:15.643570 => 2023-04-24 13:50:15.646890
[0m13:50:15.647493 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_second_dbt_model
[0m13:50:15.653972 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m13:50:15.654761 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:15.655117 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m13:50:15.655416 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table test.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1
  
[0m13:50:15.655703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:50:16.051390 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:50:16.051845 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:50:16.054528 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_second_dbt_model (execute): 2023-04-24 13:50:15.647792 => 2023-04-24 13:50:16.054449
[0m13:50:16.054873 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m13:50:16.055119 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:50:16.055338 [debug] [Thread-1  ]: On model.poc_demo.my_second_dbt_model: Close
[0m13:50:16.058026 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f8c2e84-83a1-4c6d-8e45-b55ed0fb2971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf8331b80>]}
[0m13:50:16.058717 [info ] [Thread-1  ]: 4 of 4 OK created sql table model test.my_second_dbt_model ..................... [[32mOK[0m in 0.41s]
[0m13:50:16.059243 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m13:50:16.060425 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:50:16.060672 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:16.060852 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:50:16.119713 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:50:16.120114 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:16.120348 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:50:16.120599 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:16.120831 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:50:16.121047 [debug] [MainThread]: On master: Close
[0m13:50:16.123088 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:50:16.123508 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m13:50:16.124004 [info ] [MainThread]: 
[0m13:50:16.124663 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 2.24 seconds (2.24s).
[0m13:50:16.125733 [debug] [MainThread]: Command end result
[0m13:50:16.140384 [info ] [MainThread]: 
[0m13:50:16.141273 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:50:16.141798 [info ] [MainThread]: 
[0m13:50:16.142289 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m13:50:16.142910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf9923d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf98cb7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bfbc4f400>]}
[0m13:50:16.143472 [debug] [MainThread]: Flushing usage events


============================== 2023-04-24 13:54:20.573940 | 800e99d5-1aa6-460c-9b83-4f4e99d086d5 ==============================
[0m13:54:20.573940 [info ] [MainThread]: Running with dbt=1.4.6
[0m13:54:20.575251 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:54:20.575436 [debug] [MainThread]: Tracking: tracking
[0m13:54:20.583247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac0f1f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac101070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac101340>]}
[0m13:54:20.593627 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m13:54:20.604049 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m13:54:20.604409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac101430>]}
[0m13:54:21.218427 [debug] [MainThread]: 1699: static parser successfully parsed example/test.sql
[0m13:54:21.229683 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m13:54:21.232394 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m13:54:21.234935 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m13:54:21.289012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac07bcd0>]}
[0m13:54:21.295110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac09b580>]}
[0m13:54:21.295490 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:54:21.295772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaae3ada60>]}
[0m13:54:21.297086 [info ] [MainThread]: 
[0m13:54:21.298357 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:54:21.299539 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:54:21.309120 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:54:21.309402 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:54:21.309597 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:54:21.417113 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:54:21.417655 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m13:54:21.421981 [debug] [ThreadPool]: On list_schemas: Close
[0m13:54:21.426155 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test'
[0m13:54:21.434714 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:21.435137 [debug] [ThreadPool]: Using spark connection "list_None_test"
[0m13:54:21.435422 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:54:21.435703 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:54:21.549287 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:54:21.549636 [debug] [ThreadPool]: SQL status: OK in 0 seconds
[0m13:54:21.552946 [debug] [ThreadPool]: On list_None_test: ROLLBACK
[0m13:54:21.553226 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:54:21.553399 [debug] [ThreadPool]: On list_None_test: Close
[0m13:54:21.556540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaae421400>]}
[0m13:54:21.556901 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:21.557093 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:54:21.557499 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m13:54:21.557758 [info ] [MainThread]: 
[0m13:54:21.561205 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m13:54:21.561565 [info ] [Thread-1  ]: 1 of 4 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m13:54:21.562106 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m13:54:21.562320 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m13:54:21.562606 [debug] [Thread-2  ]: Began running node model.poc_demo.table3
[0m13:54:21.565104 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m13:54:21.565435 [info ] [Thread-2  ]: 2 of 4 START sql table model test.new_table .................................... [RUN]
[0m13:54:21.566277 [debug] [Thread-2  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m13:54:21.566503 [debug] [Thread-2  ]: Began compiling node model.poc_demo.table3
[0m13:54:21.568638 [debug] [Thread-2  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m13:54:21.569065 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-24 13:54:21.562879 => 2023-04-24 13:54:21.568926
[0m13:54:21.569466 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m13:54:21.569717 [debug] [Thread-2  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-24 13:54:21.566632 => 2023-04-24 13:54:21.569665
[0m13:54:21.581651 [debug] [Thread-2  ]: Began executing node model.poc_demo.table3
[0m13:54:21.614809 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:54:21.618325 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.table3"
[0m13:54:21.618743 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m13:54:21.619126 [debug] [Thread-2  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */
drop table if exists test.new_table
[0m13:54:21.619457 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:54:21.619707 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m13:54:22.116280 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:22.116607 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:54:22.167893 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m13:54:22.168551 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:22.168779 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:54:22.168955 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:54:22.170139 [debug] [Thread-2  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:22.170403 [debug] [Thread-2  ]: SQL status: OK in 1 seconds
[0m13:54:22.218155 [debug] [Thread-2  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m13:54:22.219063 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:22.219421 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.table3"
[0m13:54:22.219694 [debug] [Thread-2  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table test.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking)
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m13:54:22.499278 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:22.499625 [debug] [Thread-1  ]: SQL status: OK in 0 seconds
[0m13:54:22.521902 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-24 13:54:21.569983 => 2023-04-24 13:54:22.521799
[0m13:54:22.522365 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m13:54:22.522699 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:54:22.522983 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m13:54:22.526163 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac23f130>]}
[0m13:54:22.526886 [info ] [Thread-1  ]: 1 of 4 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.96s]
[0m13:54:22.528491 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m13:54:22.528972 [debug] [Thread-1  ]: Began running node model.poc_demo.test
[0m13:54:22.529484 [info ] [Thread-1  ]: 3 of 4 START sql table model test.Event_details ................................ [RUN]
[0m13:54:22.530204 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m13:54:22.530417 [debug] [Thread-1  ]: Began compiling node model.poc_demo.test
[0m13:54:22.532971 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.test"
[0m13:54:22.533429 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (compile): 2023-04-24 13:54:22.530548 => 2023-04-24 13:54:22.533347
[0m13:54:22.533669 [debug] [Thread-1  ]: Began executing node model.poc_demo.test
[0m13:54:22.537319 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m13:54:22.537536 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */
drop table if exists test.event_details
[0m13:54:22.537686 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:54:22.776069 [debug] [Thread-2  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:22.776612 [debug] [Thread-2  ]: SQL status: OK in 1 seconds
[0m13:54:22.779902 [debug] [Thread-2  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-24 13:54:21.587109 => 2023-04-24 13:54:22.779828
[0m13:54:22.780223 [debug] [Thread-2  ]: On model.poc_demo.table3: ROLLBACK
[0m13:54:22.780473 [debug] [Thread-2  ]: Spark adapter: NotImplemented: rollback
[0m13:54:22.780710 [debug] [Thread-2  ]: On model.poc_demo.table3: Close
[0m13:54:22.783746 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaa339970>]}
[0m13:54:22.784466 [info ] [Thread-2  ]: 2 of 4 OK created sql table model test.new_table ............................... [[32mOK[0m in 1.22s]
[0m13:54:22.785063 [debug] [Thread-2  ]: Finished running node model.poc_demo.table3
[0m13:54:22.785507 [debug] [Thread-2  ]: Began running node model.poc_demo.my_second_dbt_model
[0m13:54:22.786039 [info ] [Thread-2  ]: 4 of 4 START sql table model test.my_second_dbt_model .......................... [RUN]
[0m13:54:22.786906 [debug] [Thread-2  ]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m13:54:22.787228 [debug] [Thread-2  ]: Began compiling node model.poc_demo.my_second_dbt_model
[0m13:54:22.791115 [debug] [Thread-2  ]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m13:54:22.791710 [debug] [Thread-2  ]: Timing info for model.poc_demo.my_second_dbt_model (compile): 2023-04-24 13:54:22.787438 => 2023-04-24 13:54:22.791610
[0m13:54:22.792083 [debug] [Thread-2  ]: Began executing node model.poc_demo.my_second_dbt_model
[0m13:54:22.798354 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m13:54:22.798768 [debug] [Thread-2  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
drop table if exists test.my_second_dbt_model
[0m13:54:22.799027 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m13:54:23.097700 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:23.098048 [debug] [Thread-1  ]: SQL status: OK in 1 seconds
[0m13:54:23.100965 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.test"
[0m13:54:23.101464 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:23.101661 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.test"
[0m13:54:23.101808 [debug] [Thread-1  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table test.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m13:54:23.441274 [debug] [Thread-2  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:23.441603 [debug] [Thread-2  ]: SQL status: OK in 1 seconds
[0m13:54:23.444897 [debug] [Thread-2  ]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m13:54:23.445361 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:23.445559 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m13:54:23.445703 [debug] [Thread-2  ]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table test.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1
  
[0m13:54:23.931542 [debug] [Thread-2  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:23.931872 [debug] [Thread-2  ]: SQL status: OK in 0 seconds
[0m13:54:23.933988 [debug] [Thread-2  ]: Timing info for model.poc_demo.my_second_dbt_model (execute): 2023-04-24 13:54:22.792353 => 2023-04-24 13:54:23.933936
[0m13:54:23.934214 [debug] [Thread-2  ]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m13:54:23.934380 [debug] [Thread-2  ]: Spark adapter: NotImplemented: rollback
[0m13:54:23.934524 [debug] [Thread-2  ]: On model.poc_demo.my_second_dbt_model: Close
[0m13:54:23.937445 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaae3a2250>]}
[0m13:54:23.937969 [info ] [Thread-2  ]: 4 of 4 OK created sql table model test.my_second_dbt_model ..................... [[32mOK[0m in 1.15s]
[0m13:54:23.938378 [debug] [Thread-2  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m13:54:23.991136 [debug] [Thread-1  ]: Spark adapter: Poll status: 2, query complete
[0m13:54:23.991480 [debug] [Thread-1  ]: SQL status: OK in 1 seconds
[0m13:54:23.993594 [debug] [Thread-1  ]: Timing info for model.poc_demo.test (execute): 2023-04-24 13:54:22.533837 => 2023-04-24 13:54:23.993540
[0m13:54:23.993819 [debug] [Thread-1  ]: On model.poc_demo.test: ROLLBACK
[0m13:54:23.993973 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:54:23.994105 [debug] [Thread-1  ]: On model.poc_demo.test: Close
[0m13:54:23.996731 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e99d5-1aa6-460c-9b83-4f4e99d086d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac23f610>]}
[0m13:54:23.997204 [info ] [Thread-1  ]: 3 of 4 OK created sql table model test.Event_details ........................... [[32mOK[0m in 1.47s]
[0m13:54:23.997559 [debug] [Thread-1  ]: Finished running node model.poc_demo.test
[0m13:54:23.998664 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:54:23.998893 [debug] [MainThread]: On master: ROLLBACK
[0m13:54:23.999078 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:54:24.056290 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:54:24.056741 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:24.057043 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:54:24.057325 [debug] [MainThread]: On master: ROLLBACK
[0m13:54:24.057571 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:54:24.057778 [debug] [MainThread]: On master: Close
[0m13:54:24.059457 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:54:24.059685 [debug] [MainThread]: Connection 'model.poc_demo.test' was properly closed.
[0m13:54:24.059870 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m13:54:24.060143 [info ] [MainThread]: 
[0m13:54:24.060444 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 2.76 seconds (2.76s).
[0m13:54:24.060918 [debug] [MainThread]: Command end result
[0m13:54:24.069848 [info ] [MainThread]: 
[0m13:54:24.070373 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:54:24.070749 [info ] [MainThread]: 
[0m13:54:24.071099 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m13:54:24.071602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac1a3eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac1a3370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaac1a3130>]}
[0m13:54:24.072141 [debug] [MainThread]: Flushing usage events


============================== 2023-04-25 12:45:14.744099 | c721bef5-addb-46f8-a545-c030317bf5e9 ==============================
[0m12:45:14.744099 [info ] [MainThread]: Running with dbt=1.4.6
[0m12:45:14.745337 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m12:45:14.745510 [debug] [MainThread]: Tracking: tracking
[0m12:45:14.753730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b9ba98610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b9ba98b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b9ba98dc0>]}
[0m12:45:14.862468 [debug] [MainThread]: Executing "git --help"
[0m12:45:14.866673 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:45:14.867084 [debug] [MainThread]: STDERR: "b''"
[0m12:45:14.870643 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:45:14.871066 [debug] [MainThread]: Using spark connection "debug"
[0m12:45:14.871253 [debug] [MainThread]: On debug: select 1 as id
[0m12:45:14.871440 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:45:25.294994 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:30.414672 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:35.534596 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:40.654133 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:45.774197 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:50.897331 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:56.013903 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:45:56.907097 [debug] [MainThread]: On debug: Close


============================== 2023-04-25 12:46:25.263918 | 256f6cd7-4a0e-489f-a4e5-7065d1b2c86c ==============================
[0m12:46:25.263918 [info ] [MainThread]: Running with dbt=1.4.6
[0m12:46:25.270978 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m12:46:25.271150 [debug] [MainThread]: Tracking: tracking
[0m12:46:25.279158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa7f790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa7fbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa7fe20>]}
[0m12:46:25.375245 [debug] [MainThread]: Executing "git --help"
[0m12:46:25.378769 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:46:25.379189 [debug] [MainThread]: STDERR: "b''"
[0m12:46:25.382358 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:46:25.382782 [debug] [MainThread]: Using spark connection "debug"
[0m12:46:25.382972 [debug] [MainThread]: On debug: select 1 as id
[0m12:46:25.383156 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:46:29.267563 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:46:29.274788 [debug] [MainThread]: SQL status: OK in 4 seconds
[0m12:46:29.277627 [debug] [MainThread]: On debug: Close
[0m12:46:29.703434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bf839ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bf825100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bf86d8b0>]}
[0m12:46:29.704556 [debug] [MainThread]: Flushing usage events
[0m12:46:31.344342 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-04-25 12:55:36.450577 | d699e228-f6f4-4a1b-85fd-418975e1201b ==============================
[0m12:55:36.450577 [info ] [MainThread]: Running with dbt=1.4.6
[0m12:55:36.451989 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:55:36.452180 [debug] [MainThread]: Tracking: tracking
[0m12:55:36.460717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ddde2f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dddf1070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dddf1310>]}
[0m12:55:36.470433 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m12:55:36.480126 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:55:36.480501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dddf1400>]}
[0m12:55:37.096621 [debug] [MainThread]: 1699: static parser successfully parsed example/test.sql
[0m12:55:37.107749 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m12:55:37.110280 [debug] [MainThread]: 1699: static parser successfully parsed example/table3.sql
[0m12:55:37.112770 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m12:55:37.163442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ddd6d070>]}
[0m12:55:37.168721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ddd88640>]}
[0m12:55:37.169023 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:55:37.169291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34e009ba60>]}
[0m12:55:37.170535 [info ] [MainThread]: 
[0m12:55:37.171751 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:55:37.172890 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:55:37.182266 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:55:37.182497 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:55:37.182675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:55:38.190836 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:55:38.191570 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m12:55:38.577071 [debug] [ThreadPool]: On list_schemas: Close
[0m12:55:38.732654 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m12:55:38.745771 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:38.746314 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m12:55:38.746719 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m12:55:38.747113 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:55:39.667590 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:55:39.668335 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m12:55:39.786716 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m12:55:39.787445 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:55:39.787912 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m12:55:40.074874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34e0114400>]}
[0m12:55:40.075820 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:40.076391 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:55:40.077571 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m12:55:40.078283 [info ] [MainThread]: 
[0m12:55:40.091920 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m12:55:40.092688 [info ] [Thread-1  ]: 1 of 4 START sql table model test_poc.my_first_dbt_model ....................... [RUN]
[0m12:55:40.093942 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m12:55:40.094430 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m12:55:40.095080 [debug] [Thread-2  ]: Began running node model.poc_demo.table3
[0m12:55:40.101001 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m12:55:40.101804 [info ] [Thread-2  ]: 2 of 4 START sql table model test_poc.new_table ................................ [RUN]
[0m12:55:40.103243 [debug] [Thread-2  ]: Acquiring new spark connection 'model.poc_demo.table3'
[0m12:55:40.103760 [debug] [Thread-2  ]: Began compiling node model.poc_demo.table3
[0m12:55:40.109377 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-25 12:55:40.095542 => 2023-04-25 12:55:40.109201
[0m12:55:40.108883 [debug] [Thread-2  ]: Writing injected SQL for node "model.poc_demo.table3"
[0m12:55:40.109942 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m12:55:40.138621 [debug] [Thread-2  ]: Timing info for model.poc_demo.table3 (compile): 2023-04-25 12:55:40.104081 => 2023-04-25 12:55:40.138399
[0m12:55:40.139193 [debug] [Thread-2  ]: Began executing node model.poc_demo.table3
[0m12:55:40.247905 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m12:55:40.249573 [debug] [Thread-2  ]: Writing runtime sql for node "model.poc_demo.table3"
[0m12:55:40.250237 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:40.250529 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:40.250812 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.table3"
[0m12:55:40.251080 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m12:55:40.251383 [debug] [Thread-2  ]: On model.poc_demo.table3: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table test_poc.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking)
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m12:55:40.251677 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:55:40.251946 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m12:55:40.252211 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m12:55:41.470969 [debug] [Thread-2  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.table3"} */

  
    
        create table test_poc.new_table
      
      
      
      
      
      
      
      as
      

with cte as(
select user_id,event_id,name,artist,city,date_of_booking,booking_status,
rank() over(partition by artist order by date_of_booking)
from public.Event_details
where booking_status = 'success'
)
select * from cte
  
[0m12:55:41.472413 [debug] [Thread-2  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 19:5 Table not found 'Event_details':29:28", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:Line 19:5 Table not found 'Event_details':41:13", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2160', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2088', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2242', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42S02', errorCode=10001, errorMessage="Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 19:5 Table not found 'Event_details'"), operationHandle=None)
[0m12:55:41.473302 [debug] [Thread-2  ]: Timing info for model.poc_demo.table3 (execute): 2023-04-25 12:55:40.144668 => 2023-04-25 12:55:41.473152
[0m12:55:41.473797 [debug] [Thread-2  ]: On model.poc_demo.table3: ROLLBACK
[0m12:55:41.474200 [debug] [Thread-2  ]: Spark adapter: NotImplemented: rollback
[0m12:55:41.474568 [debug] [Thread-2  ]: On model.poc_demo.table3: Close
[0m12:55:41.498493 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:55:41.499464 [debug] [Thread-1  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':29:28", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':39:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2360', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196', '*java.lang.RuntimeException:Cannot create staging directory \'hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db/.hive-staging_hive_2023-04-25_12-55-41_081_7171656042692985120-4\': Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:41:2', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:481', 'org.apache.hadoop.hive.ql.Context:getExtTmpPathRelTo:Context.java:780', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2356', '*org.apache.hadoop.security.AccessControlException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:56:15', 'sun.reflect.NativeConstructorAccessorImpl:newInstance0:NativeConstructorAccessorImpl.java:-2', 'sun.reflect.NativeConstructorAccessorImpl:newInstance:NativeConstructorAccessorImpl.java:62', 'sun.reflect.DelegatingConstructorAccessorImpl:newInstance:DelegatingConstructorAccessorImpl.java:45', 'java.lang.reflect.Constructor:newInstance:Constructor.java:423', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2518', 'org.apache.hadoop.hdfs.DFSClient:mkdirs:DFSClient.java:2492', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1485', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1482', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirsInternal:DistributedFileSystem.java:1499', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirs:DistributedFileSystem.java:1474', 'org.apache.hadoop.fs.FileSystem:mkdirs:FileSystem.java:2449', 'org.apache.hadoop.hive.common.FileUtils:mkdir:FileUtils.java:587', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:473', '*org.apache.hadoop.ipc.RemoteException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:66:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1630', 'org.apache.hadoop.ipc.Client:call:Client.java:1575', 'org.apache.hadoop.ipc.Client:call:Client.java:1472', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy29:mkdirs::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:mkdirs:ClientNamenodeProtocolTranslatorPB.java:676', 'sun.reflect.GeneratedMethodAccessor25:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy30:mkdirs::-1', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2516'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'"), operationHandle=None)
[0m12:55:41.500301 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-25 12:55:40.110452 => 2023-04-25 12:55:41.500159
[0m12:55:41.500774 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m12:55:41.501205 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m12:55:41.501597 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m12:55:41.548375 [debug] [Thread-2  ]: Runtime Error in model table3 (models/example/table3.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 19:5 Table not found 'Event_details'
[0m12:55:41.549261 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dc24fe50>]}
[0m12:55:41.550193 [error] [Thread-2  ]: 2 of 4 ERROR creating sql table model test_poc.new_table ....................... [[31mERROR[0m in 1.45s]
[0m12:55:41.552970 [debug] [Thread-2  ]: Finished running node model.poc_demo.table3
[0m12:55:41.553633 [debug] [Thread-2  ]: Began running node model.poc_demo.test
[0m12:55:41.554223 [info ] [Thread-2  ]: 3 of 4 START sql table model test_poc.Event_details ............................ [RUN]
[0m12:55:41.555781 [debug] [Thread-2  ]: Acquiring new spark connection 'model.poc_demo.test'
[0m12:55:41.556300 [debug] [Thread-2  ]: Began compiling node model.poc_demo.test
[0m12:55:41.561598 [debug] [Thread-2  ]: Writing injected SQL for node "model.poc_demo.test"
[0m12:55:41.562317 [debug] [Thread-2  ]: Timing info for model.poc_demo.test (compile): 2023-04-25 12:55:41.556617 => 2023-04-25 12:55:41.562192
[0m12:55:41.562824 [debug] [Thread-2  ]: Began executing node model.poc_demo.test
[0m12:55:41.572613 [debug] [Thread-2  ]: Writing runtime sql for node "model.poc_demo.test"
[0m12:55:41.574453 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m12:55:41.575040 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:41.575907 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34e0094580>]}
[0m12:55:41.576402 [debug] [Thread-2  ]: Using spark connection "model.poc_demo.test"
[0m12:55:41.577382 [error] [Thread-1  ]: 1 of 4 ERROR creating sql table model test_poc.my_first_dbt_model .............. [[31mERROR[0m in 1.48s]
[0m12:55:41.578280 [debug] [Thread-2  ]: On model.poc_demo.test: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table test_poc.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m12:55:41.579181 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m12:55:41.579712 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:55:41.581404 [debug] [Thread-1  ]: Began running node model.poc_demo.my_second_dbt_model
[0m12:55:41.581948 [info ] [Thread-1  ]: 4 of 4 SKIP relation test_poc.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m12:55:41.582661 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m12:55:43.593650 [debug] [Thread-2  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.test"} */

  
    
        create table test_poc.Event_details
      
      
      
      
      
      
      
      as
      

with source as (
select bd.id as user_id,bd.event_id,e.name,e.artist,e.city,bd.date_of_booking,bd.booking_status
from public.booking_details bd JOIN public.event e
on bd.event_id = e.id
)

select * from source order by user_id
  
[0m12:55:43.594356 [debug] [Thread-2  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 18:5 Table not found 'booking_details':29:28", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:Line 18:5 Table not found 'booking_details':41:13", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2160', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2088', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2242', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42S02', errorCode=10001, errorMessage="Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 18:5 Table not found 'booking_details'"), operationHandle=None)
[0m12:55:43.598861 [debug] [Thread-2  ]: Timing info for model.poc_demo.test (execute): 2023-04-25 12:55:41.563134 => 2023-04-25 12:55:43.598691
[0m12:55:43.599331 [debug] [Thread-2  ]: On model.poc_demo.test: ROLLBACK
[0m12:55:43.599697 [debug] [Thread-2  ]: Spark adapter: NotImplemented: rollback
[0m12:55:43.600040 [debug] [Thread-2  ]: On model.poc_demo.test: Close
[0m12:55:43.662411 [debug] [Thread-2  ]: Runtime Error in model test (models/example/test.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 18:5 Table not found 'booking_details'
[0m12:55:43.663351 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd699e228-f6f4-4a1b-85fd-418975e1201b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dc2190d0>]}
[0m12:55:43.664316 [error] [Thread-2  ]: 3 of 4 ERROR creating sql table model test_poc.Event_details ................... [[31mERROR[0m in 2.11s]
[0m12:55:43.665121 [debug] [Thread-2  ]: Finished running node model.poc_demo.test
[0m12:55:43.667871 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:55:43.668494 [debug] [MainThread]: On master: ROLLBACK
[0m12:55:43.669000 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:55:44.779027 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:55:44.779827 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:55:44.780415 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:55:44.780958 [debug] [MainThread]: On master: ROLLBACK
[0m12:55:44.781464 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:55:44.781951 [debug] [MainThread]: On master: Close
[0m12:55:45.032729 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:55:45.033369 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m12:55:45.033829 [debug] [MainThread]: Connection 'model.poc_demo.test' was properly closed.
[0m12:55:45.034421 [info ] [MainThread]: 
[0m12:55:45.035156 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 7.86 seconds (7.86s).
[0m12:55:45.036151 [debug] [MainThread]: Command end result
[0m12:55:45.048494 [info ] [MainThread]: 
[0m12:55:45.049290 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m12:55:45.049862 [info ] [MainThread]: 
[0m12:55:45.050448 [error] [MainThread]: [33mRuntime Error in model table3 (models/example/table3.sql)[0m
[0m12:55:45.051012 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 19:5 Table not found 'Event_details'
[0m12:55:45.051547 [info ] [MainThread]: 
[0m12:55:45.052194 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
[0m12:55:45.052701 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m12:55:45.053182 [info ] [MainThread]: 
[0m12:55:45.053676 [error] [MainThread]: [33mRuntime Error in model test (models/example/test.sql)[0m
[0m12:55:45.054146 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 18:5 Table not found 'booking_details'
[0m12:55:45.054636 [info ] [MainThread]: 
[0m12:55:45.055179 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=1 TOTAL=4
[0m12:55:45.055824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34dc103c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ddde2f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34e00fad90>]}
[0m12:55:45.056369 [debug] [MainThread]: Flushing usage events


============================== 2023-04-25 12:57:26.647589 | 4e9709c0-2f72-4a83-9962-60fba63a0c2c ==============================
[0m12:57:26.647589 [info ] [MainThread]: Running with dbt=1.4.6
[0m12:57:26.648696 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m12:57:26.648893 [debug] [MainThread]: Tracking: tracking
[0m12:57:26.656746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa6e26a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa6e2be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa6e2d00>]}
[0m12:57:26.751970 [debug] [MainThread]: Executing "git --help"
[0m12:57:26.756002 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:57:26.756425 [debug] [MainThread]: STDERR: "b''"
[0m12:57:26.759719 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:57:26.760149 [debug] [MainThread]: Using spark connection "debug"
[0m12:57:26.760369 [debug] [MainThread]: On debug: select 1 as id
[0m12:57:26.760583 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:57:28.512889 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:57:28.513692 [debug] [MainThread]: SQL status: OK in 2 seconds
[0m12:57:28.516169 [debug] [MainThread]: On debug: Close
[0m12:57:28.641516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa4fbf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa498100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96fa48d070>]}
[0m12:57:28.642611 [debug] [MainThread]: Flushing usage events
[0m12:57:30.198685 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-04-25 12:57:35.596657 | d6592148-65bf-47cb-924d-c3720c0c725b ==============================
[0m12:57:35.596657 [info ] [MainThread]: Running with dbt=1.4.6
[0m12:57:35.604026 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:57:35.604252 [debug] [MainThread]: Tracking: tracking
[0m12:57:35.612324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26226d4070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26226d42e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26226d4430>]}
[0m12:57:35.622969 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m12:57:35.642869 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 0 files changed.
[0m12:57:35.643183 [debug] [MainThread]: Partial parsing: deleted file: poc_demo://models/example/table3.sql
[0m12:57:35.643345 [debug] [MainThread]: Partial parsing: deleted file: poc_demo://models/example/test.sql
[0m12:57:35.652403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd6592148-65bf-47cb-924d-c3720c0c725b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26249ce0d0>]}
[0m12:57:35.657755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd6592148-65bf-47cb-924d-c3720c0c725b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2624a0a1f0>]}
[0m12:57:35.658082 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m12:57:35.658346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6592148-65bf-47cb-924d-c3720c0c725b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26249cdfd0>]}
[0m12:57:35.659509 [info ] [MainThread]: 
[0m12:57:35.660733 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:57:35.661722 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:57:35.671512 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:57:35.671803 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:57:35.672060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:57:36.547419 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:57:36.548116 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m12:57:36.726779 [debug] [ThreadPool]: On list_schemas: Close
[0m12:57:36.849155 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m12:57:36.862119 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:57:36.862719 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m12:57:36.863134 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m12:57:36.863508 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:57:37.862642 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:57:37.863376 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m12:57:37.981654 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m12:57:37.982456 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:57:37.983144 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m12:57:38.103213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6592148-65bf-47cb-924d-c3720c0c725b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26226b7b20>]}
[0m12:57:38.103722 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:57:38.103970 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:57:38.104501 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m12:57:38.104899 [info ] [MainThread]: 
[0m12:57:38.107726 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m12:57:38.108094 [info ] [Thread-1  ]: 1 of 2 START sql table model test_poc.my_first_dbt_model ....................... [RUN]
[0m12:57:38.108647 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m12:57:38.108877 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m12:57:38.111670 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m12:57:38.112120 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-25 12:57:38.109026 => 2023-04-25 12:57:38.112031
[0m12:57:38.112366 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m12:57:38.160571 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m12:57:38.161048 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m12:57:38.161255 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m12:57:38.161421 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:57:38.161580 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m12:57:39.418910 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m12:57:39.420341 [debug] [Thread-1  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.GeneratedMethodAccessor45:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2360', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196', '*java.lang.RuntimeException:Cannot create staging directory \'hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db/.hive-staging_hive_2023-04-25_12-57-39_013_4984393358746807170-4\': Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:40:2', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:481', 'org.apache.hadoop.hive.ql.Context:getExtTmpPathRelTo:Context.java:780', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2356', '*org.apache.hadoop.security.AccessControlException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:55:15', 'sun.reflect.NativeConstructorAccessorImpl:newInstance0:NativeConstructorAccessorImpl.java:-2', 'sun.reflect.NativeConstructorAccessorImpl:newInstance:NativeConstructorAccessorImpl.java:62', 'sun.reflect.DelegatingConstructorAccessorImpl:newInstance:DelegatingConstructorAccessorImpl.java:45', 'java.lang.reflect.Constructor:newInstance:Constructor.java:423', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2518', 'org.apache.hadoop.hdfs.DFSClient:mkdirs:DFSClient.java:2492', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1485', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1482', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirsInternal:DistributedFileSystem.java:1499', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirs:DistributedFileSystem.java:1474', 'org.apache.hadoop.fs.FileSystem:mkdirs:FileSystem.java:2449', 'org.apache.hadoop.hive.common.FileUtils:mkdir:FileUtils.java:587', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:473', '*org.apache.hadoop.ipc.RemoteException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:65:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1630', 'org.apache.hadoop.ipc.Client:call:Client.java:1575', 'org.apache.hadoop.ipc.Client:call:Client.java:1472', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy29:mkdirs::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:mkdirs:ClientNamenodeProtocolTranslatorPB.java:676', 'sun.reflect.GeneratedMethodAccessor25:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy30:mkdirs::-1', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2516'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'"), operationHandle=None)
[0m12:57:39.421167 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-25 12:57:38.112506 => 2023-04-25 12:57:39.421025
[0m12:57:39.421614 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m12:57:39.421980 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m12:57:39.422312 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m12:57:39.485805 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m12:57:39.486789 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6592148-65bf-47cb-924d-c3720c0c725b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f262194b160>]}
[0m12:57:39.487716 [error] [Thread-1  ]: 1 of 2 ERROR creating sql table model test_poc.my_first_dbt_model .............. [[31mERROR[0m in 1.38s]
[0m12:57:39.490578 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m12:57:39.492481 [debug] [Thread-2  ]: Began running node model.poc_demo.my_second_dbt_model
[0m12:57:39.493163 [info ] [Thread-2  ]: 2 of 2 SKIP relation test_poc.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m12:57:39.493915 [debug] [Thread-2  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m12:57:39.496319 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:57:39.496852 [debug] [MainThread]: On master: ROLLBACK
[0m12:57:39.497276 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:57:40.186764 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:57:40.187428 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:57:40.187888 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:57:40.188321 [debug] [MainThread]: On master: ROLLBACK
[0m12:57:40.188736 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:57:40.189129 [debug] [MainThread]: On master: Close
[0m12:57:40.252896 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:57:40.253476 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m12:57:40.254018 [info ] [MainThread]: 
[0m12:57:40.254665 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 4.59 seconds (4.59s).
[0m12:57:40.255537 [debug] [MainThread]: Command end result
[0m12:57:40.267660 [info ] [MainThread]: 
[0m12:57:40.268455 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:57:40.269021 [info ] [MainThread]: 
[0m12:57:40.269601 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
[0m12:57:40.270133 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m12:57:40.270673 [info ] [MainThread]: 
[0m12:57:40.271323 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m12:57:40.272043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26226b7b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f262197d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2624a06fd0>]}
[0m12:57:40.272580 [debug] [MainThread]: Flushing usage events


============================== 2023-04-25 13:00:39.365997 | b705aa77-1db8-4c74-80e3-9bfe1290ddf2 ==============================
[0m13:00:39.365997 [info ] [MainThread]: Running with dbt=1.4.6
[0m13:00:39.367398 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:00:39.367580 [debug] [MainThread]: Tracking: tracking
[0m13:00:39.375394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c1b10e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c1b10eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c1b20250>]}
[0m13:00:39.386088 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m13:00:39.405997 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:00:39.406263 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:00:39.411920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b705aa77-1db8-4c74-80e3-9bfe1290ddf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e86790>]}
[0m13:00:39.417612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b705aa77-1db8-4c74-80e3-9bfe1290ddf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e6e1c0>]}
[0m13:00:39.417934 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:00:39.418208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b705aa77-1db8-4c74-80e3-9bfe1290ddf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e6e370>]}
[0m13:00:39.419358 [info ] [MainThread]: 
[0m13:00:39.420587 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:00:39.421744 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:00:39.432463 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:00:39.432731 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:00:39.432922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:00:40.561673 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:00:40.562432 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:00:40.721000 [debug] [ThreadPool]: On list_schemas: Close
[0m13:00:40.830885 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m13:00:40.843342 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:00:40.843855 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m13:00:40.844281 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m13:00:40.844679 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:00:41.750464 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:00:41.751201 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:00:41.854787 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m13:00:41.855479 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:00:41.855982 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m13:00:41.966374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b705aa77-1db8-4c74-80e3-9bfe1290ddf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e38fd0>]}
[0m13:00:41.967457 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:00:41.967917 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:00:41.968942 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m13:00:41.969593 [info ] [MainThread]: 
[0m13:00:41.976252 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m13:00:41.977121 [info ] [Thread-1  ]: 1 of 2 START sql table model test_poc.my_first_dbt_model ....................... [RUN]
[0m13:00:41.978343 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m13:00:41.978853 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m13:00:41.984798 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m13:00:41.985665 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-25 13:00:41.979196 => 2023-04-25 13:00:41.985504
[0m13:00:41.986189 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m13:00:42.061130 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m13:00:42.061615 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:00:42.061823 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:00:42.061977 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:00:42.062126 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:00:43.270774 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:00:43.272293 [debug] [Thread-1  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.GeneratedMethodAccessor45:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2360', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196', '*java.lang.RuntimeException:Cannot create staging directory \'hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db/.hive-staging_hive_2023-04-25_13-00-42_819_1206164582694051800-4\': Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:40:2', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:481', 'org.apache.hadoop.hive.ql.Context:getExtTmpPathRelTo:Context.java:780', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2356', '*org.apache.hadoop.security.AccessControlException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:55:15', 'sun.reflect.NativeConstructorAccessorImpl:newInstance0:NativeConstructorAccessorImpl.java:-2', 'sun.reflect.NativeConstructorAccessorImpl:newInstance:NativeConstructorAccessorImpl.java:62', 'sun.reflect.DelegatingConstructorAccessorImpl:newInstance:DelegatingConstructorAccessorImpl.java:45', 'java.lang.reflect.Constructor:newInstance:Constructor.java:423', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2518', 'org.apache.hadoop.hdfs.DFSClient:mkdirs:DFSClient.java:2492', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1485', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1482', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirsInternal:DistributedFileSystem.java:1499', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirs:DistributedFileSystem.java:1474', 'org.apache.hadoop.fs.FileSystem:mkdirs:FileSystem.java:2449', 'org.apache.hadoop.hive.common.FileUtils:mkdir:FileUtils.java:587', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:473', '*org.apache.hadoop.ipc.RemoteException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:65:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1630', 'org.apache.hadoop.ipc.Client:call:Client.java:1575', 'org.apache.hadoop.ipc.Client:call:Client.java:1472', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy29:mkdirs::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:mkdirs:ClientNamenodeProtocolTranslatorPB.java:676', 'sun.reflect.GeneratedMethodAccessor25:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy30:mkdirs::-1', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2516'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'"), operationHandle=None)
[0m13:00:43.273200 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-25 13:00:41.986509 => 2023-04-25 13:00:43.273049
[0m13:00:43.273693 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m13:00:43.274122 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:00:43.274522 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m13:00:43.331386 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:00:43.332308 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b705aa77-1db8-4c74-80e3-9bfe1290ddf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3df8d60>]}
[0m13:00:43.333260 [error] [Thread-1  ]: 1 of 2 ERROR creating sql table model test_poc.my_first_dbt_model .............. [[31mERROR[0m in 1.35s]
[0m13:00:43.336234 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m13:00:43.338123 [debug] [Thread-2  ]: Began running node model.poc_demo.my_second_dbt_model
[0m13:00:43.338803 [info ] [Thread-2  ]: 2 of 2 SKIP relation test_poc.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:00:43.339566 [debug] [Thread-2  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m13:00:43.341947 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:00:43.342574 [debug] [MainThread]: On master: ROLLBACK
[0m13:00:43.343069 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:00:44.096149 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:00:44.096829 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:00:44.097298 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:00:44.097751 [debug] [MainThread]: On master: ROLLBACK
[0m13:00:44.098209 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:00:44.098651 [debug] [MainThread]: On master: Close
[0m13:00:44.168358 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:00:44.168977 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m13:00:44.169593 [info ] [MainThread]: 
[0m13:00:44.170283 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 4.75 seconds (4.75s).
[0m13:00:44.171221 [debug] [MainThread]: Command end result
[0m13:00:44.186583 [info ] [MainThread]: 
[0m13:00:44.187511 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:00:44.188072 [info ] [MainThread]: 
[0m13:00:44.188699 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
[0m13:00:44.189490 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:00:44.190064 [info ] [MainThread]: 
[0m13:00:44.190638 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:00:44.191303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e66d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c3e6e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69c0db33a0>]}
[0m13:00:44.191813 [debug] [MainThread]: Flushing usage events


============================== 2023-04-25 13:20:35.972300 | 136ecf44-d173-46ff-b3ff-6b93bb1a94a6 ==============================
[0m13:20:35.972300 [info ] [MainThread]: Running with dbt=1.4.6
[0m13:20:35.979616 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:20:35.979801 [debug] [MainThread]: Tracking: tracking
[0m13:20:35.987685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b22dcfe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b22dcfeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b22ddc250>]}
[0m13:20:35.998556 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m13:20:36.018665 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:20:36.018936 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:20:36.024470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '136ecf44-d173-46ff-b3ff-6b93bb1a94a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b250c3790>]}
[0m13:20:36.029967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '136ecf44-d173-46ff-b3ff-6b93bb1a94a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2512b1c0>]}
[0m13:20:36.030288 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:20:36.030572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '136ecf44-d173-46ff-b3ff-6b93bb1a94a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2512b370>]}
[0m13:20:36.031729 [info ] [MainThread]: 
[0m13:20:36.032940 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:20:36.033948 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:20:36.044349 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:20:36.044585 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:20:36.044767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:20:36.949606 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:20:36.950164 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:20:37.096717 [debug] [ThreadPool]: On list_schemas: Close
[0m13:20:37.196395 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m13:20:37.209054 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:37.209552 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m13:20:37.209960 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m13:20:37.210335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:20:38.038020 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:20:38.038642 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:20:38.143429 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m13:20:38.143974 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:20:38.144326 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m13:20:38.266350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '136ecf44-d173-46ff-b3ff-6b93bb1a94a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b25122f70>]}
[0m13:20:38.267396 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:38.267911 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:20:38.268977 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m13:20:38.269623 [info ] [MainThread]: 
[0m13:20:38.276729 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m13:20:38.277645 [info ] [Thread-1  ]: 1 of 2 START sql table model test_poc.my_first_dbt_model ....................... [RUN]
[0m13:20:38.279048 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m13:20:38.279647 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m13:20:38.285019 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m13:20:38.286002 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-25 13:20:38.280004 => 2023-04-25 13:20:38.285765
[0m13:20:38.286686 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m13:20:38.359617 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m13:20:38.360160 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:38.360388 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:20:38.360566 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:20:38.360737 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:20:39.780311 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:20:39.782077 [debug] [Thread-1  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.GeneratedMethodAccessor45:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2360', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196', '*java.lang.RuntimeException:Cannot create staging directory \'hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db/.hive-staging_hive_2023-04-25_13-20-39_256_6482339437545383708-2\': Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:40:2', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:481', 'org.apache.hadoop.hive.ql.Context:getExtTmpPathRelTo:Context.java:780', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2356', '*org.apache.hadoop.security.AccessControlException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:55:15', 'sun.reflect.NativeConstructorAccessorImpl:newInstance0:NativeConstructorAccessorImpl.java:-2', 'sun.reflect.NativeConstructorAccessorImpl:newInstance:NativeConstructorAccessorImpl.java:62', 'sun.reflect.DelegatingConstructorAccessorImpl:newInstance:DelegatingConstructorAccessorImpl.java:45', 'java.lang.reflect.Constructor:newInstance:Constructor.java:423', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2518', 'org.apache.hadoop.hdfs.DFSClient:mkdirs:DFSClient.java:2492', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1485', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1482', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirsInternal:DistributedFileSystem.java:1499', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirs:DistributedFileSystem.java:1474', 'org.apache.hadoop.fs.FileSystem:mkdirs:FileSystem.java:2449', 'org.apache.hadoop.hive.common.FileUtils:mkdir:FileUtils.java:587', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:473', '*org.apache.hadoop.ipc.RemoteException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:65:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1630', 'org.apache.hadoop.ipc.Client:call:Client.java:1575', 'org.apache.hadoop.ipc.Client:call:Client.java:1472', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy29:mkdirs::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:mkdirs:ClientNamenodeProtocolTranslatorPB.java:676', 'sun.reflect.GeneratedMethodAccessor25:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy30:mkdirs::-1', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2516'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'"), operationHandle=None)
[0m13:20:39.783178 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-25 13:20:38.287046 => 2023-04-25 13:20:39.783023
[0m13:20:39.783737 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m13:20:39.784209 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:20:39.784664 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m13:20:39.845422 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:20:39.846366 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '136ecf44-d173-46ff-b3ff-6b93bb1a94a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b250b3d60>]}
[0m13:20:39.847384 [error] [Thread-1  ]: 1 of 2 ERROR creating sql table model test_poc.my_first_dbt_model .............. [[31mERROR[0m in 1.57s]
[0m13:20:39.850315 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m13:20:39.852276 [debug] [Thread-2  ]: Began running node model.poc_demo.my_second_dbt_model
[0m13:20:39.853019 [info ] [Thread-2  ]: 2 of 2 SKIP relation test_poc.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:20:39.853870 [debug] [Thread-2  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m13:20:39.856250 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:20:39.856789 [debug] [MainThread]: On master: ROLLBACK
[0m13:20:39.857247 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:20:40.565508 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:20:40.566273 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:40.566838 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:20:40.567427 [debug] [MainThread]: On master: ROLLBACK
[0m13:20:40.567941 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:20:40.568407 [debug] [MainThread]: On master: Close
[0m13:20:40.617860 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:20:40.618510 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m13:20:40.619226 [info ] [MainThread]: 
[0m13:20:40.620004 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 4.59 seconds (4.59s).
[0m13:20:40.621004 [debug] [MainThread]: Command end result
[0m13:20:40.635131 [info ] [MainThread]: 
[0m13:20:40.636008 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:20:40.636622 [info ] [MainThread]: 
[0m13:20:40.637214 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
[0m13:20:40.637762 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:20:40.638373 [info ] [MainThread]: 
[0m13:20:40.638969 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:20:40.639715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b25122d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2512b100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2512b370>]}
[0m13:20:40.640287 [debug] [MainThread]: Flushing usage events


============================== 2023-04-25 13:30:12.888951 | fcf809de-90a8-415a-8004-e39198a97d47 ==============================
[0m13:30:12.888951 [info ] [MainThread]: Running with dbt=1.4.6
[0m13:30:12.890416 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:30:12.890595 [debug] [MainThread]: Tracking: tracking
[0m13:30:12.898372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61cacbf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61cadb070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61cadb340>]}
[0m13:30:12.908461 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m13:30:12.928299 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:30:12.928564 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:30:12.934177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fcf809de-90a8-415a-8004-e39198a97d47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee41850>]}
[0m13:30:12.939710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fcf809de-90a8-415a-8004-e39198a97d47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee2a280>]}
[0m13:30:12.940019 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:30:12.940285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcf809de-90a8-415a-8004-e39198a97d47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee2a430>]}
[0m13:30:12.941421 [info ] [MainThread]: 
[0m13:30:12.942639 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:30:12.943692 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:30:12.953458 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:30:12.953717 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:30:12.953918 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:30:13.908040 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:30:13.908795 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:30:14.057904 [debug] [ThreadPool]: On list_schemas: Close
[0m13:30:14.159512 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m13:30:14.171787 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:30:14.172353 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m13:30:14.172846 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m13:30:14.173296 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:30:15.002300 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:30:15.002983 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m13:30:15.094159 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m13:30:15.094715 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:30:15.095123 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m13:30:15.190410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcf809de-90a8-415a-8004-e39198a97d47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee22a30>]}
[0m13:30:15.191463 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:30:15.191972 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:30:15.193051 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m13:30:15.193692 [info ] [MainThread]: 
[0m13:30:15.200527 [debug] [Thread-1  ]: Began running node model.poc_demo.my_first_dbt_model
[0m13:30:15.201316 [info ] [Thread-1  ]: 1 of 2 START sql table model test_poc.my_first_dbt_model ....................... [RUN]
[0m13:30:15.202557 [debug] [Thread-1  ]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m13:30:15.203067 [debug] [Thread-1  ]: Began compiling node model.poc_demo.my_first_dbt_model
[0m13:30:15.209129 [debug] [Thread-1  ]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m13:30:15.210322 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (compile): 2023-04-25 13:30:15.203399 => 2023-04-25 13:30:15.210092
[0m13:30:15.211099 [debug] [Thread-1  ]: Began executing node model.poc_demo.my_first_dbt_model
[0m13:30:15.285631 [debug] [Thread-1  ]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m13:30:15.286154 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:30:15.286353 [debug] [Thread-1  ]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m13:30:15.286488 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:30:15.286626 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:30:16.533161 [debug] [Thread-1  ]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table test_poc.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:30:16.534662 [debug] [Thread-1  ]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:549', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:535', 'sun.reflect.GeneratedMethodAccessor45:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy43:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:750', "*org.apache.hadoop.hive.ql.parse.SemanticException:0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2360', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2073', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12205', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12299', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:364', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:294', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:675', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1872', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1819', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1814', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196', '*java.lang.RuntimeException:Cannot create staging directory \'hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db/.hive-staging_hive_2023-04-25_13-30-16_198_4057270691902377716-2\': Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:40:2', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:481', 'org.apache.hadoop.hive.ql.Context:getExtTmpPathRelTo:Context.java:780', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2356', '*org.apache.hadoop.security.AccessControlException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:55:15', 'sun.reflect.NativeConstructorAccessorImpl:newInstance0:NativeConstructorAccessorImpl.java:-2', 'sun.reflect.NativeConstructorAccessorImpl:newInstance:NativeConstructorAccessorImpl.java:62', 'sun.reflect.DelegatingConstructorAccessorImpl:newInstance:DelegatingConstructorAccessorImpl.java:45', 'java.lang.reflect.Constructor:newInstance:Constructor.java:423', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2518', 'org.apache.hadoop.hdfs.DFSClient:mkdirs:DFSClient.java:2492', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1485', 'org.apache.hadoop.hdfs.DistributedFileSystem$27:doCall:DistributedFileSystem.java:1482', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirsInternal:DistributedFileSystem.java:1499', 'org.apache.hadoop.hdfs.DistributedFileSystem:mkdirs:DistributedFileSystem.java:1474', 'org.apache.hadoop.fs.FileSystem:mkdirs:FileSystem.java:2449', 'org.apache.hadoop.hive.common.FileUtils:mkdir:FileUtils.java:587', 'org.apache.hadoop.hive.ql.Context:getStagingDir:Context.java:473', '*org.apache.hadoop.ipc.RemoteException:Permission denied: user=prajwali, access=WRITE, inode="/user/hive/warehouse/test_poc.db":qwerty:hdfsadmingroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3424)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:742)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1175)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1099)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3316)\n:65:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1630', 'org.apache.hadoop.ipc.Client:call:Client.java:1575', 'org.apache.hadoop.ipc.Client:call:Client.java:1472', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy29:mkdirs::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:mkdirs:ClientNamenodeProtocolTranslatorPB.java:676', 'sun.reflect.GeneratedMethodAccessor25:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy30:mkdirs::-1', 'org.apache.hadoop.hdfs.DFSClient:primitiveMkdir:DFSClient.java:2516'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'"), operationHandle=None)
[0m13:30:16.535535 [debug] [Thread-1  ]: Timing info for model.poc_demo.my_first_dbt_model (execute): 2023-04-25 13:30:15.211500 => 2023-04-25 13:30:16.535392
[0m13:30:16.535987 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m13:30:16.536356 [debug] [Thread-1  ]: Spark adapter: NotImplemented: rollback
[0m13:30:16.536693 [debug] [Thread-1  ]: On model.poc_demo.my_first_dbt_model: Close
[0m13:30:16.589038 [debug] [Thread-1  ]: Runtime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)
  Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:30:16.589913 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fcf809de-90a8-415a-8004-e39198a97d47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61edb9e20>]}
[0m13:30:16.590866 [error] [Thread-1  ]: 1 of 2 ERROR creating sql table model test_poc.my_first_dbt_model .............. [[31mERROR[0m in 1.39s]
[0m13:30:16.593783 [debug] [Thread-1  ]: Finished running node model.poc_demo.my_first_dbt_model
[0m13:30:16.595768 [debug] [Thread-2  ]: Began running node model.poc_demo.my_second_dbt_model
[0m13:30:16.596431 [info ] [Thread-2  ]: 2 of 2 SKIP relation test_poc.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:30:16.597181 [debug] [Thread-2  ]: Finished running node model.poc_demo.my_second_dbt_model
[0m13:30:16.599295 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:30:16.599862 [debug] [MainThread]: On master: ROLLBACK
[0m13:30:16.600326 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:30:17.336492 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:30:17.337230 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:30:17.337676 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:30:17.338111 [debug] [MainThread]: On master: ROLLBACK
[0m13:30:17.338523 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:30:17.338952 [debug] [MainThread]: On master: Close
[0m13:30:17.390595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:30:17.391236 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m13:30:17.391822 [info ] [MainThread]: 
[0m13:30:17.392535 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 4.45 seconds (4.45s).
[0m13:30:17.393475 [debug] [MainThread]: Command end result
[0m13:30:17.407448 [info ] [MainThread]: 
[0m13:30:17.408220 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:30:17.408788 [info ] [MainThread]: 
[0m13:30:17.409329 [error] [MainThread]: [33mRuntime Error in model my_first_dbt_model (models/example/my_first_dbt_model.sql)[0m
[0m13:30:17.409843 [error] [MainThread]:   Error while compiling statement: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ip-10-0-0-115.ap-south-1.compute.internal:8020/user/hive/warehouse/test_poc.db. Error encountered near token 'TOK_TMP_FILE'
[0m13:30:17.410342 [info ] [MainThread]: 
[0m13:30:17.410989 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:30:17.411681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee22df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee2a370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61ee2a2e0>]}
[0m13:30:17.412258 [debug] [MainThread]: Flushing usage events


============================== 2023-04-26 06:46:40.128248 | 563ddd34-2313-4eff-a5f6-00b8df291daf ==============================
[0m06:46:40.128248 [info ] [MainThread]: Running with dbt=1.4.6
[0m06:46:40.129475 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m06:46:40.129644 [debug] [MainThread]: Tracking: tracking
[0m06:46:40.137789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf5d7940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf5d7f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf5d7fd0>]}
[0m06:46:40.247569 [debug] [MainThread]: Executing "git --help"
[0m06:46:40.256745 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:46:40.257149 [debug] [MainThread]: STDERR: "b''"
[0m06:46:40.261783 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:46:40.262824 [debug] [MainThread]: Using spark connection "debug"
[0m06:46:40.263123 [debug] [MainThread]: On debug: select 1 as id
[0m06:46:40.263389 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:46:41.359482 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m06:46:41.360229 [debug] [MainThread]: SQL status: OK in 1 seconds
[0m06:46:41.362875 [debug] [MainThread]: On debug: Close
[0m06:46:41.477943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf37dc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf37d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ccf394c70>]}
[0m06:46:41.478950 [debug] [MainThread]: Flushing usage events
[0m06:46:42.761777 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-04-26 06:46:47.786968 | 604f2f65-fb15-4e6f-b643-bdce649c6c1e ==============================
[0m06:46:47.786968 [info ] [MainThread]: Running with dbt=1.4.6
[0m06:46:47.788364 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/prajwali/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m06:46:47.788548 [debug] [MainThread]: Tracking: tracking
[0m06:46:47.796622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadb64ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadb65d280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadb65d3d0>]}
[0m06:46:47.809375 [debug] [MainThread]: checksum: 0325e47f1211ebbdb24627f81d8289d705fdb573380364d07ab35982cb3d57cd, vars: {}, profile: None, target: None, version: 1.4.6
[0m06:46:47.820348 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m06:46:47.820732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '604f2f65-fb15-4e6f-b643-bdce649c6c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadb65d640>]}
[0m06:46:48.449402 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m06:46:48.460067 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m06:46:48.530680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '604f2f65-fb15-4e6f-b643-bdce649c6c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadd905130>]}
[0m06:46:48.535848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '604f2f65-fb15-4e6f-b643-bdce649c6c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadb65d070>]}
[0m06:46:48.536155 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 332 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m06:46:48.536415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '604f2f65-fb15-4e6f-b643-bdce649c6c1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadd9739d0>]}
[0m06:46:48.537533 [info ] [MainThread]: 
[0m06:46:48.538756 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m06:46:48.539759 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m06:46:48.548711 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m06:46:48.548936 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m06:46:48.549126 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:46:49.378475 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:46:49.379189 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m06:46:49.506823 [debug] [ThreadPool]: On list_schemas: Close
[0m06:46:49.596047 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_test_poc'
[0m06:46:49.609824 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:46:49.610340 [debug] [ThreadPool]: Using spark connection "list_None_test_poc"
[0m06:46:49.610741 [debug] [ThreadPool]: On list_None_test_poc: /* {"app": "dbt", "dbt_version": "1.4.6", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_test_poc"} */
show table extended in test_poc like '*'
  
[0m06:46:49.611119 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:46:50.623870 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:46:50.624588 [debug] [ThreadPool]: SQL status: OK in 1 seconds
[0m06:46:50.771577 [debug] [ThreadPool]: On list_None_test_poc: ROLLBACK
[0m06:46:50.772322 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m06:46:50.772807 [debug] [ThreadPool]: On list_None_test_poc: Close
[0m06:46:50.873295 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:46:50.873888 [debug] [MainThread]: Connection 'list_None_test_poc' was properly closed.
[0m06:46:50.874402 [info ] [MainThread]: 
[0m06:46:50.875053 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.34 seconds (2.34s).
[0m06:46:50.875852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadd94ca00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadd94c640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffadd8f3280>]}
[0m06:46:50.876434 [debug] [MainThread]: Flushing usage events
[0m06:46:52.183358 [error] [MainThread]: Encountered an error:
Runtime Error
  Invalid value from "show table extended ...", got 1 values, expected 4
[0m19:07:41.979894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390AFC1AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390D6CECB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390D6CEA10>]}


============================== 19:07:41.984981 | 5e6c4b31-2bea-4308-858f-c046b924983e ==============================
[0m19:07:41.984981 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:07:41.985988 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:07:41.986989 [info ] [MainThread]: dbt version: 1.5.2
[0m19:07:41.986989 [info ] [MainThread]: python version: 3.10.11
[0m19:07:41.987989 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m19:07:41.987989 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:07:41.988986 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m19:07:41.988986 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m19:07:41.990013 [info ] [MainThread]: Configuration:
[0m19:07:42.123243 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:07:42.135145 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:07:42.136144 [info ] [MainThread]: Required dependencies:
[0m19:07:42.136144 [debug] [MainThread]: Executing "git --help"
[0m19:07:42.162861 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:07:42.163860 [debug] [MainThread]: STDERR: "b''"
[0m19:07:42.163860 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:07:42.163860 [info ] [MainThread]: Connection:
[0m19:07:42.164861 [info ] [MainThread]:   host: localhost
[0m19:07:42.164861 [info ] [MainThread]:   port: 10000
[0m19:07:42.165862 [info ] [MainThread]:   cluster: None
[0m19:07:42.166863 [info ] [MainThread]:   endpoint: None
[0m19:07:42.166863 [info ] [MainThread]:   schema: default
[0m19:07:42.167861 [info ] [MainThread]:   organization: 0
[0m19:07:42.167861 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:07:42.168904 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m19:07:42.169952 [debug] [MainThread]: Using spark connection "debug"
[0m19:07:42.169952 [debug] [MainThread]: On debug: select 1 as id
[0m19:07:42.169952 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:07:46.270217 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m19:07:46.271239 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m19:07:46.273252 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m19:07:46.274858 [info ] [MainThread]: [31m1 check failed:[0m
[0m19:07:46.275918 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m19:07:46.276275 [debug] [MainThread]: Command `dbt debug` failed at 19:07:46.276275 after 4.33 seconds
[0m19:07:46.276275 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:07:46.276275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390DA7D1B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390DA7EB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390DA7C3A0>]}
[0m19:07:46.277282 [debug] [MainThread]: Flushing usage events
[0m19:08:40.921135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437F271AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143019CECB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143019CEA70>]}


============================== 19:08:40.924206 | 4ff8329c-58d6-4cea-9801-e3d281d6018c ==============================
[0m19:08:40.924206 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:08:40.925244 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:08:40.926139 [info ] [MainThread]: dbt version: 1.5.2
[0m19:08:40.926139 [info ] [MainThread]: python version: 3.10.11
[0m19:08:40.927191 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m19:08:40.928128 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:08:40.928128 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m19:08:40.929131 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m19:08:40.930223 [info ] [MainThread]: Configuration:
[0m19:08:41.026244 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:08:41.040343 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:08:41.041340 [info ] [MainThread]: Required dependencies:
[0m19:08:41.042378 [debug] [MainThread]: Executing "git --help"
[0m19:08:41.068295 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:08:41.068800 [debug] [MainThread]: STDERR: "b''"
[0m19:08:41.068800 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:08:41.069804 [info ] [MainThread]: Connection:
[0m19:08:41.069804 [info ] [MainThread]:   host: localhost
[0m19:08:41.070807 [info ] [MainThread]:   port: 10001
[0m19:08:41.071805 [info ] [MainThread]:   cluster: None
[0m19:08:41.072274 [info ] [MainThread]:   endpoint: None
[0m19:08:41.072274 [info ] [MainThread]:   schema: default
[0m19:08:41.073281 [info ] [MainThread]:   organization: 0
[0m19:08:41.073281 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:08:41.074281 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m19:08:41.075279 [debug] [MainThread]: Using spark connection "debug"
[0m19:08:41.075279 [debug] [MainThread]: On debug: select 1 as id
[0m19:08:41.075279 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:08:45.168271 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m19:08:45.169340 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m19:08:45.170261 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m19:08:45.171988 [info ] [MainThread]: [31m1 check failed:[0m
[0m19:08:45.174082 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m19:08:45.176092 [debug] [MainThread]: Command `dbt debug` failed at 19:08:45.176092 after 4.27 seconds
[0m19:08:45.177088 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:08:45.177088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014301E7D1B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014301E7F640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014301E7C3A0>]}
[0m19:08:45.177088 [debug] [MainThread]: Flushing usage events
[0m19:10:04.257854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6858DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6AC9AC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6AC9A980>]}


============================== 19:10:04.260855 | 74121673-09bc-4c08-bea3-b466597bdbad ==============================
[0m19:10:04.260855 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:10:04.261854 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:10:04.261854 [info ] [MainThread]: dbt version: 1.5.2
[0m19:10:04.263319 [info ] [MainThread]: python version: 3.10.11
[0m19:10:04.264323 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m19:10:04.264323 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:10:04.265324 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m19:10:04.265324 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m19:10:04.266324 [info ] [MainThread]: Configuration:
[0m19:10:04.349043 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:10:04.364726 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:10:04.364726 [info ] [MainThread]: Required dependencies:
[0m19:10:04.365733 [debug] [MainThread]: Executing "git --help"
[0m19:10:04.396530 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:10:04.397627 [debug] [MainThread]: STDERR: "b''"
[0m19:10:04.398632 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:10:04.399636 [info ] [MainThread]: Connection:
[0m19:10:04.400631 [info ] [MainThread]:   host: localhost
[0m19:10:04.400631 [info ] [MainThread]:   port: 10000
[0m19:10:04.401633 [info ] [MainThread]:   cluster: None
[0m19:10:04.401633 [info ] [MainThread]:   endpoint: None
[0m19:10:04.402634 [info ] [MainThread]:   schema: default
[0m19:10:04.402634 [info ] [MainThread]:   organization: 0
[0m19:10:04.403633 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:10:04.404633 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m19:10:04.404633 [debug] [MainThread]: Using spark connection "debug"
[0m19:10:04.404633 [debug] [MainThread]: On debug: select 1 as id
[0m19:10:04.405633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:10:08.500300 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m19:10:08.501338 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m19:10:08.503254 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m19:10:08.504258 [info ] [MainThread]: [31m1 check failed:[0m
[0m19:10:08.505593 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m19:10:08.505593 [debug] [MainThread]: Command `dbt debug` failed at 19:10:08.505593 after 4.26 seconds
[0m19:10:08.506599 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:10:08.506599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6858DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6B044190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED6B044B50>]}
[0m19:10:08.506599 [debug] [MainThread]: Flushing usage events
[0m19:18:33.180961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB78071B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB7A77EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB7A77E9B0>]}


============================== 19:18:33.184947 | 09f1485d-445d-497a-a165-569424bd8758 ==============================
[0m19:18:33.184947 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:18:33.185960 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:18:33.185960 [info ] [MainThread]: dbt version: 1.5.2
[0m19:18:33.186952 [info ] [MainThread]: python version: 3.10.11
[0m19:18:33.188059 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m19:18:33.188059 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:18:33.189066 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m19:18:33.189066 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m19:18:33.190076 [info ] [MainThread]: Configuration:
[0m19:18:33.269227 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:18:33.282312 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:18:33.283310 [info ] [MainThread]: Required dependencies:
[0m19:18:33.284232 [debug] [MainThread]: Executing "git --help"
[0m19:18:33.312852 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:18:33.313878 [debug] [MainThread]: STDERR: "b''"
[0m19:18:33.313878 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:18:33.314854 [info ] [MainThread]: Connection:
[0m19:18:33.314854 [info ] [MainThread]:   host: localhost
[0m19:18:33.315855 [info ] [MainThread]:   port: 10000
[0m19:18:33.315855 [info ] [MainThread]:   cluster: None
[0m19:18:33.323210 [info ] [MainThread]:   endpoint: None
[0m19:18:33.323210 [info ] [MainThread]:   schema: default
[0m19:18:33.324305 [info ] [MainThread]:   organization: 0
[0m19:18:33.325266 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:18:33.325266 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m19:18:33.326305 [debug] [MainThread]: Using spark connection "debug"
[0m19:18:33.326305 [debug] [MainThread]: On debug: select 1 as id
[0m19:18:33.326305 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:18:34.485698 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m19:18:34.486695 [debug] [MainThread]: SQL status: OK in 1.0 seconds
[0m19:18:34.487693 [debug] [MainThread]: On debug: Close
[0m19:18:34.513241 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m19:18:34.514747 [info ] [MainThread]: [32mAll checks passed![0m
[0m19:18:34.515755 [debug] [MainThread]: Command `dbt debug` succeeded at 19:18:34.515755 after 1.35 seconds
[0m19:18:34.515755 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:18:34.516756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB78071B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB771FFA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB771FDBA0>]}
[0m19:18:34.516756 [debug] [MainThread]: Flushing usage events
[0m19:19:14.272592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238DF891B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E1F9EC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E1F9E9E0>]}


============================== 19:19:14.275619 | 74da4bc6-ff97-46f4-a5b9-9531464c3c0c ==============================
[0m19:19:14.275619 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:19:14.276599 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:19:14.362942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E1F9EC50>]}
[0m19:19:14.370202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E2342080>]}
[0m19:19:14.370202 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:19:14.391572 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m19:19:14.443392 [debug] [MainThread]: Failed to load parsed file from disk at D:\home\Tanmay\Documents\demo_dbt\poc_demo\target\partial_parse.msgpack: Field "nodes" of type MutableMapping[str, Union[AnalysisNode, SingularTestNode, HookNode, ModelNode, RPCNode, SqlNode, GenericTestNode, SnapshotNode, SeedNode]] in Manifest has invalid value {'model.poc_demo.my_first_dbt_model': {'database': None, 'schema': 'test_poc', 'name': 'my_first_dbt_model', 'resource_type': 'model', 'package_name': 'poc_demo', 'path': 'example/my_first_dbt_model.sql', 'original_file_path': 'models/example/my_first_dbt_model.sql', 'unique_id': 'model.poc_demo.my_first_dbt_model', 'fqn': ['poc_demo', 'example', 'my_first_dbt_model'], 'alias': 'my_first_dbt_model', 'checksum': {'name': 'sha256', 'checksum': '842251f5ed1d97920d3748d9686b8c05a3a0071ec7990f948f36796491788aed'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': 'A starter dbt model', 'columns': {'id': {'name': 'id', 'description': 'The primary key for this table', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': 'poc_demo://models/example/schema.yml', 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1682491608.4818006, 'config_call_dict': {'materialized': 'table'}, 'relation_name': 'test_poc.my_first_dbt_model', 'raw_code': '/*\n    Welcome to your first dbt model!\n    Did you know that you can also configure models directly within SQL files?\n    This will override configurations stated in dbt_project.yml\n\n    Try changing "table" to "view" below\n*/\n\n{{ config(materialized=\'table\') }}\n\nwith source_data as (\n\n    select 1 as id\n    union all\n    select null as id\n\n)\n\nselect *\nfrom source_data\n\n/*\n    Uncomment the line below to remove records with null `id` values\n*/\n\n-- where id is not null', 'language': 'sql', 'refs': [], 'sources': [], 'metrics': [], 'depends_on': {'macros': [], 'nodes': []}, 'compiled_path': None}, 'model.poc_demo.my_second_dbt_model': {'database': None, 'schema': 'test_poc', 'name': 'my_second_dbt_model', 'resource_type': 'model', 'package_name': 'poc_demo', 'path': 'example/my_second_dbt_model.sql', 'original_file_path': 'models/example/my_second_dbt_model.sql', 'unique_id': 'model.poc_demo.my_second_dbt_model', 'fqn': ['poc_demo', 'example', 'my_second_dbt_model'], 'alias': 'my_second_dbt_model', 'checksum': {'name': 'sha256', 'checksum': 'b3aa346f283f3c9c9a75936f3b80d2572ca9ab39aee4c02b30553d3fe2ba5692'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': 'A starter dbt model', 'columns': {'id': {'name': 'id', 'description': 'The primary key for this table', 'meta': {}, 'data_type': None, 'quote': None, 'tags': []}}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': 'poc_demo://models/example/schema.yml', 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1682491608.4837952, 'config_call_dict': {}, 'relation_name': 'test_poc.my_second_dbt_model', 'raw_code': "-- Use the `ref` function to select from other models\n\nselect *\nfrom {{ ref('my_first_dbt_model') }}\nwhere id = 1", 'language': 'sql', 'refs': [['my_first_dbt_model']], 'sources': [], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['model.poc_demo.my_first_dbt_model']}, 'compiled_path': None}, 'test.poc_demo.unique_my_first_dbt_model_id.16e066b321': {'test_metadata': {'name': 'unique', 'kwargs': {'column_name': 'id', 'model': "{{ get_where_subquery(ref('my_first_dbt_model')) }}"}, 'namespace': None}, 'database': None, 'schema': 'test_poc_dbt_test__audit', 'name': 'unique_my_first_dbt_model_id', 'resource_type': 'test', 'package_name': 'poc_demo', 'path': 'unique_my_first_dbt_model_id.sql', 'original_file_path': 'models/example/schema.yml', 'unique_id': 'test.poc_demo.unique_my_first_dbt_model_id.16e066b321', 'fqn': ['poc_demo', 'example', 'unique_my_first_dbt_model_id'], 'alias': 'unique_my_first_dbt_model_id', 'checksum': {'name': 'none', 'checksum': ''}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1682491608.4991596, 'config_call_dict': {}, 'relation_name': None, 'raw_code': '{{ test_unique(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'refs': [['my_first_dbt_model']], 'sources': [], 'metrics': [], 'depends_on': {'macros': ['macro.dbt.test_unique'], 'nodes': ['model.poc_demo.my_first_dbt_model']}, 'compiled_path': None, 'column_name': 'id', 'file_key_name': 'models.my_first_dbt_model'}, 'test.poc_demo.not_null_my_first_dbt_model_id.5fb22c2710': {'test_metadata': {'name': 'not_null', 'kwargs': {'column_name': 'id', 'model': "{{ get_where_subquery(ref('my_first_dbt_model')) }}"}, 'namespace': None}, 'database': None, 'schema': 'test_poc_dbt_test__audit', 'name': 'not_null_my_first_dbt_model_id', 'resource_type': 'test', 'package_name': 'poc_demo', 'path': 'not_null_my_first_dbt_model_id.sql', 'original_file_path': 'models/example/schema.yml', 'unique_id': 'test.poc_demo.not_null_my_first_dbt_model_id.5fb22c2710', 'fqn': ['poc_demo', 'example', 'not_null_my_first_dbt_model_id'], 'alias': 'not_null_my_first_dbt_model_id', 'checksum': {'name': 'none', 'checksum': ''}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1682491608.5029573, 'config_call_dict': {}, 'relation_name': None, 'raw_code': '{{ test_not_null(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'refs': [['my_first_dbt_model']], 'sources': [], 'metrics': [], 'depends_on': {'macros': ['macro.dbt.test_not_null'], 'nodes': ['model.poc_demo.my_first_dbt_model']}, 'compiled_path': None, 'column_name': 'id', 'file_key_name': 'models.my_first_dbt_model'}, 'test.poc_demo.unique_my_second_dbt_model_id.57a0f8c493': {'test_metadata': {'name': 'unique', 'kwargs': {'column_name': 'id', 'model': "{{ get_where_subquery(ref('my_second_dbt_model')) }}"}, 'namespace': None}, 'database': None, 'schema': 'test_poc_dbt_test__audit', 'name': 'unique_my_second_dbt_model_id', 'resource_type': 'test', 'package_name': 'poc_demo', 'path': 'unique_my_second_dbt_model_id.sql', 'original_file_path': 'models/example/schema.yml', 'unique_id': 'test.poc_demo.unique_my_second_dbt_model_id.57a0f8c493', 'fqn': ['poc_demo', 'example', 'unique_my_second_dbt_model_id'], 'alias': 'unique_my_second_dbt_model_id', 'checksum': {'name': 'none', 'checksum': ''}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1682491608.505803, 'config_call_dict': {}, 'relation_name': None, 'raw_code': '{{ test_unique(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'refs': [['my_second_dbt_model']], 'sources': [], 'metrics': [], 'depends_on': {'macros': ['macro.dbt.test_unique'], 'nodes': ['model.poc_demo.my_second_dbt_model']}, 'compiled_path': None, 'column_name': 'id', 'file_key_name': 'models.my_second_dbt_model'}, 'test.poc_demo.not_null_my_second_dbt_model_id.151b76d778': {'test_metadata': {'name': 'not_null', 'kwargs': {'column_name': 'id', 'model': "{{ get_where_subquery(ref('my_second_dbt_model')) }}"}, 'namespace': None}, 'database': None, 'schema': 'test_poc_dbt_test__audit', 'name': 'not_null_my_second_dbt_model_id', 'resource_type': 'test', 'package_name': 'poc_demo', 'path': 'not_null_my_second_dbt_model_id.sql', 'original_file_path': 'models/example/schema.yml', 'unique_id': 'test.poc_demo.not_null_my_second_dbt_model_id.151b76d778', 'fqn': ['poc_demo', 'example', 'not_null_my_second_dbt_model_id'], 'alias': 'not_null_my_second_dbt_model_id', 'checksum': {'name': 'none', 'checksum': ''}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1682491608.5086823, 'config_call_dict': {}, 'relation_name': None, 'raw_code': '{{ test_not_null(**_dbt_generic_test_kwargs) }}', 'language': 'sql', 'refs': [['my_second_dbt_model']], 'sources': [], 'metrics': [], 'depends_on': {'macros': ['macro.dbt.test_not_null'], 'nodes': ['model.poc_demo.my_second_dbt_model']}, 'compiled_path': None, 'column_name': 'id', 'file_key_name': 'models.my_second_dbt_model'}}
[0m19:19:14.444408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238DFA96890>]}
[0m19:19:15.522857 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m19:19:15.534948 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m19:19:15.631123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E23D4C70>]}
[0m19:19:15.636121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E23D7220>]}
[0m19:19:15.637120 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m19:19:15.637857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E23D5A20>]}
[0m19:19:15.639873 [info ] [MainThread]: 
[0m19:19:15.640813 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m19:19:15.642839 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m19:19:15.649822 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m19:19:15.649822 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:19:15.650836 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:19:15.893282 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m19:19:15.893792 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:19:15.901843 [debug] [ThreadPool]: On list_schemas: Close
[0m19:19:15.914526 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m19:19:15.920207 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:19:15.920207 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m19:19:15.921189 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m19:19:15.921189 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:19:16.032992 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m19:19:16.032992 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:19:16.037199 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m19:19:16.038197 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m19:19:16.038197 [debug] [ThreadPool]: On list_None_default: Close
[0m19:19:16.048197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E1F9E230>]}
[0m19:19:16.049199 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:19:16.050209 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:19:16.050209 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:19:16.051198 [info ] [MainThread]: 
[0m19:19:16.064023 [debug] [Thread-1 (]: Began running node model.poc_demo.my_first_dbt_model
[0m19:19:16.065048 [info ] [Thread-1 (]: 1 of 2 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m19:19:16.066023 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m19:19:16.066023 [debug] [Thread-1 (]: Began compiling node model.poc_demo.my_first_dbt_model
[0m19:19:16.068042 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m19:19:16.070047 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (compile): 19:19:16.066023 => 19:19:16.069024
[0m19:19:16.070047 [debug] [Thread-1 (]: Began executing node model.poc_demo.my_first_dbt_model
[0m19:19:16.086125 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m19:19:16.087131 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists default.my_first_dbt_model
[0m19:19:16.087131 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:19:16.201951 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m19:19:16.201951 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:19:16.234245 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m19:19:16.236142 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m19:19:16.237149 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m19:19:16.237149 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table default.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:19:17.697922 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m19:19:17.699032 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m19:19:17.716127 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (execute): 19:19:16.070047 => 19:19:17.716127
[0m19:19:17.716127 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m19:19:17.717060 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m19:19:17.717060 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: Close
[0m19:19:17.731282 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E2482E30>]}
[0m19:19:17.733128 [info ] [Thread-1 (]: 1 of 2 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 1.67s]
[0m19:19:17.734132 [debug] [Thread-1 (]: Finished running node model.poc_demo.my_first_dbt_model
[0m19:19:17.735129 [debug] [Thread-1 (]: Began running node model.poc_demo.my_second_dbt_model
[0m19:19:17.735129 [info ] [Thread-1 (]: 2 of 2 START sql table model default.my_second_dbt_model ....................... [RUN]
[0m19:19:17.736633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.my_first_dbt_model, now model.poc_demo.my_second_dbt_model)
[0m19:19:17.738146 [debug] [Thread-1 (]: Began compiling node model.poc_demo.my_second_dbt_model
[0m19:19:17.741196 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m19:19:17.742196 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_second_dbt_model (compile): 19:19:17.738146 => 19:19:17.742196
[0m19:19:17.742196 [debug] [Thread-1 (]: Began executing node model.poc_demo.my_second_dbt_model
[0m19:19:17.747252 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m19:19:17.748245 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
drop table if exists default.my_second_dbt_model
[0m19:19:17.748245 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:19:17.842838 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m19:19:17.844406 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:19:17.847084 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m19:19:17.847084 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m19:19:17.848095 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m19:19:17.848095 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table default.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from default.my_first_dbt_model
where id = 1
  
[0m19:19:18.605759 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m19:19:18.605759 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m19:19:18.608272 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_second_dbt_model (execute): 19:19:17.742196 => 19:19:18.608272
[0m19:19:18.608272 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m19:19:18.608272 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m19:19:18.609784 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: Close
[0m19:19:18.620340 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74da4bc6-ff97-46f4-a5b9-9531464c3c0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E25FA770>]}
[0m19:19:18.621350 [info ] [Thread-1 (]: 2 of 2 OK created sql table model default.my_second_dbt_model .................. [[32mOK[0m in 0.88s]
[0m19:19:18.622669 [debug] [Thread-1 (]: Finished running node model.poc_demo.my_second_dbt_model
[0m19:19:18.623677 [debug] [MainThread]: On master: ROLLBACK
[0m19:19:18.624675 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:19:18.681116 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m19:19:18.682114 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:19:18.682114 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:19:18.683113 [debug] [MainThread]: On master: ROLLBACK
[0m19:19:18.683620 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m19:19:18.683620 [debug] [MainThread]: On master: Close
[0m19:19:18.691147 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:19:18.692147 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m19:19:18.693148 [debug] [MainThread]: Connection 'list_None_default' was properly closed.
[0m19:19:18.693148 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m19:19:18.694147 [info ] [MainThread]: 
[0m19:19:18.694147 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 3.05 seconds (3.05s).
[0m19:19:18.696175 [debug] [MainThread]: Command end result
[0m19:19:18.703180 [info ] [MainThread]: 
[0m19:19:18.703180 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:19:18.704182 [info ] [MainThread]: 
[0m19:19:18.704182 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:19:18.705693 [debug] [MainThread]: Command `dbt run` succeeded at 19:19:18.705693 after 4.45 seconds
[0m19:19:18.707787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238DF891B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238DDEC3B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238E2342080>]}
[0m19:19:18.707787 [debug] [MainThread]: Flushing usage events
[0m22:33:31.105834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002127657DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021278C8EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021278C8E9B0>]}


============================== 22:33:31.105834 | c4654adf-2c72-4619-897b-749a376271a6 ==============================
[0m22:33:31.105834 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:33:31.105834 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:33:31.105834 [info ] [MainThread]: dbt version: 1.5.2
[0m22:33:31.105834 [info ] [MainThread]: python version: 3.10.11
[0m22:33:31.121534 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:33:31.121534 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:33:31.121534 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:33:31.121534 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:33:31.121534 [info ] [MainThread]: Configuration:
[0m22:33:31.262969 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:33:31.278614 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:33:31.278614 [info ] [MainThread]: Required dependencies:
[0m22:33:31.278614 [debug] [MainThread]: Executing "git --help"
[0m22:33:31.309930 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:33:31.309930 [debug] [MainThread]: STDERR: "b''"
[0m22:33:31.309930 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:33:31.309930 [info ] [MainThread]: Connection:
[0m22:33:31.309930 [info ] [MainThread]:   host: localhost
[0m22:33:31.309930 [info ] [MainThread]:   port: 10000
[0m22:33:31.325575 [info ] [MainThread]:   cluster: None
[0m22:33:31.325575 [info ] [MainThread]:   endpoint: None
[0m22:33:31.325575 [info ] [MainThread]:   schema: default
[0m22:33:31.325575 [info ] [MainThread]:   organization: 0
[0m22:33:31.325575 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:33:31.325575 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:33:31.325575 [debug] [MainThread]: Using spark connection "debug"
[0m22:33:31.325575 [debug] [MainThread]: On debug: select 1 as id
[0m22:33:31.325575 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:33:35.384352 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m22:33:35.384352 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m22:33:35.384352 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m22:33:35.384352 [info ] [MainThread]: [31m1 check failed:[0m
[0m22:33:35.384352 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m22:33:35.384352 [debug] [MainThread]: Command `dbt debug` failed at 22:33:35.384352 after 4.31 seconds
[0m22:33:35.384352 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:33:35.384352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002127657DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002127903C0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002127903ED10>]}
[0m22:33:35.384352 [debug] [MainThread]: Flushing usage events
[0m22:41:17.522721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C5E2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C853EC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C853EA40>]}


============================== 22:41:17.538444 | 649d638a-1c1f-453d-bca4-fa8d2c066710 ==============================
[0m22:41:17.538444 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:41:17.538444 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:41:17.538444 [info ] [MainThread]: dbt version: 1.5.2
[0m22:41:17.538444 [info ] [MainThread]: python version: 3.10.11
[0m22:41:17.538444 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:41:17.538444 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:41:17.538444 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:41:17.538444 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:41:17.538444 [info ] [MainThread]: Configuration:
[0m22:41:17.632999 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:41:17.648616 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:41:17.648616 [info ] [MainThread]: Required dependencies:
[0m22:41:17.648616 [debug] [MainThread]: Executing "git --help"
[0m22:41:17.680136 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:41:17.680136 [debug] [MainThread]: STDERR: "b''"
[0m22:41:17.680136 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:41:17.680136 [info ] [MainThread]: Connection:
[0m22:41:17.680136 [info ] [MainThread]:   host: localhost
[0m22:41:17.680136 [info ] [MainThread]:   port: 10000
[0m22:41:17.680136 [info ] [MainThread]:   cluster: None
[0m22:41:17.695892 [info ] [MainThread]:   endpoint: None
[0m22:41:17.695892 [info ] [MainThread]:   schema: default
[0m22:41:17.695892 [info ] [MainThread]:   organization: 0
[0m22:41:17.695892 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:41:17.695892 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:41:17.695892 [debug] [MainThread]: Using spark connection "debug"
[0m22:41:17.695892 [debug] [MainThread]: On debug: select 1 as id
[0m22:41:17.695892 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:41:21.878483 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:41:21.878483 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m22:41:21.890494 [debug] [MainThread]: On debug: Close
[0m22:41:21.915565 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:41:21.922570 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:41:21.922570 [debug] [MainThread]: Command `dbt debug` succeeded at 22:41:21.922570 after 4.41 seconds
[0m22:41:21.922570 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:41:21.922570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C5E2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C4FAF8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3C4FAE950>]}
[0m22:41:21.922570 [debug] [MainThread]: Flushing usage events
[0m22:41:51.821155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C456E4DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C45955EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C45955E9B0>]}


============================== 22:41:51.824774 | 4414dbc9-8f6e-4c10-8000-1f72256f1e41 ==============================
[0m22:41:51.824774 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:41:51.824774 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:41:51.824774 [info ] [MainThread]: dbt version: 1.5.2
[0m22:41:51.824774 [info ] [MainThread]: python version: 3.10.11
[0m22:41:51.824774 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:41:51.824774 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:41:51.824774 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:41:51.824774 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:41:51.824774 [info ] [MainThread]: Configuration:
[0m22:41:51.902957 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:41:51.918597 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:41:51.918597 [info ] [MainThread]: Required dependencies:
[0m22:41:51.918597 [debug] [MainThread]: Executing "git --help"
[0m22:41:51.965467 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:41:51.965467 [debug] [MainThread]: STDERR: "b''"
[0m22:41:51.965467 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:41:51.965467 [info ] [MainThread]: Connection:
[0m22:41:51.965467 [info ] [MainThread]:   host: localhost
[0m22:41:51.965467 [info ] [MainThread]:   port: 10000
[0m22:41:51.965467 [info ] [MainThread]:   cluster: None
[0m22:41:51.965467 [info ] [MainThread]:   endpoint: None
[0m22:41:51.965467 [info ] [MainThread]:   schema: default
[0m22:41:51.965467 [info ] [MainThread]:   organization: 0
[0m22:41:51.965467 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:41:51.965467 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:41:51.965467 [debug] [MainThread]: Using spark connection "debug"
[0m22:41:51.965467 [debug] [MainThread]: On debug: select 1 as id
[0m22:41:51.965467 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:41:52.108872 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:41:52.108872 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:41:52.108872 [debug] [MainThread]: On debug: Close
[0m22:41:52.125360 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:41:52.125360 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:41:52.129389 [debug] [MainThread]: Command `dbt debug` succeeded at 22:41:52.129389 after 0.33 seconds
[0m22:41:52.129389 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:41:52.129389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C456E4DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C455FD3A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C455FD0790>]}
[0m22:41:52.129389 [debug] [MainThread]: Flushing usage events
[0m22:44:12.096317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED28EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED4FFEC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED4FFE9E0>]}


============================== 22:44:12.096317 | 6a857a60-4068-42a9-b419-507a9bdf550d ==============================
[0m22:44:12.096317 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:44:12.096317 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:44:12.096317 [info ] [MainThread]: dbt version: 1.5.2
[0m22:44:12.096317 [info ] [MainThread]: python version: 3.10.11
[0m22:44:12.104006 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:44:12.104006 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:44:12.104006 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:44:12.104006 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:44:12.104006 [info ] [MainThread]: Configuration:
[0m22:44:12.196202 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:44:12.213532 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:44:12.216544 [info ] [MainThread]: Required dependencies:
[0m22:44:12.216544 [debug] [MainThread]: Executing "git --help"
[0m22:44:12.257405 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:44:12.257405 [debug] [MainThread]: STDERR: "b''"
[0m22:44:12.257405 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:44:12.257405 [info ] [MainThread]: Connection:
[0m22:44:12.257405 [info ] [MainThread]:   host: localhost
[0m22:44:12.257405 [info ] [MainThread]:   port: 10000
[0m22:44:12.257405 [info ] [MainThread]:   cluster: None
[0m22:44:12.257405 [info ] [MainThread]:   endpoint: None
[0m22:44:12.257405 [info ] [MainThread]:   schema: default
[0m22:44:12.257405 [info ] [MainThread]:   organization: 0
[0m22:44:12.257405 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:44:12.267775 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:44:12.267775 [debug] [MainThread]: Using spark connection "debug"
[0m22:44:12.267775 [debug] [MainThread]: On debug: select 1 as id
[0m22:44:12.267775 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:44:12.279848 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m22:44:12.280369 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m22:44:12.280897 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m22:44:12.281424 [info ] [MainThread]: [31m1 check failed:[0m
[0m22:44:12.282471 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m22:44:12.283527 [debug] [MainThread]: Command `dbt debug` failed at 22:44:12.282999 after 0.20 seconds
[0m22:44:12.283527 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:44:12.284049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED28EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED53AC400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EED53AF220>]}
[0m22:44:12.285086 [debug] [MainThread]: Flushing usage events
[0m00:26:57.712181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592D6ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592FDBEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592FDBEA40>]}


============================== 00:26:57.712181 | 24a764e9-5f09-4121-942f-e2e6568c7465 ==============================
[0m00:26:57.712181 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:26:57.712181 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:26:57.712181 [info ] [MainThread]: dbt version: 1.5.2
[0m00:26:57.722475 [info ] [MainThread]: python version: 3.10.11
[0m00:26:57.722982 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m00:26:57.722982 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m00:26:57.722982 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m00:26:57.722982 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m00:26:57.722982 [info ] [MainThread]: Configuration:
[0m00:26:57.855219 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:26:57.870850 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:26:57.870850 [info ] [MainThread]: Required dependencies:
[0m00:26:57.870850 [debug] [MainThread]: Executing "git --help"
[0m00:26:57.918413 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:26:57.918413 [debug] [MainThread]: STDERR: "b''"
[0m00:26:57.918413 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:26:57.918413 [info ] [MainThread]: Connection:
[0m00:26:57.922610 [info ] [MainThread]:   host: localhost
[0m00:26:57.922610 [info ] [MainThread]:   port: 10000
[0m00:26:57.922610 [info ] [MainThread]:   cluster: None
[0m00:26:57.922610 [info ] [MainThread]:   endpoint: None
[0m00:26:57.922610 [info ] [MainThread]:   schema: default
[0m00:26:57.922610 [info ] [MainThread]:   organization: 0
[0m00:26:57.922610 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:26:57.922610 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m00:26:57.922610 [debug] [MainThread]: Using spark connection "debug"
[0m00:26:57.922610 [debug] [MainThread]: On debug: select 1 as id
[0m00:26:57.922610 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:27:02.277373 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m00:27:02.285892 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m00:27:02.288908 [debug] [MainThread]: On debug: Close
[0m00:27:02.328343 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:27:02.332864 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:27:02.334425 [debug] [MainThread]: Command `dbt debug` succeeded at 00:27:02.334425 after 4.64 seconds
[0m00:27:02.335932 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:27:02.335932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592D6ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592C8378E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002592C836950>]}
[0m00:27:02.335932 [debug] [MainThread]: Flushing usage events
[0m00:33:47.268688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BF8FEDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BFB6FEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BFB6FE9B0>]}


============================== 00:33:47.268688 | 7fb0e9e1-da31-4bd0-bab7-66ec17110c84 ==============================
[0m00:33:47.268688 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:33:47.274922 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:33:47.274922 [info ] [MainThread]: dbt version: 1.5.2
[0m00:33:47.274922 [info ] [MainThread]: python version: 3.10.11
[0m00:33:47.274922 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m00:33:47.274922 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m00:33:47.274922 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m00:33:47.283652 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m00:33:47.283652 [info ] [MainThread]: Configuration:
[0m00:33:47.408487 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:33:47.432050 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:33:47.432050 [info ] [MainThread]: Required dependencies:
[0m00:33:47.432050 [debug] [MainThread]: Executing "git --help"
[0m00:33:47.479605 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:33:47.479605 [debug] [MainThread]: STDERR: "b''"
[0m00:33:47.479605 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:33:47.479605 [info ] [MainThread]: Connection:
[0m00:33:47.479605 [info ] [MainThread]:   host: localhost
[0m00:33:47.479605 [info ] [MainThread]:   port: 10000
[0m00:33:47.479605 [info ] [MainThread]:   cluster: None
[0m00:33:47.479605 [info ] [MainThread]:   endpoint: None
[0m00:33:47.479605 [info ] [MainThread]:   schema: default
[0m00:33:47.479605 [info ] [MainThread]:   organization: 0
[0m00:33:47.479605 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:33:47.479605 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m00:33:47.479605 [debug] [MainThread]: Using spark connection "debug"
[0m00:33:47.479605 [debug] [MainThread]: On debug: select 1 as id
[0m00:33:47.495243 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:33:52.094758 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m00:33:52.094758 [debug] [MainThread]: SQL status: OK in 5.0 seconds
[0m00:33:52.097789 [debug] [MainThread]: On debug: Close
[0m00:33:52.128805 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:33:52.128805 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:33:52.132814 [debug] [MainThread]: Command `dbt debug` succeeded at 00:33:52.132814 after 4.88 seconds
[0m00:33:52.133336 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:33:52.133336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BF8FEDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BF8163A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BF8160790>]}
[0m00:33:52.135358 [debug] [MainThread]: Flushing usage events
[0m00:46:21.945151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C6C7DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C9392CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C9392A10>]}


============================== 00:46:21.945151 | 3d63027e-d82a-4b8a-a366-059dc6e91144 ==============================
[0m00:46:21.945151 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:46:21.953187 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:46:21.953187 [info ] [MainThread]: dbt version: 1.5.2
[0m00:46:21.953187 [info ] [MainThread]: python version: 3.10.11
[0m00:46:21.953187 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m00:46:21.953187 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m00:46:21.953187 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m00:46:21.961168 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m00:46:21.961168 [info ] [MainThread]: Configuration:
[0m00:46:22.090760 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:46:22.108249 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:46:22.108249 [info ] [MainThread]: Required dependencies:
[0m00:46:22.114772 [debug] [MainThread]: Executing "git --help"
[0m00:46:22.172348 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:46:22.172348 [debug] [MainThread]: STDERR: "b''"
[0m00:46:22.172348 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:46:22.180349 [info ] [MainThread]: Connection:
[0m00:46:22.180349 [info ] [MainThread]:   host: localhost
[0m00:46:22.180349 [info ] [MainThread]:   port: 10000
[0m00:46:22.180349 [info ] [MainThread]:   cluster: None
[0m00:46:22.180349 [info ] [MainThread]:   endpoint: None
[0m00:46:22.180349 [info ] [MainThread]:   schema: default
[0m00:46:22.188528 [info ] [MainThread]:   organization: 0
[0m00:46:22.189038 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:46:22.189038 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m00:46:22.189038 [debug] [MainThread]: Using spark connection "debug"
[0m00:46:22.189038 [debug] [MainThread]: On debug: select 1 as id
[0m00:46:22.189038 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:46:22.455471 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m00:46:22.463488 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m00:46:22.464502 [debug] [MainThread]: On debug: Close
[0m00:46:22.479888 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:46:22.486450 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:46:22.488235 [debug] [MainThread]: Command `dbt debug` succeeded at 00:46:22.488235 after 0.56 seconds
[0m00:46:22.488235 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:46:22.490851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C93929B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C5E04340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258C973DB10>]}
[0m00:46:22.490851 [debug] [MainThread]: Flushing usage events
[0m00:52:52.596717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209A808DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AA7A2E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AA7A2B90>]}


============================== 00:52:52.603028 | 003d60b0-4ad1-495e-b9f8-c0e36dc8dd66 ==============================
[0m00:52:52.603028 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:52:52.603028 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:52:52.739036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '003d60b0-4ad1-495e-b9f8-c0e36dc8dd66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AA7A2E00>]}
[0m00:52:52.756297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '003d60b0-4ad1-495e-b9f8-c0e36dc8dd66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AAB49EA0>]}
[0m00:52:52.756297 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:52:52.761597 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:52:52.770633 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:52:52.771846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '003d60b0-4ad1-495e-b9f8-c0e36dc8dd66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AAB4A4D0>]}
[0m00:52:53.629612 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:52:53.645245 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:52:53.645245 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:52:53.645245 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:52:53.661269 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:52:53.676949 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:52:53.693265 [debug] [MainThread]: Command `dbt run` failed at 00:52:53.693265 after 1.11 seconds
[0m00:52:53.693265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209A808DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AAD2BEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209AAD2B0A0>]}
[0m00:52:53.693265 [debug] [MainThread]: Flushing usage events
[0m00:54:20.856482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF8185DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF83F22BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF83F22950>]}


============================== 00:54:20.859813 | bfd46d57-0f99-43a1-832b-29e6e842109f ==============================
[0m00:54:20.859813 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:54:20.861831 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:54:20.862630 [info ] [MainThread]: dbt version: 1.5.2
[0m00:54:20.863192 [info ] [MainThread]: python version: 3.10.11
[0m00:54:20.864564 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m00:54:20.864564 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m00:54:20.864564 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m00:54:20.864564 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m00:54:20.864564 [info ] [MainThread]: Configuration:
[0m00:54:21.005100 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:54:21.023231 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:54:21.029314 [info ] [MainThread]: Required dependencies:
[0m00:54:21.029314 [debug] [MainThread]: Executing "git --help"
[0m00:54:21.077842 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:54:21.077842 [debug] [MainThread]: STDERR: "b''"
[0m00:54:21.077842 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:54:21.085841 [info ] [MainThread]: Connection:
[0m00:54:21.085841 [info ] [MainThread]:   host: localhost
[0m00:54:21.085841 [info ] [MainThread]:   port: 10000
[0m00:54:21.085841 [info ] [MainThread]:   cluster: None
[0m00:54:21.085841 [info ] [MainThread]:   endpoint: None
[0m00:54:21.094224 [info ] [MainThread]:   schema: ndb
[0m00:54:21.094224 [info ] [MainThread]:   organization: 0
[0m00:54:21.094224 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:54:21.094224 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m00:54:21.094224 [debug] [MainThread]: Using spark connection "debug"
[0m00:54:21.102218 [debug] [MainThread]: On debug: select 1 as id
[0m00:54:21.102218 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:54:21.342298 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m00:54:21.343177 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m00:54:21.343177 [debug] [MainThread]: On debug: Close
[0m00:54:21.395656 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:54:21.395656 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:54:21.401189 [debug] [MainThread]: Command `dbt debug` succeeded at 00:54:21.395656 after 0.57 seconds
[0m00:54:21.401189 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:54:21.402744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF8185DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF809F39A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF809F0940>]}
[0m00:54:21.402744 [debug] [MainThread]: Flushing usage events
[0m00:54:27.489228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF08FADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0B6C2DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0B6C2B90>]}


============================== 00:54:27.493861 | b2d1ccec-208f-4e57-84dc-6f5d2da73403 ==============================
[0m00:54:27.493861 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:54:27.493861 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:54:27.638071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2d1ccec-208f-4e57-84dc-6f5d2da73403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0B6C2DA0>]}
[0m00:54:27.638071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2d1ccec-208f-4e57-84dc-6f5d2da73403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0BA65DB0>]}
[0m00:54:27.638071 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:54:27.653886 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:54:27.653886 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:54:27.653886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b2d1ccec-208f-4e57-84dc-6f5d2da73403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0BA66470>]}
[0m00:54:28.534767 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:54:28.549775 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:54:28.565509 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:54:28.565509 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:54:28.581035 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:54:28.613022 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:54:28.615177 [debug] [MainThread]: Command `dbt run` failed at 00:54:28.615177 after 1.14 seconds
[0m00:54:28.615177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF08FADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0BC4BE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF0BC4B040>]}
[0m00:54:28.615177 [debug] [MainThread]: Flushing usage events
[0m00:57:29.671516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712CBDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F2F2E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F2F2C20>]}


============================== 00:57:29.671516 | 56a32630-4ce4-4499-83c1-a51bb678ef6b ==============================
[0m00:57:29.671516 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:57:29.671516 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:57:29.801240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '56a32630-4ce4-4499-83c1-a51bb678ef6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F2F2E30>]}
[0m00:57:29.823452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '56a32630-4ce4-4499-83c1-a51bb678ef6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F695ED0>]}
[0m00:57:29.823452 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:57:29.832965 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:57:29.832965 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:57:29.832965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '56a32630-4ce4-4499-83c1-a51bb678ef6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F696530>]}
[0m00:57:30.663940 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:57:30.675976 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:57:30.675976 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:57:30.675976 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:57:30.706629 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:57:30.723355 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:57:30.738803 [debug] [MainThread]: Command `dbt run` failed at 00:57:30.738803 after 1.08 seconds
[0m00:57:30.738803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712CBDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F87BEE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002712F87B0D0>]}
[0m00:57:30.738803 [debug] [MainThread]: Flushing usage events
[0m00:57:41.987744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC201DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4732E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4732BC0>]}


============================== 00:57:41.995263 | cd05ff52-62d7-422f-a5a2-e24f52ddd46f ==============================
[0m00:57:41.995263 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:57:41.995263 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:57:42.107354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd05ff52-62d7-422f-a5a2-e24f52ddd46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4732E30>]}
[0m00:57:42.107354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd05ff52-62d7-422f-a5a2-e24f52ddd46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4AE1ED0>]}
[0m00:57:42.115912 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:57:42.115912 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:57:42.115912 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:57:42.115912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cd05ff52-62d7-422f-a5a2-e24f52ddd46f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4AE2500>]}
[0m00:57:42.753631 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:57:42.769272 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:57:42.769272 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:57:42.769272 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:57:42.800643 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:57:42.816394 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:57:42.816394 [debug] [MainThread]: Command `dbt run` failed at 00:57:42.816394 after 0.84 seconds
[0m00:57:42.816394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC201DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4CBBEE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DC4CBB0D0>]}
[0m00:57:42.816394 [debug] [MainThread]: Flushing usage events
[0m00:58:13.190434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F6FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C541E12C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C541E12980>]}


============================== 00:58:13.190434 | 82bb6600-006e-43a3-ae58-43b5d06f18d3 ==============================
[0m00:58:13.190434 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:58:13.196948 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:58:13.196948 [info ] [MainThread]: dbt version: 1.5.2
[0m00:58:13.198040 [info ] [MainThread]: python version: 3.10.11
[0m00:58:13.198040 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m00:58:13.198040 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m00:58:13.198040 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m00:58:13.198040 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m00:58:13.198040 [info ] [MainThread]: Configuration:
[0m00:58:13.317944 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:58:13.342054 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:58:13.342054 [info ] [MainThread]: Required dependencies:
[0m00:58:13.349517 [debug] [MainThread]: Executing "git --help"
[0m00:58:13.399478 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:58:13.399478 [debug] [MainThread]: STDERR: "b''"
[0m00:58:13.399478 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:58:13.404629 [info ] [MainThread]: Connection:
[0m00:58:13.404629 [info ] [MainThread]:   host: localhost
[0m00:58:13.404629 [info ] [MainThread]:   port: 10000
[0m00:58:13.407635 [info ] [MainThread]:   cluster: None
[0m00:58:13.407635 [info ] [MainThread]:   endpoint: None
[0m00:58:13.407635 [info ] [MainThread]:   schema: ndb
[0m00:58:13.407635 [info ] [MainThread]:   organization: 0
[0m00:58:13.407635 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:58:13.407635 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m00:58:13.415665 [debug] [MainThread]: Using spark connection "debug"
[0m00:58:13.415665 [debug] [MainThread]: On debug: select 1 as id
[0m00:58:13.415665 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:58:13.647298 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m00:58:13.649808 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m00:58:13.649808 [debug] [MainThread]: On debug: Close
[0m00:58:13.664536 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:58:13.664536 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:58:13.664536 [debug] [MainThread]: Command `dbt debug` succeeded at 00:58:13.664536 after 0.50 seconds
[0m00:58:13.664536 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:58:13.664536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F6FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53E8839D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53E8809A0>]}
[0m00:58:13.664536 [debug] [MainThread]: Flushing usage events
[0m00:59:00.103974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF10ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF37C2CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF37C2A10>]}


============================== 00:59:00.103974 | 9f813c94-b4fc-4a16-bd11-6c44341b488d ==============================
[0m00:59:00.103974 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:59:00.103974 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:59:00.130933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f813c94-b4fc-4a16-bd11-6c44341b488d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF37C2D10>]}
[0m00:59:00.136441 [debug] [MainThread]: Command `dbt clean` succeeded at 00:59:00.136441 after 0.05 seconds
[0m00:59:00.136441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF10ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF37C2D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CF37C2AA0>]}
[0m00:59:00.136441 [debug] [MainThread]: Flushing usage events
[0m00:59:11.297626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A03E8ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A040FC2D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A040FC2AA0>]}


============================== 00:59:11.306211 | a42765c3-8ab4-4b7f-9ad7-820241fc89bf ==============================
[0m00:59:11.306211 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:59:11.306211 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:59:11.431050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a42765c3-8ab4-4b7f-9ad7-820241fc89bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A040FC2D10>]}
[0m00:59:11.450479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a42765c3-8ab4-4b7f-9ad7-820241fc89bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A041375ED0>]}
[0m00:59:11.450479 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:59:11.458495 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:59:11.458495 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:59:11.458495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a42765c3-8ab4-4b7f-9ad7-820241fc89bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A041376B00>]}
[0m00:59:12.250053 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:59:12.266174 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:59:12.266174 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:59:12.266174 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:59:12.297437 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:59:12.321528 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:59:12.321528 [debug] [MainThread]: Command `dbt run` failed at 00:59:12.321528 after 1.04 seconds
[0m00:59:12.329553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A03E8ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A041557DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A041556FE0>]}
[0m00:59:12.329553 [debug] [MainThread]: Flushing usage events
[0m00:59:47.938984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205D0CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205F7E2E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205F7E2BF0>]}


============================== 00:59:47.940996 | 4cea8cab-6179-456d-8a08-2305252a446f ==============================
[0m00:59:47.940996 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:59:47.940996 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:59:48.077871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4cea8cab-6179-456d-8a08-2305252a446f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205F7E2E00>]}
[0m00:59:48.077871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4cea8cab-6179-456d-8a08-2305252a446f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205FB89ED0>]}
[0m00:59:48.077871 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:59:48.093505 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:59:48.093505 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:59:48.093505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4cea8cab-6179-456d-8a08-2305252a446f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205FB8AB30>]}
[0m00:59:48.883354 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m00:59:48.898361 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m00:59:48.914085 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m00:59:48.915628 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m00:59:48.945787 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'ndb' in the 'models' section of file 'models\example\schema.yml'
[0m00:59:48.969529 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m00:59:48.977547 [debug] [MainThread]: Command `dbt run` failed at 00:59:48.977547 after 1.06 seconds
[0m00:59:48.977547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205D0CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205FD67EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002205FD670A0>]}
[0m00:59:48.977547 [debug] [MainThread]: Flushing usage events
[0m01:00:48.301377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECBE58DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC0CA2E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC0CA2B60>]}


============================== 01:00:48.304635 | e84897e9-e19e-489a-a645-82db30051c0f ==============================
[0m01:00:48.304635 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:00:48.306642 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:00:48.437424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC0CA2DD0>]}
[0m01:00:48.437424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC1045EA0>]}
[0m01:00:48.437424 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:00:48.452326 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:00:48.452326 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m01:00:48.452326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC10467D0>]}
[0m01:00:49.260254 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:00:49.280831 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:00:49.286136 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:00:49.292287 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:00:49.444376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC1040340>]}
[0m01:00:49.451838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC10400D0>]}
[0m01:00:49.455082 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:00:49.455082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC1040310>]}
[0m01:00:49.455082 [info ] [MainThread]: 
[0m01:00:49.460063 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:00:49.461280 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:00:49.477004 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:00:49.477004 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:00:49.477004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:49.804321 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:00:49.804321 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:00:49.956992 [debug] [ThreadPool]: On list_schemas: Close
[0m01:00:49.981640 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__ndb)
[0m01:00:49.981640 [debug] [ThreadPool]: Creating schema "schema: "ndb"
"
[0m01:00:49.997281 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:00:49.997281 [debug] [ThreadPool]: Using spark connection "create__ndb"
[0m01:00:49.997281 [debug] [ThreadPool]: On create__ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "create__ndb"} */
create schema if not exists ndb
  
[0m01:00:49.997281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:00:50.469794 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:00:50.473316 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:00:50.473316 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m01:00:50.473316 [debug] [ThreadPool]: On create__ndb: ROLLBACK
[0m01:00:50.473316 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:00:50.473316 [debug] [ThreadPool]: On create__ndb: Close
[0m01:00:50.497582 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:00:50.503983 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:00:50.503983 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:00:50.503983 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:00:50.511995 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:50.746716 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:00:50.749240 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:00:50.761359 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:00:50.761359 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:00:50.763383 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:00:50.780767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC1228550>]}
[0m01:00:50.785276 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:00:50.785276 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:00:50.785276 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:50.785276 [info ] [MainThread]: 
[0m01:00:50.804786 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:00:50.804786 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:00:50.813815 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:00:50.813815 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:00:50.820370 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:00:50.824951 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:00:50.813815 => 01:00:50.824951
[0m01:00:50.824951 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:00:50.948848 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:00:50.953746 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:00:50.953746 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:00:50.953746 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:00:50.953746 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:00:51.388452 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:00:51.396970 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:00:51.396970 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:00:51.396970 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:00:51.401524 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:00:50.828459 => 01:00:51.401007
[0m01:00:51.401524 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:00:51.401524 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:00:51.401524 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:00:51.554924 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:00:51.557454 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e84897e9-e19e-489a-a645-82db30051c0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC1154D60>]}
[0m01:00:51.557454 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.75s]
[0m01:00:51.557454 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:00:51.557454 [debug] [MainThread]: On master: ROLLBACK
[0m01:00:51.557454 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:00:51.689416 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:00:51.689416 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:00:51.689416 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:00:51.689416 [debug] [MainThread]: On master: ROLLBACK
[0m01:00:51.693940 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:00:51.693940 [debug] [MainThread]: On master: Close
[0m01:00:51.709570 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:51.709570 [debug] [MainThread]: Connection 'create__ndb' was properly closed.
[0m01:00:51.709570 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:00:51.712585 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:00:51.713109 [info ] [MainThread]: 
[0m01:00:51.713109 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.26 seconds (2.26s).
[0m01:00:51.713109 [debug] [MainThread]: Command end result
[0m01:00:51.728767 [info ] [MainThread]: 
[0m01:00:51.731540 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:00:51.734144 [info ] [MainThread]: 
[0m01:00:51.737039 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:00:51.737949 [error] [MainThread]:   Database Error
[0m01:00:51.738972 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:00:51.741673 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:00:51.741673 [error] [MainThread]:     +- 'Project [*]
[0m01:00:51.744183 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:00:51.745211 [error] [MainThread]:     
[0m01:00:51.747253 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:00:51.749973 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:00:51.755571 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:00:51.758087 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:00:51.762127 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:00:51.762127 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:00:51.764154 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:00:51.764154 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:00:51.764154 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:00:51.770703 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:00:51.772237 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:00:51.772237 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:00:51.772237 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:00:51.772237 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:00:51.777757 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:00:51.780899 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:00:51.786425 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:00:51.789452 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:00:51.789452 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:00:51.789452 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:00:51.789452 [error] [MainThread]:     +- 'Project [*]
[0m01:00:51.797193 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:00:51.802210 [error] [MainThread]:     
[0m01:00:51.806299 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:00:51.808874 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:00:51.810431 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:00:51.812939 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:00:51.815108 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:00:51.818633 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:00:51.818633 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:00:51.818633 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:00:51.818633 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:00:51.825644 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:00:51.852841 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:00:51.858862 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:00:51.858862 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:00:51.858862 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:00:51.866949 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:00:51.868706 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:00:51.868706 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:00:51.868706 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:00:51.875250 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:00:51.875250 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:00:51.875250 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:00:51.880115 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:00:51.880115 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:00:51.880115 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:00:51.883650 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:00:51.883650 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:00:51.888584 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:00:51.889602 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:00:51.891203 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:00:51.891203 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:00:51.891203 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:00:51.891203 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:00:51.891203 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:00:51.899220 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:00:51.901120 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:00:51.901120 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:00:51.901120 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:00:51.901120 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:00:51.907638 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:00:51.907638 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:00:51.907638 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:00:51.910658 [error] [MainThread]:     	... 16 more
[0m01:00:51.910658 [error] [MainThread]:     
[0m01:00:51.910658 [info ] [MainThread]: 
[0m01:00:51.910658 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:00:51.915686 [debug] [MainThread]: Command `dbt run` failed at 01:00:51.915686 after 3.63 seconds
[0m01:00:51.915686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECBE58DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC10DEFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ECC10DCBE0>]}
[0m01:00:51.915686 [debug] [MainThread]: Flushing usage events
[0m01:03:40.354200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C263EDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C28B02E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C28B02B90>]}


============================== 01:03:40.354200 | 869cfe19-e7dc-4ad1-8941-5935c61748da ==============================
[0m01:03:40.354200 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:03:40.361226 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:03:40.541034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '869cfe19-e7dc-4ad1-8941-5935c61748da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C28B02E00>]}
[0m01:03:40.559660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '869cfe19-e7dc-4ad1-8941-5935c61748da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C28EA5EA0>]}
[0m01:03:40.563186 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:03:40.582462 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:03:40.700208 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:03:40.704244 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\schema.yml
[0m01:03:40.734707 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:03:40.759519 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:03:40.787841 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_cow' (models\example\hudi_cow.sql) depends on a source named 'ndb.ntable' which was not found
[0m01:03:40.787841 [debug] [MainThread]: Command `dbt run` failed at 01:03:40.787841 after 0.45 seconds
[0m01:03:40.787841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C263EDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C290557B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C290563B0>]}
[0m01:03:40.787841 [debug] [MainThread]: Flushing usage events
[0m01:04:09.843210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADDFA7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE2192E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE2192B60>]}


============================== 01:04:09.843210 | bde34c88-d45f-47b9-a35a-71c08324b0ce ==============================
[0m01:04:09.843210 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:04:09.843210 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:04:09.996096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bde34c88-d45f-47b9-a35a-71c08324b0ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE2192DD0>]}
[0m01:04:10.011754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bde34c88-d45f-47b9-a35a-71c08324b0ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE2539EA0>]}
[0m01:04:10.011754 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:04:10.028267 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:04:10.108069 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m01:04:10.108069 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\schema.yml
[0m01:04:10.108069 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\hudi_cow.sql
[0m01:04:10.124069 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:04:10.135998 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:04:10.148008 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.poc_demo.hudi_mor' (models\example\hudi_mor.sql) depends on a source named 'ndb.ntable' which was not found
[0m01:04:10.148008 [debug] [MainThread]: Command `dbt run` failed at 01:04:10.148008 after 0.33 seconds
[0m01:04:10.148008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADDFA7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE26ED300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ADE26ED120>]}
[0m01:04:10.148008 [debug] [MainThread]: Flushing usage events
[0m01:05:47.142003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221086ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210ADC2E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210ADC2BC0>]}


============================== 01:05:47.160310 | 1621782c-9297-42cf-9b88-eddc26d0182c ==============================
[0m01:05:47.160310 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:05:47.160310 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:05:47.292849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210ADC2DD0>]}
[0m01:05:47.302354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B175EA0>]}
[0m01:05:47.302354 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:05:47.302354 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:05:47.374587 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:05:47.382611 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:05:47.382611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B33C0D0>]}
[0m01:05:47.398980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B2737C0>]}
[0m01:05:47.398980 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:05:47.398980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B273760>]}
[0m01:05:47.398980 [info ] [MainThread]: 
[0m01:05:47.398980 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:05:47.398980 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:05:47.418411 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:05:47.418411 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:05:47.418411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:05:47.577239 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:05:47.577239 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:05:47.594285 [debug] [ThreadPool]: On list_schemas: Close
[0m01:05:47.610406 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:05:47.617441 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:05:47.618447 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:05:47.618447 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:05:47.618447 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:05:47.753253 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:05:47.753253 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:05:47.764295 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:05:47.764295 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:05:47.764295 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:05:47.782479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B37B2B0>]}
[0m01:05:47.783845 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:05:47.784920 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:05:47.786616 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:05:47.787677 [info ] [MainThread]: 
[0m01:05:47.795256 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:05:47.796296 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:05:47.799009 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:05:47.799515 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:05:47.807359 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:05:47.811234 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:05:47.800563 => 01:05:47.807359
[0m01:05:47.811234 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:05:47.974759 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:05:47.977168 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:05:47.978221 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:05:47.978766 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:05:47.979304 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:05:48.096091 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:05:48.098312 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:05:48.098312 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:05:48.098312 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:05:48.102897 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:05:47.815262 => 01:05:48.102897
[0m01:05:48.103896 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:05:48.103896 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:05:48.103896 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:05:48.129934 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:05:48.134949 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1621782c-9297-42cf-9b88-eddc26d0182c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B3243A0>]}
[0m01:05:48.134949 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.34s]
[0m01:05:48.134949 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:05:48.134949 [debug] [MainThread]: On master: ROLLBACK
[0m01:05:48.141380 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:05:48.224218 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:05:48.224218 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:05:48.224218 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:05:48.224218 [debug] [MainThread]: On master: ROLLBACK
[0m01:05:48.224218 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:05:48.224218 [debug] [MainThread]: On master: Close
[0m01:05:48.241415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:05:48.242411 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:05:48.242411 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:05:48.242411 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:05:48.242411 [info ] [MainThread]: 
[0m01:05:48.242411 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.84 seconds (0.84s).
[0m01:05:48.242411 [debug] [MainThread]: Command end result
[0m01:05:48.251659 [info ] [MainThread]: 
[0m01:05:48.256700 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:05:48.256700 [info ] [MainThread]: 
[0m01:05:48.260253 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:05:48.260253 [error] [MainThread]:   Database Error
[0m01:05:48.260253 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:05:48.260253 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:05:48.260253 [error] [MainThread]:     +- 'Project [*]
[0m01:05:48.264785 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:05:48.264785 [error] [MainThread]:     
[0m01:05:48.264785 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:05:48.268682 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:05:48.270822 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:05:48.274321 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:05:48.276845 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:05:48.278531 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:05:48.279568 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:05:48.280527 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:05:48.280527 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:05:48.280527 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:05:48.280527 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:05:48.280527 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:05:48.280527 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:05:48.288109 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:05:48.288109 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:05:48.288109 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:05:48.288109 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:05:48.288109 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:05:48.296593 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:05:48.296593 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:05:48.296593 [error] [MainThread]:     +- 'Project [*]
[0m01:05:48.296593 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:05:48.296593 [error] [MainThread]:     
[0m01:05:48.296593 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:05:48.296593 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:05:48.304619 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:05:48.304619 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:05:48.304619 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:05:48.304619 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:05:48.304619 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:05:48.312632 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:05:48.312632 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:05:48.312632 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:05:48.312632 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:05:48.312632 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:05:48.320668 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:05:48.320668 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:05:48.320668 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:05:48.328685 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:05:48.328685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:05:48.328685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:05:48.328685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:05:48.328685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:05:48.328685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:05:48.336685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:05:48.336685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:05:48.336685 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:05:48.336685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:05:48.344691 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:05:48.352785 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:05:48.352785 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:05:48.352785 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:05:48.352785 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:05:48.352785 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:05:48.360798 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:05:48.360798 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:05:48.360798 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:05:48.360798 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:05:48.360798 [error] [MainThread]:     	... 16 more
[0m01:05:48.368797 [error] [MainThread]:     
[0m01:05:48.368797 [info ] [MainThread]: 
[0m01:05:48.368797 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:05:48.368797 [debug] [MainThread]: Command `dbt run` failed at 01:05:48.368797 after 1.23 seconds
[0m01:05:48.368797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221086ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B324EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002210B3243A0>]}
[0m01:05:48.368797 [debug] [MainThread]: Flushing usage events
[0m01:06:06.229795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D04DC2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D050342E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D050342BF0>]}


============================== 01:06:06.229795 | 1d8e5ddc-5a43-4fcb-8c38-00f6b2cd5369 ==============================
[0m01:06:06.229795 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:06:06.229795 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:06:06.378365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1d8e5ddc-5a43-4fcb-8c38-00f6b2cd5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D050342E00>]}
[0m01:06:06.386483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1d8e5ddc-5a43-4fcb-8c38-00f6b2cd5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0506F1EA0>]}
[0m01:06:06.386483 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:06:06.402133 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:06:06.475501 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:06:06.475501 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\schema.yml
[0m01:06:06.491501 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:06:06.512308 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:06:06.527606 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models\example\schema.yml @ sources: {'name': 'ndb', 'schema': 'ndb', 'tables': [{'name': 'ntable'}, {'identifier': 'ndb.ntable'}]} - at path ['tables'][1]: 'name' is a required property
[0m01:06:06.527606 [debug] [MainThread]: Command `dbt run` failed at 01:06:06.527606 after 0.32 seconds
[0m01:06:06.527606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D04DC2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D050897C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0508971F0>]}
[0m01:06:06.527606 [debug] [MainThread]: Flushing usage events
[0m01:06:27.726249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014466A3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014469152D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014469152B00>]}


============================== 01:06:27.729098 | a33323a1-a0ae-48cc-a60a-300368ed498f ==============================
[0m01:06:27.729098 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:06:27.729098 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:06:27.903463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014469152D10>]}
[0m01:06:27.915088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144694F9EA0>]}
[0m01:06:27.916146 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:06:27.942832 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:06:28.032920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:06:28.032920 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:06:28.040953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144696BC0D0>]}
[0m01:06:28.056462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144695EF700>]}
[0m01:06:28.056462 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:06:28.056462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144695EF6A0>]}
[0m01:06:28.056462 [info ] [MainThread]: 
[0m01:06:28.056462 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:06:28.064471 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:06:28.072910 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:06:28.078416 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:06:28.079381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:06:28.232793 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:06:28.232793 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:06:28.242658 [debug] [ThreadPool]: On list_schemas: Close
[0m01:06:28.258595 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:06:28.268071 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:06:28.269726 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:06:28.270262 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:06:28.270786 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:06:28.417870 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:06:28.418849 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:06:28.426986 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:06:28.429024 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:06:28.429024 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:06:28.447855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144696F72B0>]}
[0m01:06:28.449241 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:06:28.449734 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:06:28.450243 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:06:28.451969 [info ] [MainThread]: 
[0m01:06:28.457855 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:06:28.457855 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:06:28.462089 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:06:28.463043 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:06:28.465561 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:06:28.473741 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:06:28.463043 => 01:06:28.465561
[0m01:06:28.476177 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:06:28.588038 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:06:28.588038 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:06:28.590323 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:06:28.591115 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:06:28.591620 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:06:28.713651 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:06:28.719177 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:06:28.720869 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:06:28.721895 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:06:28.721895 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:06:28.476910 => 01:06:28.721895
[0m01:06:28.721895 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:06:28.721895 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:06:28.721895 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:06:28.745849 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:06:28.747360 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a33323a1-a0ae-48cc-a60a-300368ed498f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144696A42E0>]}
[0m01:06:28.747360 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.29s]
[0m01:06:28.747360 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:06:28.747360 [debug] [MainThread]: On master: ROLLBACK
[0m01:06:28.754376 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:06:28.835062 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:06:28.835062 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:06:28.835062 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:06:28.835062 [debug] [MainThread]: On master: ROLLBACK
[0m01:06:28.835062 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:06:28.835062 [debug] [MainThread]: On master: Close
[0m01:06:28.850937 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:06:28.855571 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:06:28.855571 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:06:28.855571 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:06:28.857584 [info ] [MainThread]: 
[0m01:06:28.859102 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.80 seconds (0.80s).
[0m01:06:28.859102 [debug] [MainThread]: Command end result
[0m01:06:28.867143 [info ] [MainThread]: 
[0m01:06:28.870151 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:06:28.870151 [info ] [MainThread]: 
[0m01:06:28.870151 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:06:28.870151 [error] [MainThread]:   Database Error
[0m01:06:28.870151 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:06:28.875169 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:06:28.875169 [error] [MainThread]:     +- 'Project [*]
[0m01:06:28.875169 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:06:28.875169 [error] [MainThread]:     
[0m01:06:28.875169 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:06:28.879408 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:06:28.879408 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:06:28.879408 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:06:28.882494 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:06:28.883501 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:06:28.883501 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:06:28.883501 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:06:28.883501 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:06:28.883501 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:06:28.883501 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:06:28.883501 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:06:28.891531 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:06:28.891531 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:06:28.891531 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:06:28.891531 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:06:28.891531 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:06:28.891531 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:06:28.899514 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:06:28.899514 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:06:28.899514 [error] [MainThread]:     +- 'Project [*]
[0m01:06:28.899514 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:06:28.899514 [error] [MainThread]:     
[0m01:06:28.899514 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:06:28.907843 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:06:28.907843 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:06:28.907843 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:06:28.907843 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:06:28.907843 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:06:28.915856 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:06:28.915856 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:06:28.924204 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:06:28.930321 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:06:28.932332 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:06:28.932332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:06:28.932332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:06:28.932332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:06:28.932332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:06:28.932332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:06:28.940735 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:06:28.940735 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:06:28.940735 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:06:28.948749 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:06:28.957142 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:06:28.957142 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:06:28.957142 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:06:28.957142 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:06:28.957142 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:06:28.965497 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:06:28.965497 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:06:28.965497 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:06:28.965497 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:06:28.965497 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:06:28.965497 [error] [MainThread]:     	... 16 more
[0m01:06:28.965497 [error] [MainThread]:     
[0m01:06:28.965497 [info ] [MainThread]: 
[0m01:06:28.973992 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:06:28.973992 [debug] [MainThread]: Command `dbt run` failed at 01:06:28.973992 after 1.27 seconds
[0m01:06:28.973992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014466A3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144696A4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144696A4DF0>]}
[0m01:06:28.979002 [debug] [MainThread]: Flushing usage events
[0m01:08:05.886891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024051A2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054142DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054142B90>]}


============================== 01:08:05.891999 | f6a465d1-be5a-4e30-9150-7d0e954c568d ==============================
[0m01:08:05.891999 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:08:05.891999 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:08:06.032553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054142DA0>]}
[0m01:08:06.032553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240544E1DB0>]}
[0m01:08:06.044561 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:08:06.044561 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:08:06.124357 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:08:06.124357 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:08:06.132870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240546AC0D0>]}
[0m01:08:06.140879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240545DF790>]}
[0m01:08:06.140879 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:08:06.140879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240545DF730>]}
[0m01:08:06.149160 [info ] [MainThread]: 
[0m01:08:06.149160 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:08:06.149160 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:08:06.157405 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:08:06.157405 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:08:06.157405 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:08:06.325169 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:08:06.327186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:08:06.334897 [debug] [ThreadPool]: On list_schemas: Close
[0m01:08:06.352738 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:08:06.359139 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:08:06.359139 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:08:06.367149 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:08:06.367149 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:08:06.502347 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:08:06.502347 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:08:06.509886 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:08:06.512401 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:08:06.512401 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:08:06.524261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240546EB2B0>]}
[0m01:08:06.524261 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:08:06.524261 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:08:06.524261 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:08:06.524261 [info ] [MainThread]: 
[0m01:08:06.536813 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:08:06.537837 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:08:06.538866 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:08:06.538866 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:08:06.550413 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:08:06.550413 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:08:06.538866 => 01:08:06.550413
[0m01:08:06.550413 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:08:06.652851 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:08:06.652851 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:08:06.652851 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:08:06.652851 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:08:06.652851 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:08:06.778317 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:08:06.778317 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:08:06.778317 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:08:06.778317 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:08:06.778317 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:08:06.550413 => 01:08:06.778317
[0m01:08:06.786600 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:08:06.786600 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:08:06.786600 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:08:06.806078 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:08:06.806078 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6a465d1-be5a-4e30-9150-7d0e954c568d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054694370>]}
[0m01:08:06.809598 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.27s]
[0m01:08:06.809598 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:08:06.809598 [debug] [MainThread]: On master: ROLLBACK
[0m01:08:06.809598 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:08:06.887134 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:08:06.890144 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:08:06.890144 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:08:06.890144 [debug] [MainThread]: On master: ROLLBACK
[0m01:08:06.890144 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:08:06.890144 [debug] [MainThread]: On master: Close
[0m01:08:06.902992 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:08:06.902992 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:08:06.902992 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:08:06.902992 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:08:06.902992 [info ] [MainThread]: 
[0m01:08:06.902992 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.75 seconds (0.75s).
[0m01:08:06.902992 [debug] [MainThread]: Command end result
[0m01:08:06.921639 [info ] [MainThread]: 
[0m01:08:06.921639 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:08:06.921639 [info ] [MainThread]: 
[0m01:08:06.921639 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:08:06.927149 [error] [MainThread]:   Database Error
[0m01:08:06.927149 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:08:06.927149 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:08:06.927149 [error] [MainThread]:     +- 'Project [*]
[0m01:08:06.932659 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:08:06.936749 [error] [MainThread]:     
[0m01:08:06.939658 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:08:06.939658 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:08:06.943009 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:08:06.943009 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:08:06.943009 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:08:06.943009 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:08:06.943009 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:08:06.943009 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:08:06.952773 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:08:06.952773 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:08:06.956580 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:08:06.956580 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:08:06.959095 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:08:06.959095 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:08:06.959095 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:08:06.959095 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:08:06.959095 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:08:06.959095 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:08:06.967119 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:08:06.967119 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:08:06.967119 [error] [MainThread]:     +- 'Project [*]
[0m01:08:06.967119 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:08:06.967119 [error] [MainThread]:     
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:08:06.975388 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:08:06.984168 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:08:06.984168 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:08:06.984168 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:08:06.984168 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:08:06.984168 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:08:06.992174 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:08:06.992174 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:08:06.992174 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:08:06.992174 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:08:06.992174 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:08:06.992174 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:08:06.999685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:08:06.999685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:08:06.999685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:08:06.999685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:08:06.999685 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:08:07.007697 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:08:07.007697 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:08:07.007697 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:08:07.007697 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:08:07.010886 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:08:07.010886 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:08:07.010886 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:08:07.015921 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:08:07.015921 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:08:07.015921 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:08:07.023933 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:08:07.031947 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:08:07.033986 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:08:07.033986 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:08:07.033986 [error] [MainThread]:     	... 16 more
[0m01:08:07.033986 [error] [MainThread]:     
[0m01:08:07.040003 [info ] [MainThread]: 
[0m01:08:07.040003 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:08:07.040003 [debug] [MainThread]: Command `dbt run` failed at 01:08:07.040003 after 1.17 seconds
[0m01:08:07.040003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024051A2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054694AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024054694E80>]}
[0m01:08:07.040003 [debug] [MainThread]: Flushing usage events
[0m01:11:11.213336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B701DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9732E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9732B90>]}


============================== 01:11:11.223227 | 958d710e-0788-42e3-9253-7cb4d6fc87d9 ==============================
[0m01:11:11.223227 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:11:11.223227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:11:11.390610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9732E00>]}
[0m01:11:11.390610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9AD2D40>]}
[0m01:11:11.390610 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:11:11.406216 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:11:11.425069 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:11:11.425069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B7001F00>]}
[0m01:11:12.255653 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:11:12.288160 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:11:12.297668 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:11:12.303729 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:11:12.440985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9BA6140>]}
[0m01:11:12.449100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B619EB30>]}
[0m01:11:12.449100 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:11:12.449100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '958d710e-0788-42e3-9253-7cb4d6fc87d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9AD3310>]}
[0m01:11:12.457154 [info ] [MainThread]: 
[0m01:11:12.457154 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:11:12.457154 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:11:12.467942 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:11:12.467942 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:11:12.467942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:11:12.489431 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:11:12.497453 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m01:11:12.497453 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m01:11:12.505587 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m01:11:12.508923 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:11:12.508923 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:11:12.508923 [info ] [MainThread]: 
[0m01:11:12.513942 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.05 seconds (0.05s).
[0m01:11:12.513942 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m01:11:12.521246 [debug] [MainThread]: Command `dbt run` failed at 01:11:12.513942 after 1.33 seconds
[0m01:11:12.522255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B701DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9BD7940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194B9BD4190>]}
[0m01:11:12.523068 [debug] [MainThread]: Flushing usage events
[0m01:11:22.689965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002165483DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021656F52C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021656F529B0>]}


============================== 01:11:22.698078 | 3b773f9b-6988-4af3-b7eb-b213f11e3204 ==============================
[0m01:11:22.698078 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:11:22.698078 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:11:22.698078 [info ] [MainThread]: dbt version: 1.5.2
[0m01:11:22.701639 [info ] [MainThread]: python version: 3.10.11
[0m01:11:22.702229 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:11:22.702229 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:11:22.702229 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:11:22.702229 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:11:22.702229 [info ] [MainThread]: Configuration:
[0m01:11:22.826534 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:11:22.850363 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:11:22.850363 [info ] [MainThread]: Required dependencies:
[0m01:11:22.850363 [debug] [MainThread]: Executing "git --help"
[0m01:11:22.899106 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:11:22.899106 [debug] [MainThread]: STDERR: "b''"
[0m01:11:22.899106 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:11:22.899106 [info ] [MainThread]: Connection:
[0m01:11:22.907108 [info ] [MainThread]:   host: localhost
[0m01:11:22.907108 [info ] [MainThread]:   port: 10000
[0m01:11:22.912834 [info ] [MainThread]:   cluster: None
[0m01:11:22.912834 [info ] [MainThread]:   endpoint: None
[0m01:11:22.915344 [info ] [MainThread]:   schema: ndb
[0m01:11:22.915344 [info ] [MainThread]:   organization: 0
[0m01:11:22.915344 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:11:22.915344 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:11:22.915344 [debug] [MainThread]: Using spark connection "debug"
[0m01:11:22.923357 [debug] [MainThread]: On debug: select 1 as id
[0m01:11:22.923357 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:11:22.956176 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m01:11:22.956176 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m01:11:22.956176 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m01:11:22.956176 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:11:22.956176 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m01:11:22.964189 [debug] [MainThread]: Command `dbt debug` failed at 01:11:22.964189 after 0.29 seconds
[0m01:11:22.964189 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:11:22.964189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002165483DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216572FDF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216572FDF60>]}
[0m01:11:22.964189 [debug] [MainThread]: Flushing usage events
[0m01:11:38.587694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E57FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E7F12C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E7F129B0>]}


============================== 01:11:38.591810 | 3fb5799e-41c4-48fb-aa58-fbbbd61f0f3b ==============================
[0m01:11:38.591810 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:11:38.591810 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:11:38.591810 [info ] [MainThread]: dbt version: 1.5.2
[0m01:11:38.591810 [info ] [MainThread]: python version: 3.10.11
[0m01:11:38.591810 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:11:38.591810 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:11:38.597629 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:11:38.598382 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:11:38.598382 [info ] [MainThread]: Configuration:
[0m01:11:38.736523 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:11:38.761080 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:11:38.761080 [info ] [MainThread]: Required dependencies:
[0m01:11:38.761080 [debug] [MainThread]: Executing "git --help"
[0m01:11:38.818473 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:11:38.818473 [debug] [MainThread]: STDERR: "b''"
[0m01:11:38.818473 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:11:38.818473 [info ] [MainThread]: Connection:
[0m01:11:38.818473 [info ] [MainThread]:   host: localhost
[0m01:11:38.826012 [info ] [MainThread]:   port: 10000
[0m01:11:38.827248 [info ] [MainThread]:   cluster: None
[0m01:11:38.827248 [info ] [MainThread]:   endpoint: None
[0m01:11:38.827248 [info ] [MainThread]:   schema: ndb
[0m01:11:38.827248 [info ] [MainThread]:   organization: 0
[0m01:11:38.827248 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:11:38.827248 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:11:38.827248 [debug] [MainThread]: Using spark connection "debug"
[0m01:11:38.827248 [debug] [MainThread]: On debug: select 1 as id
[0m01:11:38.827248 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:11:38.882908 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m01:11:38.882908 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m01:11:38.882908 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m01:11:38.882908 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:11:38.882908 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m01:11:38.890939 [debug] [MainThread]: Command `dbt debug` failed at 01:11:38.890939 after 0.32 seconds
[0m01:11:38.890939 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:11:38.890939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E57FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E82BDF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E82BDF60>]}
[0m01:11:38.890939 [debug] [MainThread]: Flushing usage events
[0m01:11:48.305567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E3267DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E34D92C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E34D929E0>]}


============================== 01:11:48.312009 | 18cf4cb7-48d3-4dc0-aeca-72fe978ef916 ==============================
[0m01:11:48.312009 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:11:48.313152 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:11:48.313152 [info ] [MainThread]: dbt version: 1.5.2
[0m01:11:48.315496 [info ] [MainThread]: python version: 3.10.11
[0m01:11:48.315496 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:11:48.316860 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:11:48.316860 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:11:48.318580 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:11:48.319086 [info ] [MainThread]: Configuration:
[0m01:11:48.449967 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:11:48.466085 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:11:48.466085 [info ] [MainThread]: Required dependencies:
[0m01:11:48.469790 [debug] [MainThread]: Executing "git --help"
[0m01:11:48.524628 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:11:48.524628 [debug] [MainThread]: STDERR: "b''"
[0m01:11:48.524628 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:11:48.524628 [info ] [MainThread]: Connection:
[0m01:11:48.524628 [info ] [MainThread]:   host: localhost
[0m01:11:48.524628 [info ] [MainThread]:   port: 10000
[0m01:11:48.524628 [info ] [MainThread]:   cluster: None
[0m01:11:48.524628 [info ] [MainThread]:   endpoint: None
[0m01:11:48.524628 [info ] [MainThread]:   schema: ndb
[0m01:11:48.524628 [info ] [MainThread]:   organization: 0
[0m01:11:48.533105 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:11:48.533105 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:11:48.533105 [debug] [MainThread]: Using spark connection "debug"
[0m01:11:48.533105 [debug] [MainThread]: On debug: select 1 as id
[0m01:11:48.533105 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:11:48.735039 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:11:48.736046 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m01:11:48.739085 [debug] [MainThread]: On debug: Close
[0m01:11:48.754792 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:11:48.757117 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:11:48.758794 [debug] [MainThread]: Command `dbt debug` succeeded at 01:11:48.758261 after 0.47 seconds
[0m01:11:48.759337 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:11:48.759791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E3267DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E317F78E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E317F6950>]}
[0m01:11:48.760835 [debug] [MainThread]: Flushing usage events
[0m01:12:00.204208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D68235DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684A72E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684A72BC0>]}


============================== 01:12:00.212390 | 897832ff-c30c-43a0-8b3c-37fb711c1381 ==============================
[0m01:12:00.212390 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:12:00.213482 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:12:00.345601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684A72DD0>]}
[0m01:12:00.353562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684E11EA0>]}
[0m01:12:00.353562 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:12:00.361569 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:12:00.393694 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:12:00.393694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6825057E0>]}
[0m01:12:01.283758 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:12:01.299413 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:12:01.299413 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:12:01.299413 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:12:01.424938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684FF8670>]}
[0m01:12:01.424938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6814DDB70>]}
[0m01:12:01.424938 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:12:01.424938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684E12650>]}
[0m01:12:01.438646 [info ] [MainThread]: 
[0m01:12:01.440845 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:12:01.440845 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:12:01.461951 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:12:01.461951 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:12:01.461951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:12:01.627185 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:12:01.627185 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:12:01.634684 [debug] [ThreadPool]: On list_schemas: Close
[0m01:12:01.645809 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:12:01.656355 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:01.656355 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:12:01.656355 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:12:01.656355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:12:01.768932 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:12:01.768932 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:12:01.775971 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:12:01.775971 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:12:01.775971 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:12:01.788952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684F136A0>]}
[0m01:12:01.788952 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:01.788952 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:12:01.788952 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:12:01.793962 [info ] [MainThread]: 
[0m01:12:01.799376 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:12:01.799376 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:12:01.799376 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:12:01.802650 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:12:01.810659 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:12:01.815233 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:12:01.802650 => 01:12:01.810659
[0m01:12:01.817235 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:12:01.916173 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:12:01.916173 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:01.916173 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:12:01.924181 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (primaryKey "application_id" , preCombineField "watermark" , type "cow" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:12:01.924181 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:12:02.027566 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:12:02.027566 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:12:02.027566 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (primaryKey "application_id" , preCombineField "watermark" , type "cow" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:12:02.031574 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:12:02.031574 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:12:01.817235 => 01:12:02.031574
[0m01:12:02.034094 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:12:02.034094 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:12:02.034094 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:12:02.049570 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:12:02.049570 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '897832ff-c30c-43a0-8b3c-37fb711c1381', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684A72350>]}
[0m01:12:02.054595 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.25s]
[0m01:12:02.054595 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:12:02.056557 [debug] [MainThread]: On master: ROLLBACK
[0m01:12:02.056557 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:12:02.134238 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:12:02.134238 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:02.137248 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:12:02.137248 [debug] [MainThread]: On master: ROLLBACK
[0m01:12:02.137248 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:12:02.140276 [debug] [MainThread]: On master: Close
[0m01:12:02.150618 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:12:02.152077 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:12:02.152077 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:12:02.152077 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:12:02.152077 [info ] [MainThread]: 
[0m01:12:02.157101 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.71 seconds (0.71s).
[0m01:12:02.157810 [debug] [MainThread]: Command end result
[0m01:12:02.165322 [info ] [MainThread]: 
[0m01:12:02.170666 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:12:02.172308 [info ] [MainThread]: 
[0m01:12:02.173815 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:12:02.173815 [error] [MainThread]:   Database Error
[0m01:12:02.173815 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:12:02.173815 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:12:02.173815 [error] [MainThread]:     +- 'Project [*]
[0m01:12:02.173815 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:12:02.173815 [error] [MainThread]:     
[0m01:12:02.173815 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:12:02.173815 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:12:02.182354 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:12:02.182354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:12:02.191151 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:12:02.191151 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:12:02.191151 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:12:02.222802 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:12:02.231369 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:12:02.238880 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:12:02.239887 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:12:02.239887 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:12:02.239887 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:12:02.239887 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:12:02.246896 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:12:02.246896 [error] [MainThread]:     +- 'Project [*]
[0m01:12:02.250840 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:12:02.255361 [error] [MainThread]:     
[0m01:12:02.255361 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:12:02.259887 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:12:02.259887 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:12:02.259887 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:12:02.263412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:12:02.263412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:12:02.263412 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:12:02.263412 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:12:02.271522 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:12:02.273546 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:12:02.273546 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:12:02.273546 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:12:02.277495 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:12:02.277495 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:12:02.277495 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:12:02.280013 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:12:02.280013 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:12:02.280013 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:12:02.280013 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:12:02.280013 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:12:02.280013 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:12:02.288027 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:12:02.291346 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:12:02.291346 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:12:02.291346 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:12:02.296359 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:12:02.305044 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:12:02.305044 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:12:02.305044 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:12:02.305044 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:12:02.312586 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:12:02.312586 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:12:02.312586 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:12:02.315593 [error] [MainThread]:     	... 16 more
[0m01:12:02.315593 [error] [MainThread]:     
[0m01:12:02.315593 [info ] [MainThread]: 
[0m01:12:02.315593 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:12:02.315593 [debug] [MainThread]: Command `dbt run` failed at 01:12:02.315593 after 2.13 seconds
[0m01:12:02.320902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D68235DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6814DEEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D684F6CAF0>]}
[0m01:12:02.320902 [debug] [MainThread]: Flushing usage events
[0m01:12:08.042086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C97FFDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9A712E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9A712BC0>]}


============================== 01:12:08.055245 | 564d4d61-abc1-46c6-8d43-7d50a265a09f ==============================
[0m01:12:08.055245 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:12:08.056943 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:12:08.255583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9A712E30>]}
[0m01:12:08.261077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9AAB9ED0>]}
[0m01:12:08.261077 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:12:08.281070 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:12:08.365090 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:12:08.365090 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:12:08.373096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9AC7C0D0>]}
[0m01:12:08.381106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9ABB37C0>]}
[0m01:12:08.381106 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:12:08.381106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9ABB3760>]}
[0m01:12:08.381106 [info ] [MainThread]: 
[0m01:12:08.381106 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:12:08.388586 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:12:08.397727 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:12:08.405716 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:12:08.405716 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:12:08.545910 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:12:08.551422 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:12:08.557452 [debug] [ThreadPool]: On list_schemas: Close
[0m01:12:08.572153 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:12:08.582710 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:08.582710 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:12:08.582710 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:12:08.586791 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:12:08.725978 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:12:08.731017 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:12:08.735538 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:12:08.735538 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:12:08.740059 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:12:08.742073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9ACBF2B0>]}
[0m01:12:08.742073 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:08.742073 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:12:08.742073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:12:08.757709 [info ] [MainThread]: 
[0m01:12:08.761240 [debug] [Thread-1 (]: Began running node model.poc_demo.my_first_dbt_model
[0m01:12:08.761240 [info ] [Thread-1 (]: 1 of 1 START sql table model ndb.my_first_dbt_model ............................ [RUN]
[0m01:12:08.767730 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m01:12:08.767730 [debug] [Thread-1 (]: Began compiling node model.poc_demo.my_first_dbt_model
[0m01:12:08.767730 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m01:12:08.774976 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (compile): 01:12:08.767730 => 01:12:08.767730
[0m01:12:08.775998 [debug] [Thread-1 (]: Began executing node model.poc_demo.my_first_dbt_model
[0m01:12:08.826008 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m01:12:08.826008 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists ndb.my_first_dbt_model
[0m01:12:08.828046 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:12:08.930129 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:12:08.930129 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:12:08.961767 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m01:12:08.977575 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:08.977575 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m01:12:08.977575 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table ndb.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m01:12:10.828054 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:12:10.828054 [debug] [Thread-1 (]: SQL status: OK in 2.0 seconds
[0m01:12:10.840127 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (execute): 01:12:08.775998 => 01:12:10.840127
[0m01:12:10.840127 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m01:12:10.854273 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:12:10.854273 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: Close
[0m01:12:10.871286 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '564d4d61-abc1-46c6-8d43-7d50a265a09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9AC61420>]}
[0m01:12:10.874329 [info ] [Thread-1 (]: 1 of 1 OK created sql table model ndb.my_first_dbt_model ....................... [[32mOK[0m in 2.11s]
[0m01:12:10.874329 [debug] [Thread-1 (]: Finished running node model.poc_demo.my_first_dbt_model
[0m01:12:10.876837 [debug] [MainThread]: On master: ROLLBACK
[0m01:12:10.876837 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:12:10.953344 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:12:10.953344 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:12:10.953344 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:12:10.953344 [debug] [MainThread]: On master: ROLLBACK
[0m01:12:10.953344 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:12:10.953344 [debug] [MainThread]: On master: Close
[0m01:12:10.960864 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:12:10.968874 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:12:10.968874 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:12:10.968874 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m01:12:10.971468 [info ] [MainThread]: 
[0m01:12:10.971468 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.59 seconds (2.59s).
[0m01:12:10.977035 [debug] [MainThread]: Command end result
[0m01:12:10.985057 [info ] [MainThread]: 
[0m01:12:10.985057 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:12:10.985057 [info ] [MainThread]: 
[0m01:12:10.990597 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:12:10.993107 [debug] [MainThread]: Command `dbt run` succeeded at 01:12:10.993107 after 2.96 seconds
[0m01:12:10.993107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C97FFDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C96633B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C9AAB9ED0>]}
[0m01:12:10.993107 [debug] [MainThread]: Flushing usage events
[0m01:16:08.701559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8921DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8B92EE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8B92EBF0>]}


============================== 01:16:08.717241 | 758cc2b4-34d3-48b5-9608-c73ad920caff ==============================
[0m01:16:08.717241 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:16:08.717241 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:16:08.846610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8B92EE00>]}
[0m01:16:08.862541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BCDDED0>]}
[0m01:16:08.862541 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:16:08.878549 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:16:08.950633 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:16:08.950633 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:16:08.961833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BE9C0D0>]}
[0m01:16:08.966857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BDCF790>]}
[0m01:16:08.974870 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:16:08.974870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BDCF730>]}
[0m01:16:08.974870 [info ] [MainThread]: 
[0m01:16:08.974870 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:16:08.986988 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:16:09.023428 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:16:09.023428 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:16:09.023428 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:16:09.176361 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:16:09.176361 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:16:09.186912 [debug] [ThreadPool]: On list_schemas: Close
[0m01:16:09.200408 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:16:09.204905 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:16:09.204905 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:16:09.204905 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:16:09.204905 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:16:09.528716 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:16:09.528716 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:16:09.544464 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:16:09.545482 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:16:09.545482 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:16:09.553057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BEDFCA0>]}
[0m01:16:09.557079 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:16:09.557079 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:16:09.557079 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:16:09.557079 [info ] [MainThread]: 
[0m01:16:09.565691 [debug] [Thread-1 (]: Began running node model.poc_demo.my_second_dbt_model
[0m01:16:09.567207 [info ] [Thread-1 (]: 1 of 1 START sql table model ndb.my_second_dbt_model ........................... [RUN]
[0m01:16:09.567207 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.my_second_dbt_model'
[0m01:16:09.569156 [debug] [Thread-1 (]: Began compiling node model.poc_demo.my_second_dbt_model
[0m01:16:09.574178 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.my_second_dbt_model"
[0m01:16:09.574178 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_second_dbt_model (compile): 01:16:09.569156 => 01:16:09.574178
[0m01:16:09.579688 [debug] [Thread-1 (]: Began executing node model.poc_demo.my_second_dbt_model
[0m01:16:09.612185 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m01:16:09.620200 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */
drop table if exists ndb.my_second_dbt_model
[0m01:16:09.620200 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:16:09.733024 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:16:09.733024 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:16:09.764740 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.my_second_dbt_model"
[0m01:16:09.764740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:16:09.776211 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_second_dbt_model"
[0m01:16:09.776211 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_second_dbt_model"} */

  
    
        create table ndb.my_second_dbt_model
      
      
      
      
      
      
      
      as
      -- Use the `ref` function to select from other models

select *
from ndb.my_first_dbt_model
where id = 1
  
[0m01:16:10.778185 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:16:10.780513 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m01:16:10.813756 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_second_dbt_model (execute): 01:16:09.579688 => 01:16:10.813756
[0m01:16:10.814866 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: ROLLBACK
[0m01:16:10.814866 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:16:10.814866 [debug] [Thread-1 (]: On model.poc_demo.my_second_dbt_model: Close
[0m01:16:10.827949 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '758cc2b4-34d3-48b5-9608-c73ad920caff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BE82DA0>]}
[0m01:16:10.834525 [info ] [Thread-1 (]: 1 of 1 OK created sql table model ndb.my_second_dbt_model ...................... [[32mOK[0m in 1.26s]
[0m01:16:10.835032 [debug] [Thread-1 (]: Finished running node model.poc_demo.my_second_dbt_model
[0m01:16:10.835032 [debug] [MainThread]: On master: ROLLBACK
[0m01:16:10.835032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:16:10.965302 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:16:10.965302 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:16:10.965302 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:16:10.971315 [debug] [MainThread]: On master: ROLLBACK
[0m01:16:10.971315 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:16:10.973381 [debug] [MainThread]: On master: Close
[0m01:16:10.985448 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:16:10.986534 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:16:10.987614 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:16:10.988159 [debug] [MainThread]: Connection 'model.poc_demo.my_second_dbt_model' was properly closed.
[0m01:16:10.988769 [info ] [MainThread]: 
[0m01:16:10.989950 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.01 seconds (2.01s).
[0m01:16:10.993723 [debug] [MainThread]: Command end result
[0m01:16:10.996240 [info ] [MainThread]: 
[0m01:16:10.996240 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:16:11.005251 [info ] [MainThread]: 
[0m01:16:11.007797 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:16:11.007797 [debug] [MainThread]: Command `dbt run` succeeded at 01:16:11.007797 after 2.31 seconds
[0m01:16:11.007797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8921DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC89563AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8BCDDED0>]}
[0m01:16:11.007797 [debug] [MainThread]: Flushing usage events
[0m01:23:57.036062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E44DDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E6BF2C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E6BF2A40>]}


============================== 01:23:57.036062 | 6c38530f-7543-46dc-8fcf-3030ef9e63ae ==============================
[0m01:23:57.036062 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:23:57.036062 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:23:57.036062 [info ] [MainThread]: dbt version: 1.5.2
[0m01:23:57.048688 [info ] [MainThread]: python version: 3.10.11
[0m01:23:57.048688 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:23:57.048688 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:23:57.048688 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:23:57.048688 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:23:57.048688 [info ] [MainThread]: Configuration:
[0m01:23:57.190293 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:23:57.234848 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:23:57.234848 [info ] [MainThread]: Required dependencies:
[0m01:23:57.234848 [debug] [MainThread]: Executing "git --help"
[0m01:23:57.293370 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:23:57.295327 [debug] [MainThread]: STDERR: "b''"
[0m01:23:57.296831 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:23:57.299348 [info ] [MainThread]: Connection:
[0m01:23:57.299348 [info ] [MainThread]:   host: localhost
[0m01:23:57.299348 [info ] [MainThread]:   port: 10000
[0m01:23:57.299348 [info ] [MainThread]:   cluster: None
[0m01:23:57.299348 [info ] [MainThread]:   endpoint: None
[0m01:23:57.299348 [info ] [MainThread]:   schema: default
[0m01:23:57.299348 [info ] [MainThread]:   organization: 0
[0m01:23:57.299348 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:23:57.308221 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:23:57.308221 [debug] [MainThread]: Using spark connection "debug"
[0m01:23:57.308221 [debug] [MainThread]: On debug: select 1 as id
[0m01:23:57.308221 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:23:57.480872 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:23:57.485893 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m01:23:57.487909 [debug] [MainThread]: On debug: Close
[0m01:23:57.533111 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:23:57.534627 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:23:57.534627 [debug] [MainThread]: Command `dbt debug` succeeded at 01:23:57.534627 after 0.51 seconds
[0m01:23:57.537966 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:23:57.537966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E44DDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E36638E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283E3662950>]}
[0m01:23:57.537966 [debug] [MainThread]: Flushing usage events
[0m01:24:12.269633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BA8FEDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BAB702E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BAB702C20>]}


============================== 01:24:12.269633 | 5b96f079-d870-4947-8515-c7d2a6bea60e ==============================
[0m01:24:12.269633 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:24:12.269633 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:24:12.431350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BAB702E30>]}
[0m01:24:12.451378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BABAADED0>]}
[0m01:24:12.455387 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:24:12.463450 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:24:12.479839 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:24:12.479839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BA8FE2EF0>]}
[0m01:24:13.372018 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:24:13.388401 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:24:13.396379 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:24:13.396379 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:24:13.601878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BABC846D0>]}
[0m01:24:13.606441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BA816D9F0>]}
[0m01:24:13.606441 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:24:13.614451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BABAAF220>]}
[0m01:24:13.614451 [info ] [MainThread]: 
[0m01:24:13.614451 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:24:13.614451 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:24:13.630477 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:24:13.630477 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:24:13.630477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:24:13.783728 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:24:13.783728 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:24:13.801262 [debug] [ThreadPool]: On list_schemas: Close
[0m01:24:13.815389 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:24:13.818902 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:24:13.818902 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:24:13.818902 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:24:13.818902 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:24:13.954112 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:24:13.954112 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:24:13.958129 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:24:13.965643 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:24:13.965643 [debug] [ThreadPool]: On list_None_default: Close
[0m01:24:13.975154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BABC23AF0>]}
[0m01:24:13.975154 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:24:13.975154 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:24:13.975154 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:24:13.985177 [info ] [MainThread]: 
[0m01:24:13.990853 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:24:13.990853 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default.hudi_cow ............................ [RUN]
[0m01:24:13.990853 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:24:13.990853 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:24:14.001365 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:24:14.007885 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:24:13.990853 => 01:24:14.005881
[0m01:24:14.007885 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:24:14.124533 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:24:14.124533 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:24:14.133014 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:24:14.134540 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table default.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:24:14.134540 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:24:14.305527 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `default`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `default`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:24:14.305527 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:24:14.305527 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table default.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:24:14.311536 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `default`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `default`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:24:14.311536 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:24:14.007885 => 01:24:14.311536
[0m01:24:14.311536 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:24:14.311536 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:24:14.311536 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:24:14.332804 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `default`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `default`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:24:14.335822 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b96f079-d870-4947-8515-c7d2a6bea60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BAB7023B0>]}
[0m01:24:14.335822 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default.hudi_cow ................... [[31mERROR[0m in 0.34s]
[0m01:24:14.338832 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:24:14.340799 [debug] [MainThread]: On master: ROLLBACK
[0m01:24:14.340799 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:24:14.420126 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:24:14.420126 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:24:14.420126 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:24:14.420126 [debug] [MainThread]: On master: ROLLBACK
[0m01:24:14.423135 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:24:14.423135 [debug] [MainThread]: On master: Close
[0m01:24:14.440070 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:24:14.441455 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:24:14.442469 [debug] [MainThread]: Connection 'list_None_default' was properly closed.
[0m01:24:14.444594 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:24:14.444594 [info ] [MainThread]: 
[0m01:24:14.447271 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.83 seconds (0.83s).
[0m01:24:14.449459 [debug] [MainThread]: Command end result
[0m01:24:14.458956 [info ] [MainThread]: 
[0m01:24:14.461481 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:24:14.465495 [info ] [MainThread]: 
[0m01:24:14.467124 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:24:14.467641 [error] [MainThread]:   Database Error
[0m01:24:14.468477 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:24:14.468477 [error] [MainThread]:     'CreateTable `default`.`hudi_cow`, ErrorIfExists
[0m01:24:14.468477 [error] [MainThread]:     +- 'Project [*]
[0m01:24:14.468477 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:24:14.468477 [error] [MainThread]:     
[0m01:24:14.473999 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:24:14.473999 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:24:14.477052 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:24:14.477052 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:24:14.482074 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:24:14.482074 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:24:14.482074 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:24:14.482074 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:24:14.482074 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:24:14.487818 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:24:14.490337 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:24:14.490844 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:24:14.490844 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:24:14.490844 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:24:14.498374 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:24:14.498374 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:24:14.498374 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:24:14.498374 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:24:14.503747 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:24:14.503747 [error] [MainThread]:     'CreateTable `default`.`hudi_cow`, ErrorIfExists
[0m01:24:14.503747 [error] [MainThread]:     +- 'Project [*]
[0m01:24:14.507537 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:24:14.507537 [error] [MainThread]:     
[0m01:24:14.507537 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:24:14.513045 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:24:14.515072 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:24:14.515072 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:24:14.515072 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:24:14.519664 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:24:14.519664 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:24:14.519664 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:24:14.523173 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:24:14.523173 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:24:14.523173 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:24:14.523173 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:24:14.523173 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:24:14.523173 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:24:14.523173 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:24:14.523173 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:24:14.531184 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:24:14.539668 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:24:14.540831 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:24:14.540831 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:24:14.544767 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:24:14.544767 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:24:14.548151 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:24:14.548151 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:24:14.548151 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:24:14.548151 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:24:14.553352 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:24:14.553352 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:24:14.555361 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:24:14.555361 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:24:14.555361 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:24:14.555361 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:24:14.555361 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:24:14.562250 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:24:14.563764 [error] [MainThread]:     	... 16 more
[0m01:24:14.563764 [error] [MainThread]:     
[0m01:24:14.563764 [info ] [MainThread]: 
[0m01:24:14.563764 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:24:14.571810 [debug] [MainThread]: Command `dbt run` failed at 01:24:14.571810 after 2.32 seconds
[0m01:24:14.571810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BA8FEDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BA816D9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BABB53EB0>]}
[0m01:24:14.580562 [debug] [MainThread]: Flushing usage events
[0m01:25:05.923387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DBDB8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC02A2E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC02A2B90>]}


============================== 01:25:05.923387 | 89650c0b-81d6-494e-a3cb-49797effae0e ==============================
[0m01:25:05.923387 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:25:05.923387 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:25:06.066810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC02A2E00>]}
[0m01:25:06.079600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC0649ED0>]}
[0m01:25:06.080601 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:25:06.090602 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:25:06.152119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:25:06.153125 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:25:06.160121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC080C0D0>]}
[0m01:25:06.168141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC073F790>]}
[0m01:25:06.169122 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:25:06.171124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC073F730>]}
[0m01:25:06.173476 [info ] [MainThread]: 
[0m01:25:06.174480 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:25:06.176491 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:25:06.185494 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:25:06.186497 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:25:06.186497 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:25:06.289767 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:25:06.289767 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:25:06.297796 [debug] [ThreadPool]: On list_schemas: Close
[0m01:25:06.308722 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:25:06.311243 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:25:06.311243 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:25:06.311243 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:25:06.311243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:25:06.411031 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:25:06.411031 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:25:06.411031 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:25:06.411031 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:25:06.418046 [debug] [ThreadPool]: On list_None_default: Close
[0m01:25:06.426599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC084B2B0>]}
[0m01:25:06.428277 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:25:06.428277 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:25:06.428277 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:25:06.428277 [info ] [MainThread]: 
[0m01:25:06.428277 [debug] [Thread-1 (]: Began running node model.poc_demo.my_first_dbt_model
[0m01:25:06.437290 [info ] [Thread-1 (]: 1 of 1 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m01:25:06.437290 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.my_first_dbt_model'
[0m01:25:06.437290 [debug] [Thread-1 (]: Began compiling node model.poc_demo.my_first_dbt_model
[0m01:25:06.442988 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.my_first_dbt_model"
[0m01:25:06.445000 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (compile): 01:25:06.437290 => 01:25:06.445000
[0m01:25:06.446024 [debug] [Thread-1 (]: Began executing node model.poc_demo.my_first_dbt_model
[0m01:25:06.467699 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m01:25:06.468703 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */
drop table if exists default.my_first_dbt_model
[0m01:25:06.468703 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:25:06.580094 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:25:06.580094 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:25:06.611000 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.my_first_dbt_model"
[0m01:25:06.611000 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:25:06.611000 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.my_first_dbt_model"
[0m01:25:06.611000 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.my_first_dbt_model"} */

  
    
        create table default.my_first_dbt_model
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m01:25:07.001331 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:25:07.001331 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:25:07.015890 [debug] [Thread-1 (]: Timing info for model.poc_demo.my_first_dbt_model (execute): 01:25:06.446024 => 01:25:07.015890
[0m01:25:07.015890 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: ROLLBACK
[0m01:25:07.015890 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:25:07.015890 [debug] [Thread-1 (]: On model.poc_demo.my_first_dbt_model: Close
[0m01:25:07.025930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89650c0b-81d6-494e-a3cb-49797effae0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC07F13F0>]}
[0m01:25:07.025930 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 0.59s]
[0m01:25:07.025930 [debug] [Thread-1 (]: Finished running node model.poc_demo.my_first_dbt_model
[0m01:25:07.025930 [debug] [MainThread]: On master: ROLLBACK
[0m01:25:07.025930 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:25:07.073886 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:25:07.073886 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:25:07.074886 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:25:07.074886 [debug] [MainThread]: On master: ROLLBACK
[0m01:25:07.075891 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:25:07.075891 [debug] [MainThread]: On master: Close
[0m01:25:07.083749 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:25:07.084750 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:25:07.085755 [debug] [MainThread]: Connection 'list_None_default' was properly closed.
[0m01:25:07.085755 [debug] [MainThread]: Connection 'model.poc_demo.my_first_dbt_model' was properly closed.
[0m01:25:07.085755 [info ] [MainThread]: 
[0m01:25:07.087261 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.91 seconds (0.91s).
[0m01:25:07.088270 [debug] [MainThread]: Command end result
[0m01:25:07.093272 [info ] [MainThread]: 
[0m01:25:07.094272 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:25:07.095282 [info ] [MainThread]: 
[0m01:25:07.096679 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:25:07.097686 [debug] [MainThread]: Command `dbt run` succeeded at 01:25:07.097686 after 1.19 seconds
[0m01:25:07.097686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DBDB8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DBDED3AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020DC0649ED0>]}
[0m01:25:07.098687 [debug] [MainThread]: Flushing usage events
[0m01:33:36.498415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DC39DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEAB2CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEAB2A40>]}


============================== 01:33:36.514141 | 9a63a3f6-7c24-4ce0-92e3-535252e1e981 ==============================
[0m01:33:36.514141 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:33:36.514141 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:33:36.646551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEAB2CB0>]}
[0m01:33:36.654562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEE72110>]}
[0m01:33:36.654562 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:33:36.662661 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:33:36.704607 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:33:36.705606 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:33:36.711601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DF02C0D0>]}
[0m01:33:36.719626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEF5F640>]}
[0m01:33:36.720632 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:33:36.721977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEF5F5E0>]}
[0m01:33:36.724084 [info ] [MainThread]: 
[0m01:33:36.724988 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:33:36.727041 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:33:36.735046 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:33:36.736046 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:33:36.737061 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:33:36.861586 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:33:36.862601 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:33:36.871627 [debug] [ThreadPool]: On list_schemas: Close
[0m01:33:36.884475 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:33:36.886754 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:36.886754 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:33:36.886754 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:33:36.886754 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:33:37.008509 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:33:37.008509 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:33:37.016036 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:33:37.016036 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:33:37.016036 [debug] [ThreadPool]: On list_None_default: Close
[0m01:33:37.022071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DEF5EB30>]}
[0m01:33:37.028578 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:37.028578 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:33:37.028578 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:33:37.028578 [info ] [MainThread]: 
[0m01:33:37.028578 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:33:37.028578 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default.hudi_cow ............................ [RUN]
[0m01:33:37.028578 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:33:37.038098 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:33:37.038789 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:33:37.038789 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:33:37.038098 => 01:33:37.038789
[0m01:33:37.038789 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:33:37.120069 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:33:37.121069 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:37.122075 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:33:37.122075 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table default.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:33:37.122075 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:33:37.199410 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `default`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `default`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:33:37.199410 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:33:37.199410 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table default.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:33:37.199410 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `default`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `default`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:33:37.199410 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:33:37.038789 => 01:33:37.199410
[0m01:33:37.199410 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:33:37.199410 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:33:37.199410 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:33:37.213628 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `default`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `default`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:33:37.213628 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a63a3f6-7c24-4ce0-92e3-535252e1e981', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DF012740>]}
[0m01:33:37.213628 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default.hudi_cow ................... [[31mERROR[0m in 0.19s]
[0m01:33:37.213628 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:33:37.224136 [debug] [MainThread]: On master: ROLLBACK
[0m01:33:37.224136 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:33:37.260659 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:33:37.261666 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:37.262666 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:33:37.262666 [debug] [MainThread]: On master: ROLLBACK
[0m01:33:37.263667 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:33:37.263667 [debug] [MainThread]: On master: Close
[0m01:33:37.274193 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:33:37.274859 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:33:37.275869 [debug] [MainThread]: Connection 'list_None_default' was properly closed.
[0m01:33:37.275869 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:33:37.276863 [info ] [MainThread]: 
[0m01:33:37.276863 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.55 seconds (0.55s).
[0m01:33:37.277864 [debug] [MainThread]: Command end result
[0m01:33:37.284400 [info ] [MainThread]: 
[0m01:33:37.285389 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:33:37.285389 [info ] [MainThread]: 
[0m01:33:37.286399 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:33:37.286399 [error] [MainThread]:   Database Error
[0m01:33:37.288913 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:33:37.288913 [error] [MainThread]:     'CreateTable `default`.`hudi_cow`, ErrorIfExists
[0m01:33:37.289915 [error] [MainThread]:     +- 'Project [*]
[0m01:33:37.290914 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:33:37.291916 [error] [MainThread]:     
[0m01:33:37.291916 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:33:37.292914 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:33:37.293914 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:33:37.294916 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:33:37.295915 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:33:37.296917 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:33:37.297915 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:33:37.298913 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:33:37.299914 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:33:37.300916 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:33:37.301936 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:33:37.304916 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:33:37.305916 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:33:37.307429 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:33:37.308445 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:33:37.308445 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:33:37.309437 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:33:37.309437 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:33:37.310441 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:33:37.310441 [error] [MainThread]:     'CreateTable `default`.`hudi_cow`, ErrorIfExists
[0m01:33:37.311440 [error] [MainThread]:     +- 'Project [*]
[0m01:33:37.311440 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:33:37.312437 [error] [MainThread]:     
[0m01:33:37.312437 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:33:37.313440 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:33:37.313440 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:33:37.314438 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:33:37.315490 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:33:37.316464 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:33:37.318974 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:33:37.318974 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:33:37.320970 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:33:37.320970 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:33:37.321975 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:33:37.322972 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:33:37.323971 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:33:37.323971 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:33:37.324970 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:33:37.325968 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:33:37.325968 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:33:37.325968 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:33:37.327473 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:33:37.328478 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:33:37.328478 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:33:37.329478 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:33:37.329478 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:33:37.330481 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:33:37.330481 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:33:37.331479 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:33:37.332479 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:33:37.333481 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:33:37.335491 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:33:37.336480 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:33:37.336480 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:33:37.337834 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:33:37.338479 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:33:37.338479 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:33:37.339479 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:33:37.340491 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:33:37.340491 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:33:37.341484 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:33:37.342479 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:33:37.342479 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:33:37.343479 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:33:37.344479 [error] [MainThread]:     	... 16 more
[0m01:33:37.344479 [error] [MainThread]:     
[0m01:33:37.345479 [info ] [MainThread]: 
[0m01:33:37.345479 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:33:37.346986 [debug] [MainThread]: Command `dbt run` failed at 01:33:37.346986 after 0.85 seconds
[0m01:33:37.348014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DC39DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DF010250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3DF012740>]}
[0m01:33:37.349994 [debug] [MainThread]: Flushing usage events
[0m01:41:33.178840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993BCCDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993E3E2C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993E3E29B0>]}


============================== 01:41:33.178840 | 022b3f48-efc3-460b-b150-a50a1caf7d60 ==============================
[0m01:41:33.178840 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:41:33.178840 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:41:33.178840 [info ] [MainThread]: dbt version: 1.5.2
[0m01:41:33.178840 [info ] [MainThread]: python version: 3.10.11
[0m01:41:33.178840 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:41:33.178840 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:41:33.191050 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:41:33.192090 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:41:33.193058 [info ] [MainThread]: Configuration:
[0m01:41:33.306735 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:41:33.325722 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:41:33.326886 [info ] [MainThread]: Required dependencies:
[0m01:41:33.327894 [debug] [MainThread]: Executing "git --help"
[0m01:41:33.363650 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:41:33.364734 [debug] [MainThread]: STDERR: "b''"
[0m01:41:33.365656 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:41:33.366658 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:41:33.368684 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo", target "dev" invalid: Runtime Error
        schema: default 
        database: ndb 
    On Spark, database must be omitted or have the same value as schema.


[0m01:41:33.372655 [debug] [MainThread]: Command `dbt debug` failed at 01:41:33.372655 after 0.21 seconds
[0m01:41:33.373657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993BCCDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993AE57550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001993AE540A0>]}
[0m01:41:33.374655 [debug] [MainThread]: Flushing usage events
[0m01:41:52.913900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF3836DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF3AA82C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF3AA829E0>]}


============================== 01:41:52.918295 | bee81ab5-c501-4d07-ac07-eb5aa380da18 ==============================
[0m01:41:52.918295 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:41:52.918295 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:41:52.918295 [info ] [MainThread]: dbt version: 1.5.2
[0m01:41:52.918295 [info ] [MainThread]: python version: 3.10.11
[0m01:41:52.918295 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:41:52.918295 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:41:52.918295 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:41:52.926147 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m01:41:52.927160 [info ] [MainThread]: Configuration:
[0m01:41:53.029471 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:41:53.044472 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:41:53.045487 [info ] [MainThread]: Required dependencies:
[0m01:41:53.046472 [debug] [MainThread]: Executing "git --help"
[0m01:41:53.088320 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:41:53.088320 [debug] [MainThread]: STDERR: "b''"
[0m01:41:53.089315 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:41:53.090318 [info ] [MainThread]: Connection:
[0m01:41:53.091427 [info ] [MainThread]:   host: localhost
[0m01:41:53.092334 [info ] [MainThread]:   port: 10000
[0m01:41:53.094522 [info ] [MainThread]:   cluster: None
[0m01:41:53.095537 [info ] [MainThread]:   endpoint: None
[0m01:41:53.098524 [info ] [MainThread]:   schema: ndb
[0m01:41:53.099159 [info ] [MainThread]:   organization: 0
[0m01:41:53.100165 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:41:53.101199 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:41:53.102167 [debug] [MainThread]: Using spark connection "debug"
[0m01:41:53.103189 [debug] [MainThread]: On debug: select 1 as id
[0m01:41:53.104168 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:41:53.261062 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:41:53.262067 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m01:41:53.263161 [debug] [MainThread]: On debug: Close
[0m01:41:53.272877 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:41:53.274527 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:41:53.276122 [debug] [MainThread]: Command `dbt debug` succeeded at 01:41:53.276122 after 0.38 seconds
[0m01:41:53.276635 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:41:53.276635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF3836DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF374F75E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF374F7E20>]}
[0m01:41:53.276635 [debug] [MainThread]: Flushing usage events
[0m01:42:00.469626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82859DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82ACB2E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82ACB2BC0>]}


============================== 01:42:00.469626 | 74ff0e2d-748c-411b-bb4e-29c9eb942a5c ==============================
[0m01:42:00.469626 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:42:00.469626 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:42:00.601418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82ACB2E30>]}
[0m01:42:00.610424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82B052D70>]}
[0m01:42:00.611512 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:42:00.622799 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:42:00.631477 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:42:00.632390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C828896FB0>]}
[0m01:42:01.282438 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:42:01.282438 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:42:01.298057 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:42:01.298057 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:42:01.392347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82B126620>]}
[0m01:42:01.407980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82771E9B0>]}
[0m01:42:01.407980 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:42:01.407980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82B0506A0>]}
[0m01:42:01.407980 [info ] [MainThread]: 
[0m01:42:01.407980 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:42:01.407980 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:42:01.425170 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:42:01.425170 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:42:01.426713 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:01.533155 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:42:01.533155 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:42:01.534177 [debug] [ThreadPool]: On list_schemas: Close
[0m01:42:01.549809 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:42:01.554814 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:01.554814 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:42:01.554814 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:42:01.554814 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:01.669881 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:42:01.669881 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:42:01.680289 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:42:01.680850 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:42:01.680850 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:42:01.691038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82B0CF8E0>]}
[0m01:42:01.692558 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:01.697180 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:42:01.697180 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:42:01.700224 [info ] [MainThread]: 
[0m01:42:01.700224 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:42:01.700224 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:42:01.700224 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:42:01.708666 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:42:01.712675 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:42:01.713676 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:42:01.709675 => 01:42:01.713676
[0m01:42:01.714672 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:42:01.802230 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:42:01.802230 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:01.802230 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:42:01.802230 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:42:01.802230 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:42:01.893810 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:42:01.893810 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:42:01.893810 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (preCombineField "watermark" , type "cow" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:42:01.897326 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:42:01.897326 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:42:01.714672 => 01:42:01.897326
[0m01:42:01.897326 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:42:01.897326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:42:01.897326 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:42:01.914388 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:42:01.914388 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74ff0e2d-748c-411b-bb4e-29c9eb942a5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82B1B4880>]}
[0m01:42:01.914388 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.21s]
[0m01:42:01.916898 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:42:01.916898 [debug] [MainThread]: On master: ROLLBACK
[0m01:42:01.916898 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:42:01.962089 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:42:01.963100 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:01.963100 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:42:01.964097 [debug] [MainThread]: On master: ROLLBACK
[0m01:42:01.964097 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:42:01.965101 [debug] [MainThread]: On master: Close
[0m01:42:01.979132 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:42:01.980129 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:42:01.980129 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:42:01.981128 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:42:01.981128 [info ] [MainThread]: 
[0m01:42:01.982129 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.57 seconds (0.57s).
[0m01:42:01.983129 [debug] [MainThread]: Command end result
[0m01:42:01.992339 [info ] [MainThread]: 
[0m01:42:01.993345 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:42:01.994346 [info ] [MainThread]: 
[0m01:42:01.995614 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:42:01.995614 [error] [MainThread]:   Database Error
[0m01:42:01.997539 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:42:01.998547 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:42:01.999549 [error] [MainThread]:     +- 'Project [*]
[0m01:42:02.000558 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:42:02.001544 [error] [MainThread]:     
[0m01:42:02.002558 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:42:02.002558 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:42:02.004548 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:42:02.004548 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:42:02.005545 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:42:02.006570 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:42:02.006570 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:42:02.008078 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:42:02.009085 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:42:02.011099 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:42:02.012085 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:42:02.013086 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:42:02.014407 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:42:02.015230 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:42:02.015230 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:42:02.016230 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:42:02.017231 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:42:02.018229 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:42:02.019240 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:42:02.019240 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:42:02.020231 [error] [MainThread]:     +- 'Project [*]
[0m01:42:02.021232 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:42:02.021232 [error] [MainThread]:     
[0m01:42:02.022235 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:42:02.023257 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:42:02.024233 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:42:02.025317 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:42:02.025317 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:42:02.026828 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:42:02.026828 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:42:02.027839 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:42:02.029851 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:42:02.030839 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:42:02.030839 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:42:02.031842 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:42:02.032838 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:42:02.032838 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:42:02.032838 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:42:02.034346 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:42:02.035351 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:42:02.036357 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:42:02.037358 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:42:02.037358 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:42:02.038359 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:42:02.040373 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:42:02.041881 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:42:02.042889 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:42:02.042889 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:42:02.043888 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:42:02.043888 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:42:02.044886 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:42:02.044886 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:02.045894 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:42:02.045894 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:42:02.046885 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:42:02.047886 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:42:02.047886 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:42:02.048887 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:02.051886 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:42:02.053916 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:42:02.054914 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:02.054914 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:42:02.055916 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:42:02.056927 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:42:02.057914 [error] [MainThread]:     	... 16 more
[0m01:42:02.058915 [error] [MainThread]:     
[0m01:42:02.059915 [info ] [MainThread]: 
[0m01:42:02.059915 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:42:02.061471 [debug] [MainThread]: Command `dbt run` failed at 01:42:02.061471 after 1.60 seconds
[0m01:42:02.061471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82859DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82771E9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C82ACB2D10>]}
[0m01:42:02.062485 [debug] [MainThread]: Flushing usage events
[0m01:42:29.251150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FC0EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FE802E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FE802B60>]}


============================== 01:42:29.251150 | 2d57386e-ea7c-40fd-bf30-8fe2b6464b81 ==============================
[0m01:42:29.251150 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:42:29.251150 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:42:29.392865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FE802DD0>]}
[0m01:42:29.392865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEBAAD40>]}
[0m01:42:29.392865 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:42:29.408502 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:42:29.472242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:42:29.472242 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\hudi_cow.sql
[0m01:42:29.472242 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:42:29.503598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEBA9690>]}
[0m01:42:29.503598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FED2BF70>]}
[0m01:42:29.503598 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:42:29.503598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEB9F190>]}
[0m01:42:29.503598 [info ] [MainThread]: 
[0m01:42:29.503598 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:42:29.519153 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:42:29.519153 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:42:29.519153 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:42:29.519153 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:29.630446 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:42:29.630446 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:42:29.638208 [debug] [ThreadPool]: On list_schemas: Close
[0m01:42:29.647544 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:42:29.654840 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:29.655979 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:42:29.656513 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:42:29.657035 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:29.751944 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:42:29.751944 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:42:29.751944 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:42:29.751944 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:42:29.751944 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:42:29.768575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEDEF6A0>]}
[0m01:42:29.770591 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:29.771106 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:42:29.772122 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:42:29.772122 [info ] [MainThread]: 
[0m01:42:29.772122 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m01:42:29.772122 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m01:42:29.778876 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m01:42:29.778876 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m01:42:29.782414 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m01:42:29.785422 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 01:42:29.778876 => 01:42:29.784421
[0m01:42:29.786412 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m01:42:29.865508 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m01:42:29.867507 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:29.867507 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m01:42:29.868509 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (primaryKey "application_id" , preCombineField "watermark" , type "cow" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:42:29.868509 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:42:29.937068 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:42:29.937068 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:42:29.937068 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (primaryKey "application_id" , preCombineField "watermark" , type "cow" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:42:29.937068 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:42:29.937068 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 01:42:29.786412 => 01:42:29.937068
[0m01:42:29.937068 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m01:42:29.937068 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:42:29.937068 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m01:42:29.945875 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:42:29.945875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2d57386e-ea7c-40fd-bf30-8fe2b6464b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEE4FBB0>]}
[0m01:42:29.954884 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.17s]
[0m01:42:29.954884 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m01:42:29.954884 [debug] [MainThread]: On master: ROLLBACK
[0m01:42:29.959906 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:42:30.002443 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:42:30.003441 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:42:30.003441 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:42:30.003441 [debug] [MainThread]: On master: ROLLBACK
[0m01:42:30.004442 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:42:30.004442 [debug] [MainThread]: On master: Close
[0m01:42:30.013982 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:42:30.014979 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:42:30.014979 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:42:30.015986 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m01:42:30.015986 [info ] [MainThread]: 
[0m01:42:30.016979 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.51 seconds (0.51s).
[0m01:42:30.018491 [debug] [MainThread]: Command end result
[0m01:42:30.026317 [info ] [MainThread]: 
[0m01:42:30.027315 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:42:30.028499 [info ] [MainThread]: 
[0m01:42:30.029512 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m01:42:30.029512 [error] [MainThread]:   Database Error
[0m01:42:30.029512 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:42:30.031017 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:42:30.031878 [error] [MainThread]:     +- 'Project [*]
[0m01:42:30.031878 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:42:30.032885 [error] [MainThread]:     
[0m01:42:30.033900 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:42:30.033900 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:42:30.034886 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:42:30.035890 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:42:30.035890 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:42:30.036886 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:42:30.036886 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:42:30.037885 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:42:30.038884 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:42:30.039890 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:42:30.039890 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:42:30.040890 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:42:30.041885 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:42:30.042888 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:42:30.042888 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:42:30.044886 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:42:30.045892 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:42:30.045892 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:42:30.046884 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m01:42:30.046884 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m01:42:30.047887 [error] [MainThread]:     +- 'Project [*]
[0m01:42:30.048886 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m01:42:30.048886 [error] [MainThread]:     
[0m01:42:30.049885 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m01:42:30.050903 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m01:42:30.051890 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:42:30.051890 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:42:30.052889 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:42:30.053898 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:42:30.053898 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:42:30.054888 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:42:30.055889 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:42:30.055889 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:42:30.056947 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:42:30.057895 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:42:30.058898 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:42:30.058898 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:42:30.059893 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:42:30.060895 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:42:30.061895 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:42:30.063894 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:42:30.064894 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:42:30.064894 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:42:30.065893 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:42:30.065893 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:42:30.066896 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:42:30.066896 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:42:30.067893 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:42:30.067893 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:42:30.068895 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:42:30.068895 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:42:30.069895 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:30.069895 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:42:30.070895 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:42:30.071400 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:42:30.072410 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:42:30.073406 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:42:30.073406 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:30.074407 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:42:30.074407 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:42:30.075407 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:42:30.075407 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:42:30.076410 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:42:30.077410 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:42:30.078410 [error] [MainThread]:     	... 16 more
[0m01:42:30.079407 [error] [MainThread]:     
[0m01:42:30.079407 [info ] [MainThread]: 
[0m01:42:30.080407 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:42:30.081406 [debug] [MainThread]: Command `dbt run` failed at 01:42:30.081406 after 0.84 seconds
[0m01:42:30.081406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FC0EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEB9EB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224FEE4DD50>]}
[0m01:42:30.081406 [debug] [MainThread]: Flushing usage events
[0m01:44:11.674453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D640DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8B22E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8B22BC0>]}


============================== 01:44:11.674453 | 90d74bfa-916a-46d9-8e7f-4589037dfbd4 ==============================
[0m01:44:11.674453 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:44:11.674453 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:44:11.796049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '90d74bfa-916a-46d9-8e7f-4589037dfbd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8B22E30>]}
[0m01:44:11.796049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '90d74bfa-916a-46d9-8e7f-4589037dfbd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8EC6E90>]}
[0m01:44:11.796049 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:44:11.825906 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:44:11.879299 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:44:11.880310 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:44:11.887402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90d74bfa-916a-46d9-8e7f-4589037dfbd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D90880D0>]}
[0m01:44:11.894400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90d74bfa-916a-46d9-8e7f-4589037dfbd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8F5F7F0>]}
[0m01:44:11.895405 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:44:11.896407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90d74bfa-916a-46d9-8e7f-4589037dfbd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8F5F790>]}
[0m01:44:11.897413 [info ] [MainThread]: 
[0m01:44:11.898709 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:44:11.904716 [info ] [MainThread]: Done.
[0m01:44:11.906720 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:44:11.906720 after 0.24 seconds
[0m01:44:11.907716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D640DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D65EA620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185D8F5CF70>]}
[0m01:44:11.908718 [debug] [MainThread]: Flushing usage events
[0m02:15:28.102281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D50F4BDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D511BD2E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D511BD2B90>]}


============================== 02:15:28.107289 | 335216af-6542-4da0-a3af-db1f4d1a04a1 ==============================
[0m02:15:28.107289 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:15:28.108288 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:15:28.295399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D511BD2E00>]}
[0m02:15:28.303407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D511F76D40>]}
[0m02:15:28.304525 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:15:28.317591 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m02:15:28.398972 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:15:28.399910 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\example\hudi_cow.sql
[0m02:15:28.415808 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m02:15:28.423649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D50E632740>]}
[0m02:15:28.438749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5120FBE20>]}
[0m02:15:28.438749 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m02:15:28.438749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5120FBEE0>]}
[0m02:15:28.438749 [info ] [MainThread]: 
[0m02:15:28.438749 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:15:28.438749 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m02:15:28.454382 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m02:15:28.454382 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m02:15:28.454382 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:15:28.555959 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:15:28.555959 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:15:28.567852 [debug] [ThreadPool]: On list_schemas: Close
[0m02:15:28.576292 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m02:15:28.580794 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:28.580794 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m02:15:28.587879 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m02:15:28.588310 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:15:28.690687 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:15:28.690687 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:15:28.699073 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m02:15:28.702205 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:15:28.702205 [debug] [ThreadPool]: On list_None_ndb: Close
[0m02:15:28.708214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5121BB6D0>]}
[0m02:15:28.708214 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:28.708214 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:15:28.708214 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:15:28.708214 [info ] [MainThread]: 
[0m02:15:28.719158 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m02:15:28.719158 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m02:15:28.728197 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m02:15:28.728197 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m02:15:28.734207 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m02:15:28.734207 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 02:15:28.728197 => 02:15:28.734207
[0m02:15:28.734207 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m02:15:28.809757 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m02:15:28.810681 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:28.811674 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m02:15:28.812760 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , preCombineField "watermark" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m02:15:28.813673 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:15:28.890097 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;\n'CreateTable `ndb`.`hudi_cow`, ErrorIfExists\n+- 'Project [*]\n   +- 'UnresolvedRelation [ndb, ntable], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m02:15:28.890097 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m02:15:28.890097 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , preCombineField "watermark" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m02:15:28.890097 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
  'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
  +- 'Project [*]
     +- 'UnresolvedRelation [ndb, ntable], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m02:15:28.890097 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 02:15:28.734207 => 02:15:28.890097
[0m02:15:28.890097 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m02:15:28.890097 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m02:15:28.890097 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m02:15:28.907004 [debug] [Thread-1 (]: Runtime Error in model hudi_cow (models\example\hudi_cow.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
    'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
    +- 'Project [*]
       +- 'UnresolvedRelation [ndb, ntable], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m02:15:28.907004 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '335216af-6542-4da0-a3af-db1f4d1a04a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5121CA620>]}
[0m02:15:28.907004 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.hudi_cow ....................... [[31mERROR[0m in 0.19s]
[0m02:15:28.907004 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m02:15:28.920612 [debug] [MainThread]: On master: ROLLBACK
[0m02:15:28.920612 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:15:28.961332 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:15:28.964015 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:28.964015 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:15:28.964015 [debug] [MainThread]: On master: ROLLBACK
[0m02:15:28.965012 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:15:28.965012 [debug] [MainThread]: On master: Close
[0m02:15:28.972536 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:15:28.973531 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m02:15:28.973531 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m02:15:28.974532 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m02:15:28.975041 [info ] [MainThread]: 
[0m02:15:28.977055 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.54 seconds (0.54s).
[0m02:15:28.978054 [debug] [MainThread]: Command end result
[0m02:15:28.985099 [info ] [MainThread]: 
[0m02:15:28.986301 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:15:28.987303 [info ] [MainThread]: 
[0m02:15:28.988304 [error] [MainThread]: [33mRuntime Error in model hudi_cow (models\example\hudi_cow.sql)[0m
[0m02:15:28.989300 [error] [MainThread]:   Database Error
[0m02:15:28.989300 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m02:15:28.990305 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m02:15:28.991303 [error] [MainThread]:     +- 'Project [*]
[0m02:15:28.992344 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m02:15:28.993301 [error] [MainThread]:     
[0m02:15:28.994461 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m02:15:28.995300 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m02:15:28.996304 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m02:15:28.997317 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m02:15:28.998302 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m02:15:28.999316 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m02:15:28.999316 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m02:15:29.000811 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m02:15:29.001819 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m02:15:29.002819 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m02:15:29.002819 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m02:15:29.003818 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m02:15:29.003818 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m02:15:29.005326 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m02:15:29.006392 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m02:15:29.007332 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m02:15:29.007332 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m02:15:29.008332 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m02:15:29.009332 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: ndb.ntable; line 19 pos 5;
[0m02:15:29.010339 [error] [MainThread]:     'CreateTable `ndb`.`hudi_cow`, ErrorIfExists
[0m02:15:29.011431 [error] [MainThread]:     +- 'Project [*]
[0m02:15:29.012333 [error] [MainThread]:        +- 'UnresolvedRelation [ndb, ntable], [], false
[0m02:15:29.013333 [error] [MainThread]:     
[0m02:15:29.014333 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m02:15:29.015332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m02:15:29.016340 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m02:15:29.017332 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m02:15:29.018331 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m02:15:29.018331 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m02:15:29.019332 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m02:15:29.020423 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m02:15:29.021378 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m02:15:29.022342 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m02:15:29.022342 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m02:15:29.023330 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m02:15:29.024334 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m02:15:29.025344 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m02:15:29.026412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m02:15:29.027842 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m02:15:29.028860 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m02:15:29.028860 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m02:15:29.029850 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m02:15:29.030851 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m02:15:29.030851 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m02:15:29.031850 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m02:15:29.032851 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m02:15:29.033852 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m02:15:29.033852 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m02:15:29.034849 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m02:15:29.034849 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m02:15:29.035851 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m02:15:29.036849 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m02:15:29.036849 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m02:15:29.037888 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m02:15:29.038849 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m02:15:29.038849 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m02:15:29.039852 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m02:15:29.039852 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m02:15:29.041130 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m02:15:29.042144 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m02:15:29.043170 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m02:15:29.044138 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m02:15:29.044138 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m02:15:29.045135 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m02:15:29.046141 [error] [MainThread]:     	... 16 more
[0m02:15:29.046141 [error] [MainThread]:     
[0m02:15:29.047147 [info ] [MainThread]: 
[0m02:15:29.048138 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:15:29.048138 [debug] [MainThread]: Command `dbt run` failed at 02:15:29.048138 after 0.96 seconds
[0m02:15:29.049175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D50F4BDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5121C92A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5121C9180>]}
[0m02:15:29.049175 [debug] [MainThread]: Flushing usage events
[0m02:18:41.296858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286203ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028622AC2C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028622AC2A10>]}


============================== 02:18:41.296858 | e4946bac-0a5a-4bfc-97e0-811c8946ff31 ==============================
[0m02:18:41.296858 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:18:41.296858 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:18:41.296858 [info ] [MainThread]: dbt version: 1.5.2
[0m02:18:41.296858 [info ] [MainThread]: python version: 3.10.11
[0m02:18:41.306059 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m02:18:41.306059 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m02:18:41.306059 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m02:18:41.309128 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m02:18:41.310133 [info ] [MainThread]: Configuration:
[0m02:18:41.409812 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m02:18:41.428670 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m02:18:41.430296 [info ] [MainThread]: Required dependencies:
[0m02:18:41.431219 [debug] [MainThread]: Executing "git --help"
[0m02:18:41.470379 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m02:18:41.471386 [debug] [MainThread]: STDERR: "b''"
[0m02:18:41.471386 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m02:18:41.472450 [info ] [MainThread]: Connection:
[0m02:18:41.473726 [info ] [MainThread]:   host: localhost
[0m02:18:41.475688 [info ] [MainThread]:   port: 10001
[0m02:18:41.476690 [info ] [MainThread]:   cluster: None
[0m02:18:41.477720 [info ] [MainThread]:   endpoint: None
[0m02:18:41.478762 [info ] [MainThread]:   schema: ndb
[0m02:18:41.479714 [info ] [MainThread]:   organization: 0
[0m02:18:41.481688 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:18:41.482693 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m02:18:41.482693 [debug] [MainThread]: Using spark connection "debug"
[0m02:18:41.483903 [debug] [MainThread]: On debug: select 1 as id
[0m02:18:41.483903 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:18:45.533621 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m02:18:45.533836 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m02:18:45.533836 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m02:18:45.533836 [info ] [MainThread]: [31m1 check failed:[0m
[0m02:18:45.533836 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m02:18:45.542090 [debug] [MainThread]: Command `dbt debug` failed at 02:18:45.542090 after 4.26 seconds
[0m02:18:45.543092 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m02:18:45.544129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286203ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028622E6E2F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028622E6DED0>]}
[0m02:18:45.544129 [debug] [MainThread]: Flushing usage events
[0m02:25:39.925007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCB4CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCDBE2C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCDBE2A40>]}


============================== 02:25:39.925007 | e99a4ce5-cb14-4893-a68a-1b64e6e16f79 ==============================
[0m02:25:39.925007 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:25:39.925007 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:25:39.925007 [info ] [MainThread]: dbt version: 1.5.2
[0m02:25:39.925007 [info ] [MainThread]: python version: 3.10.11
[0m02:25:39.925007 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m02:25:39.925007 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m02:25:39.925007 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m02:25:39.925007 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m02:25:39.940795 [info ] [MainThread]: Configuration:
[0m02:25:40.028556 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m02:25:40.044080 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m02:25:40.045306 [info ] [MainThread]: Required dependencies:
[0m02:25:40.046489 [debug] [MainThread]: Executing "git --help"
[0m02:25:40.080803 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m02:25:40.081904 [debug] [MainThread]: STDERR: "b''"
[0m02:25:40.082893 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m02:25:40.083560 [info ] [MainThread]: Connection:
[0m02:25:40.084567 [info ] [MainThread]:   host: localhost
[0m02:25:40.085571 [info ] [MainThread]:   port: 10001
[0m02:25:40.086567 [info ] [MainThread]:   cluster: None
[0m02:25:40.087573 [info ] [MainThread]:   endpoint: None
[0m02:25:40.088534 [info ] [MainThread]:   schema: ndb
[0m02:25:40.089547 [info ] [MainThread]:   organization: 0
[0m02:25:40.090858 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:25:40.091865 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m02:25:40.092865 [debug] [MainThread]: Using spark connection "debug"
[0m02:25:40.092865 [debug] [MainThread]: On debug: select 1 as id
[0m02:25:40.092865 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:25:44.128455 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m02:25:44.128455 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m02:25:44.128455 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m02:25:44.128455 [info ] [MainThread]: [31m1 check failed:[0m
[0m02:25:44.128455 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m02:25:44.141543 [debug] [MainThread]: Command `dbt debug` failed at 02:25:44.141543 after 4.22 seconds
[0m02:25:44.142478 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m02:25:44.142478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCB4CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCDF8E3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FCDF8DF90>]}
[0m02:25:44.143518 [debug] [MainThread]: Flushing usage events
[0m02:26:04.020373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177BAD3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177BD452C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177BD452A10>]}


============================== 02:26:04.023748 | 5e9d0932-fa57-4e49-b416-b2102c1fbec9 ==============================
[0m02:26:04.023748 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:26:04.026259 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:26:04.026259 [info ] [MainThread]: dbt version: 1.5.2
[0m02:26:04.027670 [info ] [MainThread]: python version: 3.10.11
[0m02:26:04.028678 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m02:26:04.029735 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m02:26:04.030767 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m02:26:04.031677 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m02:26:04.032692 [info ] [MainThread]: Configuration:
[0m02:26:04.129251 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m02:26:04.146859 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m02:26:04.148837 [info ] [MainThread]: Required dependencies:
[0m02:26:04.150842 [debug] [MainThread]: Executing "git --help"
[0m02:26:04.188698 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m02:26:04.189909 [debug] [MainThread]: STDERR: "b''"
[0m02:26:04.190470 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m02:26:04.191669 [info ] [MainThread]: Connection:
[0m02:26:04.192727 [info ] [MainThread]:   host: localhost
[0m02:26:04.193251 [info ] [MainThread]:   port: 10000
[0m02:26:04.194384 [info ] [MainThread]:   cluster: None
[0m02:26:04.195423 [info ] [MainThread]:   endpoint: None
[0m02:26:04.196601 [info ] [MainThread]:   schema: ndb
[0m02:26:04.197767 [info ] [MainThread]:   organization: 0
[0m02:26:04.198302 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:26:04.199474 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m02:26:04.200004 [debug] [MainThread]: Using spark connection "debug"
[0m02:26:04.200536 [debug] [MainThread]: On debug: select 1 as id
[0m02:26:04.200536 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:26:04.342639 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m02:26:04.343638 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m02:26:04.345147 [debug] [MainThread]: On debug: Close
[0m02:26:04.356253 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m02:26:04.357252 [info ] [MainThread]: [32mAll checks passed![0m
[0m02:26:04.358524 [debug] [MainThread]: Command `dbt debug` succeeded at 02:26:04.358524 after 0.35 seconds
[0m02:26:04.359522 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m02:26:04.360528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177BAD3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177B9EC3A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177B9EC0790>]}
[0m02:26:04.360528 [debug] [MainThread]: Flushing usage events
[0m02:26:10.124935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED9452DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED96C42E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED96C42B60>]}


============================== 02:26:10.124935 | 51579d29-01a0-411f-a31c-95c3365fb13c ==============================
[0m02:26:10.124935 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:26:10.124935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:26:10.242344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED96C42DD0>]}
[0m02:26:10.242344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED96FE1EA0>]}
[0m02:26:10.242344 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:26:10.253368 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m02:26:10.280785 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m02:26:10.281775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED946D57E0>]}
[0m02:26:10.922077 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m02:26:10.931170 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m02:26:10.931170 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m02:26:10.931170 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m02:26:11.025410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED971C4670>]}
[0m02:26:11.043543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED936ADB70>]}
[0m02:26:11.043543 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m02:26:11.043543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED96FE2650>]}
[0m02:26:11.043543 [info ] [MainThread]: 
[0m02:26:11.043543 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:26:11.043543 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m02:26:11.060882 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m02:26:11.060882 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m02:26:11.061801 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:26:11.161785 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:26:11.161785 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:26:11.174772 [debug] [ThreadPool]: On list_schemas: Close
[0m02:26:11.185778 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m02:26:11.185778 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:26:11.185778 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m02:26:11.185778 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m02:26:11.185778 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:26:11.316005 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:26:11.317004 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:26:11.324002 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m02:26:11.325003 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:26:11.326007 [debug] [ThreadPool]: On list_None_ndb: Close
[0m02:26:11.335089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED971578E0>]}
[0m02:26:11.336089 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:26:11.336089 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:26:11.337111 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:26:11.338142 [info ] [MainThread]: 
[0m02:26:11.344806 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_cow
[0m02:26:11.345814 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_cow ................................ [RUN]
[0m02:26:11.346848 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_cow'
[0m02:26:11.347819 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_cow
[0m02:26:11.350856 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_cow"
[0m02:26:11.351852 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (compile): 02:26:11.347819 => 02:26:11.351852
[0m02:26:11.352819 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_cow
[0m02:26:11.441853 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_cow"
[0m02:26:11.442837 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m02:26:11.443895 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_cow"
[0m02:26:11.443895 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_cow"} */

  
    
        create table ndb.hudi_cow
      
      
    using hudi
      options (type "cow" , primaryKey "application_id" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m02:26:11.445014 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:26:16.495905 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m02:26:20.237012 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m02:26:20.237012 [debug] [Thread-1 (]: SQL status: OK in 9.0 seconds
[0m02:26:20.263275 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_cow (execute): 02:26:11.352819 => 02:26:20.262275
[0m02:26:20.263275 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: ROLLBACK
[0m02:26:20.264272 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m02:26:20.264272 [debug] [Thread-1 (]: On model.poc_demo.hudi_cow: Close
[0m02:26:20.276487 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '51579d29-01a0-411f-a31c-95c3365fb13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED970C70D0>]}
[0m02:26:20.278486 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model ndb.hudi_cow ........................... [[32mOK[0m in 8.93s]
[0m02:26:20.279620 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_cow
[0m02:26:20.280627 [debug] [MainThread]: On master: ROLLBACK
[0m02:26:20.282135 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:26:20.327859 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:26:20.328856 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:26:20.328856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:26:20.329853 [debug] [MainThread]: On master: ROLLBACK
[0m02:26:20.329853 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:26:20.331063 [debug] [MainThread]: On master: Close
[0m02:26:20.337056 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:26:20.338057 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m02:26:20.339054 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m02:26:20.339054 [debug] [MainThread]: Connection 'model.poc_demo.hudi_cow' was properly closed.
[0m02:26:20.340063 [info ] [MainThread]: 
[0m02:26:20.341357 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 9.30 seconds (9.30s).
[0m02:26:20.343367 [debug] [MainThread]: Command end result
[0m02:26:20.349893 [info ] [MainThread]: 
[0m02:26:20.350884 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:26:20.350884 [info ] [MainThread]: 
[0m02:26:20.351877 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m02:26:20.352880 [debug] [MainThread]: Command `dbt run` succeeded at 02:26:20.352880 after 10.24 seconds
[0m02:26:20.352880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED9452DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED94870430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED936AEEC0>]}
[0m02:26:20.352880 [debug] [MainThread]: Flushing usage events
[0m02:28:14.461827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB5036DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52A82E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52A82BF0>]}


============================== 02:28:14.477472 | 31b108c2-abf5-454f-a90e-b70ae400ab92 ==============================
[0m02:28:14.477472 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:28:14.477472 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:28:14.587215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52A82E00>]}
[0m02:28:14.596162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52E29EA0>]}
[0m02:28:14.596499 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:28:14.596499 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m02:28:14.661615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:28:14.662611 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:28:14.668611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52FEC0D0>]}
[0m02:28:14.674609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52EC3790>]}
[0m02:28:14.675522 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m02:28:14.676880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52EC3730>]}
[0m02:28:14.678965 [info ] [MainThread]: 
[0m02:28:14.679891 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:28:14.681930 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m02:28:14.690975 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m02:28:14.691890 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m02:28:14.691890 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:28:14.791800 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:28:14.791800 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:28:14.804895 [debug] [ThreadPool]: On list_schemas: Close
[0m02:28:14.813494 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m02:28:14.821594 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:28:14.821594 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m02:28:14.821594 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m02:28:14.821594 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:28:14.953173 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:28:14.954135 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m02:28:14.962674 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m02:28:14.963318 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:28:14.963318 [debug] [ThreadPool]: On list_None_ndb: Close
[0m02:28:14.968831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB530232B0>]}
[0m02:28:14.968831 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:28:14.968831 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:28:14.968831 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:28:14.968831 [info ] [MainThread]: 
[0m02:28:14.978653 [debug] [Thread-1 (]: Began running node model.poc_demo.hudi_mor
[0m02:28:14.978653 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.hudi_mor ................................ [RUN]
[0m02:28:14.984481 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.hudi_mor'
[0m02:28:14.984481 [debug] [Thread-1 (]: Began compiling node model.poc_demo.hudi_mor
[0m02:28:14.989930 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.hudi_mor"
[0m02:28:14.992930 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_mor (compile): 02:28:14.984481 => 02:28:14.991915
[0m02:28:14.992930 [debug] [Thread-1 (]: Began executing node model.poc_demo.hudi_mor
[0m02:28:15.060768 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.hudi_mor"
[0m02:28:15.062773 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m02:28:15.063801 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.hudi_mor"
[0m02:28:15.064783 [debug] [Thread-1 (]: On model.poc_demo.hudi_mor: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.hudi_mor"} */

  
    
        create table ndb.hudi_mor
      
      
    using hudi
      options (primaryKey "application_id" , preCombineField "watermark" , type "mor" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m02:28:15.064783 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:28:18.125856 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m02:28:18.126851 [debug] [Thread-1 (]: SQL status: OK in 3.0 seconds
[0m02:28:18.143780 [debug] [Thread-1 (]: Timing info for model.poc_demo.hudi_mor (execute): 02:28:14.993917 => 02:28:18.143780
[0m02:28:18.144763 [debug] [Thread-1 (]: On model.poc_demo.hudi_mor: ROLLBACK
[0m02:28:18.145309 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m02:28:18.145309 [debug] [Thread-1 (]: On model.poc_demo.hudi_mor: Close
[0m02:28:18.162314 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31b108c2-abf5-454f-a90e-b70ae400ab92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52FD0F40>]}
[0m02:28:18.162314 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model ndb.hudi_mor ........................... [[32mOK[0m in 3.17s]
[0m02:28:18.162314 [debug] [Thread-1 (]: Finished running node model.poc_demo.hudi_mor
[0m02:28:18.162314 [debug] [MainThread]: On master: ROLLBACK
[0m02:28:18.162314 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:28:18.208841 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:28:18.209518 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:28:18.210523 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:28:18.210523 [debug] [MainThread]: On master: ROLLBACK
[0m02:28:18.210523 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:28:18.211523 [debug] [MainThread]: On master: Close
[0m02:28:18.218523 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:28:18.219527 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m02:28:18.219527 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m02:28:18.220561 [debug] [MainThread]: Connection 'model.poc_demo.hudi_mor' was properly closed.
[0m02:28:18.220561 [info ] [MainThread]: 
[0m02:28:18.221654 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.54 seconds (3.54s).
[0m02:28:18.222548 [debug] [MainThread]: Command end result
[0m02:28:18.228548 [info ] [MainThread]: 
[0m02:28:18.229569 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:28:18.230555 [info ] [MainThread]: 
[0m02:28:18.231550 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m02:28:18.233551 [debug] [MainThread]: Command `dbt run` succeeded at 02:28:18.232551 after 3.77 seconds
[0m02:28:18.233551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB5036DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB52FD0F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB506B15A0>]}
[0m02:28:18.234646 [debug] [MainThread]: Flushing usage events
[0m02:40:00.564078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231764ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023178BBEC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023178BBE9E0>]}


============================== 02:40:00.564078 | caad2edc-6d0f-4dde-9ea8-919e289ebe75 ==============================
[0m02:40:00.564078 [info ] [MainThread]: Running with dbt=1.5.2
[0m02:40:00.564078 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:40:00.564078 [info ] [MainThread]: dbt version: 1.5.2
[0m02:40:00.579797 [info ] [MainThread]: python version: 3.10.11
[0m02:40:00.579797 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m02:40:00.579797 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m02:40:00.583823 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m02:40:00.584928 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m02:40:00.586828 [info ] [MainThread]: Configuration:
[0m02:40:00.681564 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m02:40:00.704287 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m02:40:00.704287 [info ] [MainThread]: Required dependencies:
[0m02:40:00.709988 [debug] [MainThread]: Executing "git --help"
[0m02:40:00.753995 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m02:40:00.755017 [debug] [MainThread]: STDERR: "b''"
[0m02:40:00.755017 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m02:40:00.757100 [info ] [MainThread]: Connection:
[0m02:40:00.757632 [info ] [MainThread]:   host: localhost
[0m02:40:00.758638 [info ] [MainThread]:   port: 10000
[0m02:40:00.759729 [info ] [MainThread]:   cluster: None
[0m02:40:00.760657 [info ] [MainThread]:   endpoint: None
[0m02:40:00.761641 [info ] [MainThread]:   schema: ndb
[0m02:40:00.762704 [info ] [MainThread]:   organization: 0
[0m02:40:00.764646 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m02:40:00.764646 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m02:40:00.766132 [debug] [MainThread]: Using spark connection "debug"
[0m02:40:00.767139 [debug] [MainThread]: On debug: select 1 as id
[0m02:40:00.767139 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:40:04.958787 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m02:40:04.959537 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m02:40:04.960548 [debug] [MainThread]: On debug: Close
[0m02:40:05.004632 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m02:40:05.005644 [info ] [MainThread]: [32mAll checks passed![0m
[0m02:40:05.008703 [debug] [MainThread]: Command `dbt debug` succeeded at 02:40:05.007709 after 4.45 seconds
[0m02:40:05.008703 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m02:40:05.008703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231764ADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002317563F8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002317563E950>]}
[0m02:40:05.010209 [debug] [MainThread]: Flushing usage events
[0m03:04:57.591258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501855DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501AC72C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501AC729E0>]}


============================== 03:04:57.598313 | 20fbba09-2937-4024-8821-5d3e9e04a0bc ==============================
[0m03:04:57.598313 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:04:57.598313 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:04:57.598313 [info ] [MainThread]: dbt version: 1.5.2
[0m03:04:57.598313 [info ] [MainThread]: python version: 3.10.11
[0m03:04:57.598313 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:04:57.598313 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:04:57.606830 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:04:57.608084 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:04:57.609088 [info ] [MainThread]: Configuration:
[0m03:04:57.725677 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:04:57.742393 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:04:57.743407 [info ] [MainThread]: Required dependencies:
[0m03:04:57.744401 [debug] [MainThread]: Executing "git --help"
[0m03:04:57.779604 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:04:57.780608 [debug] [MainThread]: STDERR: "b''"
[0m03:04:57.781609 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:04:57.783650 [info ] [MainThread]: Connection:
[0m03:04:57.784609 [info ] [MainThread]:   host: localhost
[0m03:04:57.785650 [info ] [MainThread]:   port: 10000
[0m03:04:57.786622 [info ] [MainThread]:   cluster: None
[0m03:04:57.787628 [info ] [MainThread]:   endpoint: None
[0m03:04:57.788628 [info ] [MainThread]:   schema: ndb
[0m03:04:57.788628 [info ] [MainThread]:   organization: 0
[0m03:04:57.789630 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:04:57.791652 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:04:57.791652 [debug] [MainThread]: Using spark connection "debug"
[0m03:04:57.792696 [debug] [MainThread]: On debug: select 1 as id
[0m03:04:57.793649 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:04:58.549428 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:04:58.550435 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:04:58.551429 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:04:58.552428 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:04:58.552428 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:04:58.554426 [debug] [MainThread]: Command `dbt debug` failed at 03:04:58.554426 after 0.98 seconds
[0m03:04:58.555435 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:04:58.555435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501855DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501B016AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001501B015CC0>]}
[0m03:04:58.556428 [debug] [MainThread]: Flushing usage events
[0m03:10:07.901407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D481DADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4844BEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4844BE9B0>]}


============================== 03:10:07.901407 | 249ed4c7-5973-4570-ae74-33a3833c84bc ==============================
[0m03:10:07.901407 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:10:07.901407 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:10:07.901407 [info ] [MainThread]: dbt version: 1.5.2
[0m03:10:07.901407 [info ] [MainThread]: python version: 3.10.11
[0m03:10:07.901407 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:10:07.901407 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:10:07.901407 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:10:07.914464 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:10:07.914464 [info ] [MainThread]: Configuration:
[0m03:10:08.008922 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:10:08.024556 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:10:08.024556 [info ] [MainThread]: Required dependencies:
[0m03:10:08.024556 [debug] [MainThread]: Executing "git --help"
[0m03:10:08.071767 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:10:08.071767 [debug] [MainThread]: STDERR: "b''"
[0m03:10:08.071767 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:10:08.071767 [info ] [MainThread]: Connection:
[0m03:10:08.071767 [info ] [MainThread]:   host: localhost
[0m03:10:08.071767 [info ] [MainThread]:   port: 10000
[0m03:10:08.071767 [info ] [MainThread]:   cluster: None
[0m03:10:08.071767 [info ] [MainThread]:   endpoint: None
[0m03:10:08.071767 [info ] [MainThread]:   schema: ndb
[0m03:10:08.071767 [info ] [MainThread]:   organization: 0
[0m03:10:08.071767 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:10:08.071767 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:10:08.071767 [debug] [MainThread]: Using spark connection "debug"
[0m03:10:08.071767 [debug] [MainThread]: On debug: select 1 as id
[0m03:10:08.071767 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:10:08.770305 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:10:08.770305 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:10:08.776825 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:10:08.776825 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:10:08.776825 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:10:08.776825 [debug] [MainThread]: Command `dbt debug` failed at 03:10:08.776825 after 0.89 seconds
[0m03:10:08.780348 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:10:08.780881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D481DADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D48486E500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D48486E7A0>]}
[0m03:10:08.780881 [debug] [MainThread]: Flushing usage events
[0m03:10:38.124657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835FDFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001836250EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001836250E9B0>]}


============================== 03:10:38.124657 | 6f7bf5d8-0344-4166-9b5c-026000f46157 ==============================
[0m03:10:38.124657 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:10:38.124657 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:10:38.124657 [info ] [MainThread]: dbt version: 1.5.2
[0m03:10:38.124657 [info ] [MainThread]: python version: 3.10.11
[0m03:10:38.140283 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:10:38.140283 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:10:38.140283 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:10:38.140283 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:10:38.140283 [info ] [MainThread]: Configuration:
[0m03:10:38.250113 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:10:38.269863 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:10:38.271029 [info ] [MainThread]: Required dependencies:
[0m03:10:38.271535 [debug] [MainThread]: Executing "git --help"
[0m03:10:38.297259 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:10:38.297259 [debug] [MainThread]: STDERR: "b''"
[0m03:10:38.297259 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:10:38.297259 [info ] [MainThread]: Connection:
[0m03:10:38.297259 [info ] [MainThread]:   host: localhost
[0m03:10:38.297259 [info ] [MainThread]:   port: 10000
[0m03:10:38.297259 [info ] [MainThread]:   cluster: None
[0m03:10:38.297259 [info ] [MainThread]:   endpoint: None
[0m03:10:38.297259 [info ] [MainThread]:   schema: ndb
[0m03:10:38.312944 [info ] [MainThread]:   organization: 0
[0m03:10:38.314053 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:10:38.314053 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:10:38.314053 [debug] [MainThread]: Using spark connection "debug"
[0m03:10:38.314053 [debug] [MainThread]: On debug: select 1 as id
[0m03:10:38.314053 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:10:41.776693 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:10:41.778715 [debug] [MainThread]: SQL status: OK in 3.0 seconds
[0m03:10:41.778715 [debug] [MainThread]: On debug: Close
[0m03:10:41.801861 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m03:10:41.801861 [info ] [MainThread]: [32mAll checks passed![0m
[0m03:10:41.801861 [debug] [MainThread]: Command `dbt debug` succeeded at 03:10:41.801861 after 3.69 seconds
[0m03:10:41.801861 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:10:41.801861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835FDFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835EF83A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835EF80790>]}
[0m03:10:41.801861 [debug] [MainThread]: Flushing usage events
[0m03:16:32.701702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001967DE6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000196005FEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000196005FEA10>]}


============================== 03:16:32.701702 | 31fdaa99-7094-45bd-8141-2b42de181d4b ==============================
[0m03:16:32.701702 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:16:32.701702 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:16:32.701702 [info ] [MainThread]: dbt version: 1.5.2
[0m03:16:32.701702 [info ] [MainThread]: python version: 3.10.11
[0m03:16:32.701702 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:16:32.701702 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:16:32.701702 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:16:32.701702 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:16:32.717345 [info ] [MainThread]: Configuration:
[0m03:16:32.811639 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:16:32.827180 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:16:32.827180 [info ] [MainThread]: Required dependencies:
[0m03:16:32.827180 [debug] [MainThread]: Executing "git --help"
[0m03:16:32.858539 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:16:32.858539 [debug] [MainThread]: STDERR: "b''"
[0m03:16:32.858539 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:16:32.858539 [info ] [MainThread]: Connection:
[0m03:16:32.874209 [info ] [MainThread]:   host: localhost
[0m03:16:32.874209 [info ] [MainThread]:   port: 10000
[0m03:16:32.874209 [info ] [MainThread]:   cluster: None
[0m03:16:32.874209 [info ] [MainThread]:   endpoint: None
[0m03:16:32.874209 [info ] [MainThread]:   schema: ndb
[0m03:16:32.874209 [info ] [MainThread]:   organization: 0
[0m03:16:32.874209 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:16:32.874209 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:16:32.874209 [debug] [MainThread]: Using spark connection "debug"
[0m03:16:32.874209 [debug] [MainThread]: On debug: select 1 as id
[0m03:16:32.874209 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:16:32.894056 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:16:32.895092 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:16:32.895693 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:16:32.896899 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:16:32.897418 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:16:32.898542 [debug] [MainThread]: Command `dbt debug` failed at 03:16:32.898542 after 0.21 seconds
[0m03:16:32.898542 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:16:32.899061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001967DE6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019600ABD5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019600ABD600>]}
[0m03:16:32.899577 [debug] [MainThread]: Flushing usage events
[0m03:18:02.417329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472844DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472AB5EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472AB5EA10>]}


============================== 03:18:02.417329 | 4dc48da6-f955-41fd-bb05-fa72979b5f6a ==============================
[0m03:18:02.417329 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:18:02.417329 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:18:02.417329 [info ] [MainThread]: dbt version: 1.5.2
[0m03:18:02.417329 [info ] [MainThread]: python version: 3.10.11
[0m03:18:02.417329 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:18:02.417329 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:18:02.417329 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:18:02.417329 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:18:02.433004 [info ] [MainThread]: Configuration:
[0m03:18:02.527932 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:18:02.543023 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:18:02.543023 [info ] [MainThread]: Required dependencies:
[0m03:18:02.543023 [debug] [MainThread]: Executing "git --help"
[0m03:18:02.576420 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:18:02.576420 [debug] [MainThread]: STDERR: "b''"
[0m03:18:02.576420 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:18:02.576420 [info ] [MainThread]: Connection:
[0m03:18:02.576420 [info ] [MainThread]:   host: localhost
[0m03:18:02.590931 [info ] [MainThread]:   port: 10000
[0m03:18:02.590931 [info ] [MainThread]:   cluster: None
[0m03:18:02.590931 [info ] [MainThread]:   endpoint: None
[0m03:18:02.590931 [info ] [MainThread]:   schema: ndb
[0m03:18:02.590931 [info ] [MainThread]:   organization: 0
[0m03:18:02.590931 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:18:02.590931 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:18:02.590931 [debug] [MainThread]: Using spark connection "debug"
[0m03:18:02.590931 [debug] [MainThread]: On debug: select 1 as id
[0m03:18:02.590931 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:18:03.203535 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:18:03.203535 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:18:03.203535 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:18:03.203535 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:18:03.203535 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:18:03.203535 [debug] [MainThread]: Command `dbt debug` failed at 03:18:03.203535 after 0.80 seconds
[0m03:18:03.203535 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:18:03.203535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472844DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472AF0EA40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002472AF0DCC0>]}
[0m03:18:03.203535 [debug] [MainThread]: Flushing usage events
[0m03:25:46.600810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8982DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8BF3EC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8BF3E9E0>]}


============================== 03:25:46.600810 | 5b4e24bd-736a-49fc-b3a9-f763b7c48506 ==============================
[0m03:25:46.600810 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:25:46.600810 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:25:46.600810 [info ] [MainThread]: dbt version: 1.5.2
[0m03:25:46.600810 [info ] [MainThread]: python version: 3.10.11
[0m03:25:46.600810 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:25:46.610339 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:25:46.610339 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:25:46.610339 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:25:46.610339 [info ] [MainThread]: Configuration:
[0m03:25:46.711698 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:25:46.727434 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:25:46.727434 [info ] [MainThread]: Required dependencies:
[0m03:25:46.727434 [debug] [MainThread]: Executing "git --help"
[0m03:25:46.759694 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:25:46.759694 [debug] [MainThread]: STDERR: "b''"
[0m03:25:46.759694 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:25:46.759694 [info ] [MainThread]: Connection:
[0m03:25:46.759694 [info ] [MainThread]:   host: localhost
[0m03:25:46.759694 [info ] [MainThread]:   port: 10000
[0m03:25:46.774750 [info ] [MainThread]:   cluster: None
[0m03:25:46.774750 [info ] [MainThread]:   endpoint: None
[0m03:25:46.774750 [info ] [MainThread]:   schema: ndb
[0m03:25:46.774750 [info ] [MainThread]:   organization: 0
[0m03:25:46.774750 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:25:46.774750 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:25:46.774750 [debug] [MainThread]: Using spark connection "debug"
[0m03:25:46.774750 [debug] [MainThread]: On debug: select 1 as id
[0m03:25:46.774750 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:25:47.417732 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:25:47.417732 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:25:47.422247 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:25:47.422247 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:25:47.422247 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:25:47.422247 [debug] [MainThread]: Command `dbt debug` failed at 03:25:47.422247 after 0.84 seconds
[0m03:25:47.425758 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:25:47.425758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8982DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8C2EE7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E8C2EEB60>]}
[0m03:25:47.425758 [debug] [MainThread]: Flushing usage events
[0m03:37:52.407220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01BF0DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01E61EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01E61E9B0>]}


============================== 03:37:52.414732 | 121675bb-4b25-497c-9482-0a496a205335 ==============================
[0m03:37:52.414732 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:37:52.414732 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:37:52.416237 [info ] [MainThread]: dbt version: 1.5.2
[0m03:37:52.416742 [info ] [MainThread]: python version: 3.10.11
[0m03:37:52.417461 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:37:52.417461 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:37:52.417461 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:37:52.417461 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:37:52.417461 [info ] [MainThread]: Configuration:
[0m03:37:52.509359 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:37:52.516366 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:37:52.516366 [info ] [MainThread]: Required dependencies:
[0m03:37:52.525409 [debug] [MainThread]: Executing "git --help"
[0m03:37:52.556692 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:37:52.556692 [debug] [MainThread]: STDERR: "b''"
[0m03:37:52.556692 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:37:52.556692 [info ] [MainThread]: Connection:
[0m03:37:52.556692 [info ] [MainThread]:   host: localhost
[0m03:37:52.556692 [info ] [MainThread]:   port: 10000
[0m03:37:52.556692 [info ] [MainThread]:   cluster: None
[0m03:37:52.556692 [info ] [MainThread]:   endpoint: None
[0m03:37:52.556692 [info ] [MainThread]:   schema: ndb
[0m03:37:52.556692 [info ] [MainThread]:   organization: 0
[0m03:37:52.556692 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:37:52.572327 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:37:52.572327 [debug] [MainThread]: Using spark connection "debug"
[0m03:37:52.572327 [debug] [MainThread]: On debug: select 1 as id
[0m03:37:52.572327 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:37:52.584498 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:37:52.585577 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:37:52.586103 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:37:52.586752 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:37:52.587773 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:37:52.588486 [debug] [MainThread]: Command `dbt debug` failed at 03:37:52.588486 after 0.19 seconds
[0m03:37:52.589708 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:37:52.590233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01BF0DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01E9CD5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E01E9CD600>]}
[0m03:37:52.590773 [debug] [MainThread]: Flushing usage events
[0m03:39:24.394557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218220DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218491EBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218491E950>]}


============================== 03:39:24.410976 | d1129870-a6e4-4d33-b33f-4f83d6046b54 ==============================
[0m03:39:24.410976 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:39:24.410976 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:39:24.410976 [info ] [MainThread]: dbt version: 1.5.2
[0m03:39:24.410976 [info ] [MainThread]: python version: 3.10.11
[0m03:39:24.410976 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:39:24.410976 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:39:24.410976 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:39:24.410976 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:39:24.410976 [info ] [MainThread]: Configuration:
[0m03:39:24.511257 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:39:24.521269 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:39:24.536904 [info ] [MainThread]: Required dependencies:
[0m03:39:24.536904 [debug] [MainThread]: Executing "git --help"
[0m03:39:24.569407 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:39:24.569407 [debug] [MainThread]: STDERR: "b''"
[0m03:39:24.569407 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:39:24.569407 [info ] [MainThread]: Connection:
[0m03:39:24.569407 [info ] [MainThread]:   host: localhost
[0m03:39:24.569407 [info ] [MainThread]:   port: 10000
[0m03:39:24.569407 [info ] [MainThread]:   cluster: None
[0m03:39:24.569407 [info ] [MainThread]:   endpoint: None
[0m03:39:24.569407 [info ] [MainThread]:   schema: ndb
[0m03:39:24.569407 [info ] [MainThread]:   organization: 0
[0m03:39:24.569407 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:39:24.569407 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:39:24.569407 [debug] [MainThread]: Using spark connection "debug"
[0m03:39:24.569407 [debug] [MainThread]: On debug: select 1 as id
[0m03:39:24.569407 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:39:28.222498 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:39:28.222498 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m03:39:28.222498 [debug] [MainThread]: On debug: Close
[0m03:39:28.254246 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m03:39:28.254246 [info ] [MainThread]: [32mAll checks passed![0m
[0m03:39:28.254246 [debug] [MainThread]: Command `dbt debug` succeeded at 03:39:28.254246 after 3.86 seconds
[0m03:39:28.254246 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:39:28.254246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218220DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218139F9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002218139C940>]}
[0m03:39:28.254246 [debug] [MainThread]: Flushing usage events
[0m03:39:46.004892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F6A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F917EBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F917E920>]}


============================== 03:39:46.011431 | 896395bd-6eb0-4d8c-9062-97161565e809 ==============================
[0m03:39:46.011431 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:39:46.011431 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:39:46.011431 [info ] [MainThread]: dbt version: 1.5.2
[0m03:39:46.011431 [info ] [MainThread]: python version: 3.10.11
[0m03:39:46.011431 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:39:46.011431 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:39:46.011431 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:39:46.011431 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:39:46.011431 [info ] [MainThread]: Configuration:
[0m03:39:46.105011 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:39:46.120629 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:39:46.120629 [info ] [MainThread]: Required dependencies:
[0m03:39:46.120629 [debug] [MainThread]: Executing "git --help"
[0m03:39:46.167751 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:39:46.167751 [debug] [MainThread]: STDERR: "b''"
[0m03:39:46.167751 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:39:46.167751 [info ] [MainThread]: Connection:
[0m03:39:46.167751 [info ] [MainThread]:   host: localhost
[0m03:39:46.167751 [info ] [MainThread]:   port: 10000
[0m03:39:46.167751 [info ] [MainThread]:   cluster: None
[0m03:39:46.167751 [info ] [MainThread]:   endpoint: None
[0m03:39:46.167751 [info ] [MainThread]:   schema: ndb
[0m03:39:46.167751 [info ] [MainThread]:   organization: 0
[0m03:39:46.167751 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:39:46.167751 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:39:46.167751 [debug] [MainThread]: Using spark connection "debug"
[0m03:39:46.167751 [debug] [MainThread]: On debug: select 1 as id
[0m03:39:46.167751 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:39:46.188808 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:39:46.190094 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:39:46.190743 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:39:46.191462 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:39:46.192128 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:39:46.193319 [debug] [MainThread]: Command `dbt debug` failed at 03:39:46.193232 after 0.20 seconds
[0m03:39:46.193898 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:39:46.194402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F6A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F9525240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254F95240A0>]}
[0m03:39:46.195096 [debug] [MainThread]: Flushing usage events
[0m03:40:13.671024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E77E4DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E7A55AC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E7A55A9E0>]}


============================== 03:40:13.678868 | 2e2c44d5-c610-4109-aba2-1c28a31f512c ==============================
[0m03:40:13.678868 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:40:13.679961 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:40:13.681183 [info ] [MainThread]: dbt version: 1.5.2
[0m03:40:13.681183 [info ] [MainThread]: python version: 3.10.11
[0m03:40:13.683126 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:40:13.683126 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:40:13.683126 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:40:13.684989 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:40:13.684989 [info ] [MainThread]: Configuration:
[0m03:40:13.763930 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:40:13.771444 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:40:13.771444 [info ] [MainThread]: Required dependencies:
[0m03:40:13.771444 [debug] [MainThread]: Executing "git --help"
[0m03:40:13.814402 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:40:13.819142 [debug] [MainThread]: STDERR: "b''"
[0m03:40:13.819142 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:40:13.820479 [info ] [MainThread]: Connection:
[0m03:40:13.822228 [info ] [MainThread]:   host: localhost
[0m03:40:13.822971 [info ] [MainThread]:   port: 10000
[0m03:40:13.824112 [info ] [MainThread]:   cluster: None
[0m03:40:13.824112 [info ] [MainThread]:   endpoint: None
[0m03:40:13.824112 [info ] [MainThread]:   schema: ndb
[0m03:40:13.824112 [info ] [MainThread]:   organization: 0
[0m03:40:13.824112 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:40:13.824112 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:40:13.824112 [debug] [MainThread]: Using spark connection "debug"
[0m03:40:13.824112 [debug] [MainThread]: On debug: select 1 as id
[0m03:40:13.824112 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:40:17.347446 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:40:17.347446 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m03:40:17.351961 [debug] [MainThread]: On debug: Close
[0m03:40:17.381635 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m03:40:17.382660 [info ] [MainThread]: [32mAll checks passed![0m
[0m03:40:17.382660 [debug] [MainThread]: Command `dbt debug` succeeded at 03:40:17.382660 after 3.72 seconds
[0m03:40:17.382660 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:40:17.382660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E77E4DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E76FDF9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E76FDC9A0>]}
[0m03:40:17.382660 [debug] [MainThread]: Flushing usage events
[0m03:46:52.444974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC3E91DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC4102EC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC4102EA40>]}


============================== 03:46:52.454588 | d4843323-c412-47f6-bebc-57505af1afda ==============================
[0m03:46:52.454588 [info ] [MainThread]: Running with dbt=1.5.2
[0m03:46:52.454588 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m03:46:52.454588 [info ] [MainThread]: dbt version: 1.5.2
[0m03:46:52.454588 [info ] [MainThread]: python version: 3.10.11
[0m03:46:52.454588 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m03:46:52.454588 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m03:46:52.454588 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m03:46:52.454588 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m03:46:52.454588 [info ] [MainThread]: Configuration:
[0m03:46:52.548983 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m03:46:52.564769 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m03:46:52.564769 [info ] [MainThread]: Required dependencies:
[0m03:46:52.564769 [debug] [MainThread]: Executing "git --help"
[0m03:46:52.596938 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:46:52.596938 [debug] [MainThread]: STDERR: "b''"
[0m03:46:52.596938 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m03:46:52.596938 [info ] [MainThread]: Connection:
[0m03:46:52.596938 [info ] [MainThread]:   host: localhost
[0m03:46:52.596938 [info ] [MainThread]:   port: 10000
[0m03:46:52.596938 [info ] [MainThread]:   cluster: None
[0m03:46:52.596938 [info ] [MainThread]:   endpoint: None
[0m03:46:52.612829 [info ] [MainThread]:   schema: ndb
[0m03:46:52.612829 [info ] [MainThread]:   organization: 0
[0m03:46:52.614439 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m03:46:52.615992 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m03:46:52.617862 [debug] [MainThread]: Using spark connection "debug"
[0m03:46:52.618876 [debug] [MainThread]: On debug: select 1 as id
[0m03:46:52.618876 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:46:52.629898 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m03:46:52.631451 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m03:46:52.632540 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m03:46:52.633081 [info ] [MainThread]: [31m1 check failed:[0m
[0m03:46:52.633610 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m03:46:52.634872 [debug] [MainThread]: Command `dbt debug` failed at 03:46:52.634872 after 0.20 seconds
[0m03:46:52.635399 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m03:46:52.635937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC3E91DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC413DC4C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CC413DF5B0>]}
[0m03:46:52.636470 [debug] [MainThread]: Flushing usage events
[0m04:02:05.668681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A219DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A48AEBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A48AE9B0>]}


============================== 04:02:05.668681 | 904f30a6-1fb1-48d3-ac32-cfcd885fc2ea ==============================
[0m04:02:05.668681 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:02:05.668681 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:02:05.668681 [info ] [MainThread]: dbt version: 1.5.2
[0m04:02:05.668681 [info ] [MainThread]: python version: 3.10.11
[0m04:02:05.668681 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m04:02:05.677732 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m04:02:05.677732 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m04:02:05.679074 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m04:02:05.679074 [info ] [MainThread]: Configuration:
[0m04:02:05.779691 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:02:05.795406 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:02:05.795406 [info ] [MainThread]: Required dependencies:
[0m04:02:05.795406 [debug] [MainThread]: Executing "git --help"
[0m04:02:05.838797 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:02:05.838797 [debug] [MainThread]: STDERR: "b''"
[0m04:02:05.838797 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:02:05.840518 [info ] [MainThread]: Connection:
[0m04:02:05.840518 [info ] [MainThread]:   host: localhost
[0m04:02:05.842524 [info ] [MainThread]:   port: 10000
[0m04:02:05.842524 [info ] [MainThread]:   cluster: None
[0m04:02:05.842524 [info ] [MainThread]:   endpoint: None
[0m04:02:05.842524 [info ] [MainThread]:   schema: ndb
[0m04:02:05.845929 [info ] [MainThread]:   organization: 0
[0m04:02:05.845929 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:02:05.847144 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m04:02:05.847144 [debug] [MainThread]: Using spark connection "debug"
[0m04:02:05.847144 [debug] [MainThread]: On debug: select 1 as id
[0m04:02:05.847144 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:02:09.790712 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m04:02:09.790712 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m04:02:09.790712 [debug] [MainThread]: On debug: Close
[0m04:02:09.816275 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:02:09.822793 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:02:09.822793 [debug] [MainThread]: Command `dbt debug` succeeded at 04:02:09.822793 after 4.17 seconds
[0m04:02:09.822793 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:02:09.822793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A219DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A13239A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234A1320940>]}
[0m04:02:09.827250 [debug] [MainThread]: Flushing usage events
[0m04:04:01.014823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB7F08DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB017D2C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB017D29B0>]}


============================== 04:04:01.028916 | f771e430-e9bd-43e5-995b-0b9625d2ee2e ==============================
[0m04:04:01.028916 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:04:01.031014 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:04:01.031014 [info ] [MainThread]: dbt version: 1.5.2
[0m04:04:01.031014 [info ] [MainThread]: python version: 3.10.11
[0m04:04:01.031014 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m04:04:01.034592 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m04:04:01.034592 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m04:04:01.034592 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m04:04:01.034592 [info ] [MainThread]: Configuration:
[0m04:04:01.134188 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:04:01.143694 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:04:01.143694 [info ] [MainThread]: Required dependencies:
[0m04:04:01.157179 [debug] [MainThread]: Executing "git --help"
[0m04:04:01.189525 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:04:01.189525 [debug] [MainThread]: STDERR: "b''"
[0m04:04:01.189525 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:04:01.197523 [info ] [MainThread]: Connection:
[0m04:04:01.197523 [info ] [MainThread]:   host: localhost
[0m04:04:01.197523 [info ] [MainThread]:   port: 10000
[0m04:04:01.197523 [info ] [MainThread]:   cluster: None
[0m04:04:01.197523 [info ] [MainThread]:   endpoint: None
[0m04:04:01.197523 [info ] [MainThread]:   schema: ndb
[0m04:04:01.197523 [info ] [MainThread]:   organization: 0
[0m04:04:01.197523 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:04:01.205659 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m04:04:01.205659 [debug] [MainThread]: Using spark connection "debug"
[0m04:04:01.205659 [debug] [MainThread]: On debug: select 1 as id
[0m04:04:01.205659 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:04:01.429252 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m04:04:01.429252 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:04:01.429252 [debug] [MainThread]: On debug: Close
[0m04:04:01.444324 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:04:01.444324 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:04:01.444324 [debug] [MainThread]: Command `dbt debug` succeeded at 04:04:01.444324 after 0.44 seconds
[0m04:04:01.444324 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:04:01.444324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB7F08DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB01C57A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB01C54790>]}
[0m04:04:01.444324 [debug] [MainThread]: Flushing usage events
[0m04:04:22.892209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C180DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C3F22E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C3F22BF0>]}


============================== 04:04:22.892209 | 543cedaf-c9ce-4f69-b6ae-8b233912e068 ==============================
[0m04:04:22.892209 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:04:22.892209 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:04:23.019430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C3F22E00>]}
[0m04:04:23.028035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C42C9EA0>]}
[0m04:04:23.028035 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:04:23.028035 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m04:04:23.132891 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m04:04:23.135960 [debug] [MainThread]: Partial parsing: added file: poc_demo://models\example\delta_lake.sql
[0m04:04:23.149101 [debug] [MainThread]: 1699: static parser successfully parsed example\delta_lake.sql
[0m04:04:23.180632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C435FB80>]}
[0m04:04:23.180632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C4447EE0>]}
[0m04:04:23.180632 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m04:04:23.180632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C4447FA0>]}
[0m04:04:23.180632 [info ] [MainThread]: 
[0m04:04:23.180632 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:04:23.180632 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m04:04:23.204925 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m04:04:23.204925 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m04:04:23.204925 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:04:23.509557 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:04:23.509557 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:04:23.625307 [debug] [ThreadPool]: On list_schemas: Close
[0m04:04:23.647958 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m04:04:23.654173 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:04:23.654173 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m04:04:23.654173 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m04:04:23.654173 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:04:24.418669 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:04:24.418669 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m04:04:24.435749 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m04:04:24.437270 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:04:24.437270 [debug] [ThreadPool]: On list_None_ndb: Close
[0m04:04:24.457318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C44DA140>]}
[0m04:04:24.457318 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:04:24.459824 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:04:24.460339 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:04:24.460339 [info ] [MainThread]: 
[0m04:04:24.460339 [debug] [Thread-1 (]: Began running node model.poc_demo.delta_lake
[0m04:04:24.469870 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.delta_lake .............................. [RUN]
[0m04:04:24.471537 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.delta_lake'
[0m04:04:24.472044 [debug] [Thread-1 (]: Began compiling node model.poc_demo.delta_lake
[0m04:04:24.479572 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.delta_lake"
[0m04:04:24.482471 [debug] [Thread-1 (]: Timing info for model.poc_demo.delta_lake (compile): 04:04:24.472044 => 04:04:24.481465
[0m04:04:24.482471 [debug] [Thread-1 (]: Began executing node model.poc_demo.delta_lake
[0m04:04:24.573670 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.delta_lake"
[0m04:04:24.577216 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m04:04:24.577216 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.delta_lake"
[0m04:04:24.577216 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.delta_lake"} */

  
    
        create or replace table ndb.delta_lake
      
      
    using delta
      
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m04:04:24.577216 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:04:25.216637 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore\nPlease ensure that the delta-storage dependency is included.\n\nIf using Python, please ensure you call `configure_spark_with_delta_pip` or use\n`--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.\nSee https://docs.delta.io/latest/quick-start.html#python.\n\nMore information about this dependency and how to include it can be found here:\nhttps://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:44)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore\nPlease ensure that the delta-storage dependency is included.\n\nIf using Python, please ensure you call `configure_spark_with_delta_pip` or use\n`--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.\nSee https://docs.delta.io/latest/quick-start.html#python.\n\nMore information about this dependency and how to include it can be found here:\nhttps://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.\n\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:808)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:818)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:722)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:660)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$8(DeltaCatalog.scala:150)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:148)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:86)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:472)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:434)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:507)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:491)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:486)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:199)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:232)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\nCaused by: java.lang.NoClassDefFoundError: io/delta/storage/LogStore\nPlease ensure that the delta-storage dependency is included.\n\nIf using Python, please ensure you call `configure_spark_with_delta_pip` or use\n`--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.\nSee https://docs.delta.io/latest/quick-start.html#python.\n\nMore information about this dependency and how to include it can be found here:\nhttps://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.\n\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar(DeltaErrors.scala:2498)\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar$(DeltaErrors.scala:2495)\n\tat org.apache.spark.sql.delta.DeltaErrors$.missingDeltaStorageJar(DeltaErrors.scala:2681)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore$.<init>(DelegatingLogStore.scala:163)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore$.<clinit>(DelegatingLogStore.scala)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.$anonfun$schemeBasedLogStore$1(DelegatingLogStore.scala:67)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.schemeBasedLogStore(DelegatingLogStore.scala:67)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:85)\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:394)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:393)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:387)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:386)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:263)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:261)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:797)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:792)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:602)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:602)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:602)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:791)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:809)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 72 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m04:04:25.216637 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m04:04:25.216637 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.delta_lake"} */

  
    
        create or replace table ndb.delta_lake
      
      
    using delta
      
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m04:04:25.216637 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
  Please ensure that the delta-storage dependency is included.
  
  If using Python, please ensure you call `configure_spark_with_delta_pip` or use
  `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
  See https://docs.delta.io/latest/quick-start.html#python.
  
  More information about this dependency and how to include it can be found here:
  https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:44)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
  Please ensure that the delta-storage dependency is included.
  
  If using Python, please ensure you call `configure_spark_with_delta_pip` or use
  `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
  See https://docs.delta.io/latest/quick-start.html#python.
  
  More information about this dependency and how to include it can be found here:
  https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
  
  	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
  	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
  	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
  	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:808)
  	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:818)
  	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:722)
  	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:660)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$8(DeltaCatalog.scala:150)
  	at scala.Option.map(Option.scala:230)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:148)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:86)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:472)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:434)
  	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:507)
  	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
  	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:491)
  	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:486)
  	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:199)
  	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:232)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
  	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  Caused by: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
  Please ensure that the delta-storage dependency is included.
  
  If using Python, please ensure you call `configure_spark_with_delta_pip` or use
  `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
  See https://docs.delta.io/latest/quick-start.html#python.
  
  More information about this dependency and how to include it can be found here:
  https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
  
  	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar(DeltaErrors.scala:2498)
  	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar$(DeltaErrors.scala:2495)
  	at org.apache.spark.sql.delta.DeltaErrors$.missingDeltaStorageJar(DeltaErrors.scala:2681)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<init>(DelegatingLogStore.scala:163)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<clinit>(DelegatingLogStore.scala)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore.$anonfun$schemeBasedLogStore$1(DelegatingLogStore.scala:67)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore.schemeBasedLogStore(DelegatingLogStore.scala:67)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:85)
  	at org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)
  	at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:394)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
  	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
  	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
  	at org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
  	at org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:393)
  	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:387)
  	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:386)
  	at org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:263)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:261)
  	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
  	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)
  	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
  	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)
  	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:797)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:792)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
  	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:602)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
  	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
  	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
  	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:602)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
  	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
  	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:602)
  	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:791)
  	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:809)
  	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
  	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
  	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
  	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
  	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
  	... 72 more
  
[0m04:04:25.216637 [debug] [Thread-1 (]: Timing info for model.poc_demo.delta_lake (execute): 04:04:24.482471 => 04:04:25.216637
[0m04:04:25.216637 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: ROLLBACK
[0m04:04:25.216637 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m04:04:25.216637 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: Close
[0m04:04:25.243598 [debug] [Thread-1 (]: Runtime Error in model delta_lake (models\example\delta_lake.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
    Please ensure that the delta-storage dependency is included.
    
    If using Python, please ensure you call `configure_spark_with_delta_pip` or use
    `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
    See https://docs.delta.io/latest/quick-start.html#python.
    
    More information about this dependency and how to include it can be found here:
    https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:44)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
    Please ensure that the delta-storage dependency is included.
    
    If using Python, please ensure you call `configure_spark_with_delta_pip` or use
    `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
    See https://docs.delta.io/latest/quick-start.html#python.
    
    More information about this dependency and how to include it can be found here:
    https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
    
    	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
    	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
    	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
    	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:808)
    	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:818)
    	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:722)
    	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:660)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$8(DeltaCatalog.scala:150)
    	at scala.Option.map(Option.scala:230)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:148)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:86)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:472)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:434)
    	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:507)
    	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
    	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:491)
    	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:486)
    	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:199)
    	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:232)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    Caused by: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
    Please ensure that the delta-storage dependency is included.
    
    If using Python, please ensure you call `configure_spark_with_delta_pip` or use
    `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
    See https://docs.delta.io/latest/quick-start.html#python.
    
    More information about this dependency and how to include it can be found here:
    https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
    
    	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar(DeltaErrors.scala:2498)
    	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar$(DeltaErrors.scala:2495)
    	at org.apache.spark.sql.delta.DeltaErrors$.missingDeltaStorageJar(DeltaErrors.scala:2681)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<init>(DelegatingLogStore.scala:163)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<clinit>(DelegatingLogStore.scala)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore.$anonfun$schemeBasedLogStore$1(DelegatingLogStore.scala:67)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore.schemeBasedLogStore(DelegatingLogStore.scala:67)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:85)
    	at org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)
    	at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:394)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
    	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
    	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
    	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
    	at org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
    	at org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:393)
    	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:387)
    	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:386)
    	at org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:263)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
    	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:261)
    	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
    	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)
    	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
    	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)
    	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:797)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:792)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
    	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:602)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
    	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
    	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
    	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:602)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
    	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
    	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:602)
    	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:791)
    	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:809)
    	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
    	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
    	... 72 more
    
[0m04:04:25.243598 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543cedaf-c9ce-4f69-b6ae-8b233912e068', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C4446FE0>]}
[0m04:04:25.243598 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model ndb.delta_lake ..................... [[31mERROR[0m in 0.77s]
[0m04:04:25.250774 [debug] [Thread-1 (]: Finished running node model.poc_demo.delta_lake
[0m04:04:25.250774 [debug] [MainThread]: On master: ROLLBACK
[0m04:04:25.250774 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:04:25.324273 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:04:25.324273 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:04:25.324273 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:04:25.324273 [debug] [MainThread]: On master: ROLLBACK
[0m04:04:25.326790 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:04:25.326790 [debug] [MainThread]: On master: Close
[0m04:04:25.337069 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:04:25.337069 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m04:04:25.337069 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m04:04:25.340575 [debug] [MainThread]: Connection 'model.poc_demo.delta_lake' was properly closed.
[0m04:04:25.340575 [info ] [MainThread]: 
[0m04:04:25.340575 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.16 seconds (2.16s).
[0m04:04:25.340575 [debug] [MainThread]: Command end result
[0m04:04:25.350968 [info ] [MainThread]: 
[0m04:04:25.350968 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m04:04:25.350968 [info ] [MainThread]: 
[0m04:04:25.350968 [error] [MainThread]: [33mRuntime Error in model delta_lake (models\example\delta_lake.sql)[0m
[0m04:04:25.357555 [error] [MainThread]:   Database Error
[0m04:04:25.357555 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
[0m04:04:25.357555 [error] [MainThread]:     Please ensure that the delta-storage dependency is included.
[0m04:04:25.357555 [error] [MainThread]:     
[0m04:04:25.357555 [error] [MainThread]:     If using Python, please ensure you call `configure_spark_with_delta_pip` or use
[0m04:04:25.357555 [error] [MainThread]:     `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
[0m04:04:25.365212 [error] [MainThread]:     See https://docs.delta.io/latest/quick-start.html#python.
[0m04:04:25.365212 [error] [MainThread]:     
[0m04:04:25.365212 [error] [MainThread]:     More information about this dependency and how to include it can be found here:
[0m04:04:25.368563 [error] [MainThread]:     https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
[0m04:04:25.368563 [error] [MainThread]:     
[0m04:04:25.368563 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:44)
[0m04:04:25.368563 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m04:04:25.368563 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m04:04:25.374100 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m04:04:25.375526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m04:04:25.375526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m04:04:25.375526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m04:04:25.379467 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m04:04:25.381040 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m04:04:25.382848 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m04:04:25.385351 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m04:04:25.387261 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m04:04:25.388895 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m04:04:25.390101 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m04:04:25.391699 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m04:04:25.392756 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m04:04:25.393804 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m04:04:25.396320 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m04:04:25.396320 [error] [MainThread]:     Caused by: com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
[0m04:04:25.397835 [error] [MainThread]:     Please ensure that the delta-storage dependency is included.
[0m04:04:25.397835 [error] [MainThread]:     
[0m04:04:25.400181 [error] [MainThread]:     If using Python, please ensure you call `configure_spark_with_delta_pip` or use
[0m04:04:25.400181 [error] [MainThread]:     `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
[0m04:04:25.402197 [error] [MainThread]:     See https://docs.delta.io/latest/quick-start.html#python.
[0m04:04:25.402197 [error] [MainThread]:     
[0m04:04:25.406219 [error] [MainThread]:     More information about this dependency and how to include it can be found here:
[0m04:04:25.406219 [error] [MainThread]:     https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
[0m04:04:25.406219 [error] [MainThread]:     
[0m04:04:25.406219 [error] [MainThread]:     	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
[0m04:04:25.406219 [error] [MainThread]:     	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[0m04:04:25.406219 [error] [MainThread]:     	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
[0m04:04:25.406219 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:808)
[0m04:04:25.406219 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:818)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:722)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:660)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$8(DeltaCatalog.scala:150)
[0m04:04:25.414248 [error] [MainThread]:     	at scala.Option.map(Option.scala:230)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:148)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
[0m04:04:25.414248 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:86)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:472)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:434)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:507)
[0m04:04:25.422759 [error] [MainThread]:     	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:491)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:486)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:199)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:232)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[0m04:04:25.431229 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[0m04:04:25.439244 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[0m04:04:25.447254 [error] [MainThread]:     	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m04:04:25.455239 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m04:04:25.455239 [error] [MainThread]:     	... 16 more
[0m04:04:25.463249 [error] [MainThread]:     Caused by: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
[0m04:04:25.463249 [error] [MainThread]:     Please ensure that the delta-storage dependency is included.
[0m04:04:25.463249 [error] [MainThread]:     
[0m04:04:25.463249 [error] [MainThread]:     If using Python, please ensure you call `configure_spark_with_delta_pip` or use
[0m04:04:25.463249 [error] [MainThread]:     `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
[0m04:04:25.463249 [error] [MainThread]:     See https://docs.delta.io/latest/quick-start.html#python.
[0m04:04:25.463249 [error] [MainThread]:     
[0m04:04:25.463249 [error] [MainThread]:     More information about this dependency and how to include it can be found here:
[0m04:04:25.463249 [error] [MainThread]:     https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
[0m04:04:25.463249 [error] [MainThread]:     
[0m04:04:25.463249 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar(DeltaErrors.scala:2498)
[0m04:04:25.471265 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar$(DeltaErrors.scala:2495)
[0m04:04:25.471265 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaErrors$.missingDeltaStorageJar(DeltaErrors.scala:2681)
[0m04:04:25.471265 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<init>(DelegatingLogStore.scala:163)
[0m04:04:25.474276 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<clinit>(DelegatingLogStore.scala)
[0m04:04:25.474276 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore.$anonfun$schemeBasedLogStore$1(DelegatingLogStore.scala:67)
[0m04:04:25.474276 [error] [MainThread]:     	at scala.Option.orElse(Option.scala:447)
[0m04:04:25.474276 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore.schemeBasedLogStore(DelegatingLogStore.scala:67)
[0m04:04:25.474276 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:85)
[0m04:04:25.474276 [error] [MainThread]:     	at org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:394)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
[0m04:04:25.479285 [error] [MainThread]:     	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
[0m04:04:25.479285 [error] [MainThread]:     	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
[0m04:04:25.479285 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:393)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:387)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:386)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:263)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
[0m04:04:25.487298 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:261)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:260)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:797)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:792)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
[0m04:04:25.495294 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:602)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
[0m04:04:25.503306 [error] [MainThread]:     	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
[0m04:04:25.503306 [error] [MainThread]:     	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:602)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
[0m04:04:25.503306 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:602)
[0m04:04:25.511317 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:791)
[0m04:04:25.511317 [error] [MainThread]:     	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:809)
[0m04:04:25.511317 [error] [MainThread]:     	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
[0m04:04:25.511317 [error] [MainThread]:     	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[0m04:04:25.511317 [error] [MainThread]:     	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[0m04:04:25.511317 [error] [MainThread]:     	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[0m04:04:25.511317 [error] [MainThread]:     	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[0m04:04:25.511317 [error] [MainThread]:     	... 72 more
[0m04:04:25.511317 [error] [MainThread]:     
[0m04:04:25.511317 [info ] [MainThread]: 
[0m04:04:25.511317 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m04:04:25.511317 [debug] [MainThread]: Command `dbt run` failed at 04:04:25.511317 after 2.64 seconds
[0m04:04:25.519316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C180DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C4447B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C44478B0>]}
[0m04:04:25.519316 [debug] [MainThread]: Flushing usage events
[0m04:07:04.038859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001650DF2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001651063EB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001651063E8F0>]}


============================== 04:07:04.038859 | 68368a62-8c5c-4b1e-a181-fdb45813382c ==============================
[0m04:07:04.038859 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:07:04.054932 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:07:04.055989 [info ] [MainThread]: dbt version: 1.5.2
[0m04:07:04.055989 [info ] [MainThread]: python version: 3.10.11
[0m04:07:04.055989 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m04:07:04.055989 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m04:07:04.055989 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m04:07:04.055989 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m04:07:04.055989 [info ] [MainThread]: Configuration:
[0m04:07:04.149899 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:07:04.149899 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:07:04.165534 [info ] [MainThread]: Required dependencies:
[0m04:07:04.165534 [debug] [MainThread]: Executing "git --help"
[0m04:07:04.199157 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:07:04.199157 [debug] [MainThread]: STDERR: "b''"
[0m04:07:04.199157 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:07:04.199157 [info ] [MainThread]: Connection:
[0m04:07:04.199157 [info ] [MainThread]:   host: localhost
[0m04:07:04.199157 [info ] [MainThread]:   port: 10000
[0m04:07:04.199157 [info ] [MainThread]:   cluster: None
[0m04:07:04.199157 [info ] [MainThread]:   endpoint: None
[0m04:07:04.199157 [info ] [MainThread]:   schema: ndb
[0m04:07:04.199157 [info ] [MainThread]:   organization: 0
[0m04:07:04.199157 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:07:04.199157 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m04:07:04.199157 [debug] [MainThread]: Using spark connection "debug"
[0m04:07:04.199157 [debug] [MainThread]: On debug: select 1 as id
[0m04:07:04.199157 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:07:08.422653 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m04:07:08.422653 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m04:07:08.422653 [debug] [MainThread]: On debug: Close
[0m04:07:08.456658 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:07:08.456658 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:07:08.456658 [debug] [MainThread]: Command `dbt debug` succeeded at 04:07:08.456658 after 4.42 seconds
[0m04:07:08.462168 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:07:08.462816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001650DF2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001650D0A7940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001650D0A7220>]}
[0m04:07:08.462816 [debug] [MainThread]: Flushing usage events
[0m04:07:20.467560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235A8BEDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB302D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB302AD0>]}


============================== 04:07:20.469167 | ba5d7b64-e935-4990-9a2e-2f3c8785120f ==============================
[0m04:07:20.469167 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:07:20.469167 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:07:20.582156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB302D40>]}
[0m04:07:20.598308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB6ADED0>]}
[0m04:07:20.598308 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:07:20.607609 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m04:07:20.679408 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:07:20.679408 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:07:20.687422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB86C0D0>]}
[0m04:07:20.695378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB73FB50>]}
[0m04:07:20.695378 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 357 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m04:07:20.695378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB73FAF0>]}
[0m04:07:20.695378 [info ] [MainThread]: 
[0m04:07:20.695378 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:07:20.695378 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m04:07:20.722141 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m04:07:20.722141 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m04:07:20.722141 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:07:21.000104 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:07:21.000104 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:07:21.074123 [debug] [ThreadPool]: On list_schemas: Close
[0m04:07:21.090172 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m04:07:21.098727 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:07:21.098727 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m04:07:21.098727 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m04:07:21.098727 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:07:21.836226 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:07:21.836226 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m04:07:21.854825 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m04:07:21.854825 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:07:21.854825 [debug] [ThreadPool]: On list_None_ndb: Close
[0m04:07:21.886331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB8A33A0>]}
[0m04:07:21.886331 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:07:21.886331 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:07:21.891854 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:07:21.892874 [info ] [MainThread]: 
[0m04:07:21.896390 [debug] [Thread-1 (]: Began running node model.poc_demo.delta_lake
[0m04:07:21.896390 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.delta_lake .............................. [RUN]
[0m04:07:21.899965 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.delta_lake'
[0m04:07:21.899965 [debug] [Thread-1 (]: Began compiling node model.poc_demo.delta_lake
[0m04:07:21.899965 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.delta_lake"
[0m04:07:21.905979 [debug] [Thread-1 (]: Timing info for model.poc_demo.delta_lake (compile): 04:07:21.899965 => 04:07:21.905979
[0m04:07:21.908009 [debug] [Thread-1 (]: Began executing node model.poc_demo.delta_lake
[0m04:07:22.007857 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.delta_lake"
[0m04:07:22.009879 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m04:07:22.009879 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.delta_lake"
[0m04:07:22.012389 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.delta_lake"} */

  
    
        create or replace table ndb.delta_lake
      
      
    using delta
      
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m04:07:22.012389 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:07:27.080345 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m04:07:32.081225 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m04:07:32.276318 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m04:07:32.282000 [debug] [Thread-1 (]: SQL status: OK in 10.0 seconds
[0m04:07:32.301033 [debug] [Thread-1 (]: Timing info for model.poc_demo.delta_lake (execute): 04:07:21.909016 => 04:07:32.301033
[0m04:07:32.309156 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: ROLLBACK
[0m04:07:32.309156 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m04:07:32.309156 [debug] [Thread-1 (]: On model.poc_demo.delta_lake: Close
[0m04:07:32.317398 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba5d7b64-e935-4990-9a2e-2f3c8785120f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB851720>]}
[0m04:07:32.317398 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model ndb.delta_lake ......................... [[32mOK[0m in 10.42s]
[0m04:07:32.325417 [debug] [Thread-1 (]: Finished running node model.poc_demo.delta_lake
[0m04:07:32.325417 [debug] [MainThread]: On master: ROLLBACK
[0m04:07:32.327567 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:07:32.382995 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:07:32.382995 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:07:32.382995 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:07:32.382995 [debug] [MainThread]: On master: ROLLBACK
[0m04:07:32.382995 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:07:32.382995 [debug] [MainThread]: On master: Close
[0m04:07:32.401876 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:07:32.401876 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m04:07:32.401876 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m04:07:32.401876 [debug] [MainThread]: Connection 'model.poc_demo.delta_lake' was properly closed.
[0m04:07:32.401876 [info ] [MainThread]: 
[0m04:07:32.407507 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 11.71 seconds (11.71s).
[0m04:07:32.407507 [debug] [MainThread]: Command end result
[0m04:07:32.416324 [info ] [MainThread]: 
[0m04:07:32.416324 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:07:32.416324 [info ] [MainThread]: 
[0m04:07:32.416324 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m04:07:32.416324 [debug] [MainThread]: Command `dbt run` succeeded at 04:07:32.416324 after 11.97 seconds
[0m04:07:32.416324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235A8BEDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235A8D3CDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AB6ADED0>]}
[0m04:07:32.424447 [debug] [MainThread]: Flushing usage events
[0m04:12:55.449734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001442485DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014426F6EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014426F6E9E0>]}


============================== 04:12:55.449734 | 05dbdb1d-36ab-46d1-889f-1fc4162e079c ==============================
[0m04:12:55.449734 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:12:55.449734 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:12:55.449734 [info ] [MainThread]: dbt version: 1.5.2
[0m04:12:55.449734 [info ] [MainThread]: python version: 3.10.11
[0m04:12:55.449734 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m04:12:55.457167 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m04:12:55.458277 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m04:12:55.458277 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m04:12:55.460022 [info ] [MainThread]: Configuration:
[0m04:12:55.551669 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:12:55.569274 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:12:55.569274 [info ] [MainThread]: Required dependencies:
[0m04:12:55.569274 [debug] [MainThread]: Executing "git --help"
[0m04:12:55.599743 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:12:55.599743 [debug] [MainThread]: STDERR: "b''"
[0m04:12:55.599743 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:12:55.615425 [info ] [MainThread]: Connection:
[0m04:12:55.615425 [info ] [MainThread]:   host: localhost
[0m04:12:55.615425 [info ] [MainThread]:   port: 10000
[0m04:12:55.615425 [info ] [MainThread]:   cluster: None
[0m04:12:55.618454 [info ] [MainThread]:   endpoint: None
[0m04:12:55.618454 [info ] [MainThread]:   schema: ndb
[0m04:12:55.618454 [info ] [MainThread]:   organization: 0
[0m04:12:55.622351 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:12:55.623447 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m04:12:55.623447 [debug] [MainThread]: Using spark connection "debug"
[0m04:12:55.624558 [debug] [MainThread]: On debug: select 1 as id
[0m04:12:55.624762 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:12:59.542114 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m04:12:59.542114 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m04:12:59.542114 [debug] [MainThread]: On debug: Close
[0m04:12:59.566306 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:12:59.566306 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:12:59.566306 [debug] [MainThread]: Command `dbt debug` succeeded at 04:12:59.566306 after 4.14 seconds
[0m04:12:59.575814 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:12:59.575814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001442485DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144239E39D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144239E09A0>]}
[0m04:12:59.575814 [debug] [MainThread]: Flushing usage events
[0m04:18:46.185712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18256DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184C32C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184C32A10>]}


============================== 04:18:46.187716 | 7f9019ef-5bcc-48e1-889e-bcc4898beacb ==============================
[0m04:18:46.187716 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:18:46.187716 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:18:46.192510 [info ] [MainThread]: dbt version: 1.5.2
[0m04:18:46.193328 [info ] [MainThread]: python version: 3.10.11
[0m04:18:46.194311 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m04:18:46.194311 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m04:18:46.194311 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m04:18:46.194311 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m04:18:46.197989 [info ] [MainThread]: Configuration:
[0m04:18:46.303105 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:18:46.315934 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:18:46.315934 [info ] [MainThread]: Required dependencies:
[0m04:18:46.323587 [debug] [MainThread]: Executing "git --help"
[0m04:18:46.356563 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:18:46.356563 [debug] [MainThread]: STDERR: "b''"
[0m04:18:46.356563 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:18:46.364674 [info ] [MainThread]: Connection:
[0m04:18:46.364674 [info ] [MainThread]:   host: localhost
[0m04:18:46.364674 [info ] [MainThread]:   port: 10000
[0m04:18:46.364674 [info ] [MainThread]:   cluster: None
[0m04:18:46.364674 [info ] [MainThread]:   endpoint: None
[0m04:18:46.372580 [info ] [MainThread]:   schema: ndb
[0m04:18:46.372580 [info ] [MainThread]:   organization: 0
[0m04:18:46.372580 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:18:46.372580 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m04:18:46.372580 [debug] [MainThread]: Using spark connection "debug"
[0m04:18:46.372580 [debug] [MainThread]: On debug: select 1 as id
[0m04:18:46.372580 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:18:46.582832 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m04:18:46.582832 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m04:18:46.582832 [debug] [MainThread]: On debug: Close
[0m04:18:46.601999 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:18:46.601999 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:18:46.607048 [debug] [MainThread]: Command `dbt debug` succeeded at 04:18:46.607048 after 0.44 seconds
[0m04:18:46.607048 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:18:46.607048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18256DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A181707A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A181704790>]}
[0m04:18:46.607048 [debug] [MainThread]: Flushing usage events
[0m04:19:22.175079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1928DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1B99EE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1B99EB90>]}


============================== 04:19:22.175079 | d2228d4c-c0ac-41ab-afda-c3cd66083a08 ==============================
[0m04:19:22.175079 [info ] [MainThread]: Running with dbt=1.5.2
[0m04:19:22.175079 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m04:19:22.278898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1B99EE00>]}
[0m04:19:22.285003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BD4DED0>]}
[0m04:19:22.285003 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m04:19:22.295015 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m04:19:22.343044 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m04:19:22.351032 [debug] [MainThread]: Partial parsing: added file: poc_demo://macros\generate_schema_name.sql
[0m04:19:22.351032 [debug] [MainThread]: Partial parsing: added file: poc_demo://models\example\iceberg_partition.sql
[0m04:19:22.351032 [info ] [MainThread]: Unable to do partial parsing because change detected to override macro. Starting full parse.
[0m04:19:22.959553 [debug] [MainThread]: 1699: static parser successfully parsed example\delta_lake.sql
[0m04:19:22.975085 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m04:19:22.975085 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m04:19:22.975085 [debug] [MainThread]: 1699: static parser successfully parsed example\iceberg_partition.sql
[0m04:19:22.994037 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m04:19:22.996059 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m04:19:23.085139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BF1CFD0>]}
[0m04:19:23.102307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BECA2F0>]}
[0m04:19:23.102307 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 358 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m04:19:23.102307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F194357E0>]}
[0m04:19:23.102307 [info ] [MainThread]: 
[0m04:19:23.102307 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:19:23.102307 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m04:19:23.117602 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m04:19:23.117602 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m04:19:23.117602 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:19:23.420308 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:19:23.420308 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:19:23.497138 [debug] [ThreadPool]: On list_schemas: Close
[0m04:19:23.515039 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__spark_catalog.ndb)
[0m04:19:23.523560 [debug] [ThreadPool]: Creating schema "schema: "spark_catalog.ndb"
"
[0m04:19:23.529116 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:23.530125 [debug] [ThreadPool]: Using spark connection "create__spark_catalog.ndb"
[0m04:19:23.530125 [debug] [ThreadPool]: On create__spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "create__spark_catalog.ndb"} */
create schema if not exists spark_catalog.ndb
  
[0m04:19:23.530125 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:19:23.709186 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:19:23.709186 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:19:23.711208 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m04:19:23.711208 [debug] [ThreadPool]: On create__spark_catalog.ndb: ROLLBACK
[0m04:19:23.711208 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:19:23.711208 [debug] [ThreadPool]: On create__spark_catalog.ndb: Close
[0m04:19:23.725874 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m04:19:23.736461 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:23.736461 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m04:19:23.736461 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m04:19:23.736461 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:19:24.477908 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:19:24.477908 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m04:19:24.497342 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m04:19:24.497342 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:19:24.497342 [debug] [ThreadPool]: On list_None_ndb: Close
[0m04:19:24.514427 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m04:19:24.516451 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:24.516451 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m04:19:24.516451 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m04:19:24.516451 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:19:24.897817 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:19:24.897817 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m04:19:24.908358 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m04:19:24.909890 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:19:24.909890 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m04:19:24.923547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BFB6680>]}
[0m04:19:24.923547 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:24.923547 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:19:24.928056 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:19:24.928056 [info ] [MainThread]: 
[0m04:19:24.936300 [debug] [Thread-1 (]: Began running node model.poc_demo.iceberg_partition
[0m04:19:24.936300 [info ] [Thread-1 (]: 1 of 1 START sql incremental model spark_catalog.ndb.iceberg_partition ......... [RUN]
[0m04:19:24.938923 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.iceberg_partition'
[0m04:19:24.938923 [debug] [Thread-1 (]: Began compiling node model.poc_demo.iceberg_partition
[0m04:19:24.945678 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.iceberg_partition"
[0m04:19:24.949465 [debug] [Thread-1 (]: Timing info for model.poc_demo.iceberg_partition (compile): 04:19:24.938923 => 04:19:24.947435
[0m04:19:24.951456 [debug] [Thread-1 (]: Began executing node model.poc_demo.iceberg_partition
[0m04:19:25.044827 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.iceberg_partition"
[0m04:19:25.044827 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:25.044827 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.iceberg_partition"
[0m04:19:25.044827 [debug] [Thread-1 (]: On model.poc_demo.iceberg_partition: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.iceberg_partition"} */

  
    
        create or replace table spark_catalog.ndb.iceberg_partition
      
      
    using iceberg
      
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
order by category
  
[0m04:19:25.044827 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:19:28.960312 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m04:19:28.960312 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m04:19:28.972377 [debug] [Thread-1 (]: Timing info for model.poc_demo.iceberg_partition (execute): 04:19:24.952251 => 04:19:28.972377
[0m04:19:28.972377 [debug] [Thread-1 (]: On model.poc_demo.iceberg_partition: ROLLBACK
[0m04:19:28.988017 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m04:19:28.988535 [debug] [Thread-1 (]: On model.poc_demo.iceberg_partition: Close
[0m04:19:28.988535 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd2228d4c-c0ac-41ab-afda-c3cd66083a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BEAD660>]}
[0m04:19:28.988535 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model spark_catalog.ndb.iceberg_partition .... [[32mOK[0m in 4.05s]
[0m04:19:29.004087 [debug] [Thread-1 (]: Finished running node model.poc_demo.iceberg_partition
[0m04:19:29.004087 [debug] [MainThread]: On master: ROLLBACK
[0m04:19:29.004087 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:19:29.076943 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:19:29.077968 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:19:29.077968 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:19:29.077968 [debug] [MainThread]: On master: ROLLBACK
[0m04:19:29.079484 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m04:19:29.079484 [debug] [MainThread]: On master: Close
[0m04:19:29.086858 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:19:29.086858 [debug] [MainThread]: Connection 'create__spark_catalog.ndb' was properly closed.
[0m04:19:29.086858 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m04:19:29.086858 [debug] [MainThread]: Connection 'model.poc_demo.iceberg_partition' was properly closed.
[0m04:19:29.086858 [info ] [MainThread]: 
[0m04:19:29.086858 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 5.98 seconds (5.98s).
[0m04:19:29.094373 [debug] [MainThread]: Command end result
[0m04:19:29.104174 [info ] [MainThread]: 
[0m04:19:29.104174 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:19:29.104174 [info ] [MainThread]: 
[0m04:19:29.104174 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m04:19:29.109345 [debug] [MainThread]: Command `dbt run` succeeded at 04:19:29.109345 after 6.95 seconds
[0m04:19:29.109345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1928DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BF7CC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F1BF7EC20>]}
[0m04:19:29.111859 [debug] [MainThread]: Flushing usage events
[0m17:48:47.852211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C494AEDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C4971FECB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C4971FEA70>]}


============================== 17:48:47.867244 | ac515c7f-c51c-4ef2-bf3a-7a8de7cf443f ==============================
[0m17:48:47.867244 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:48:47.867244 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:48:47.874966 [info ] [MainThread]: dbt version: 1.5.2
[0m17:48:47.874966 [info ] [MainThread]: python version: 3.10.11
[0m17:48:47.874966 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m17:48:47.874966 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m17:48:47.874966 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m17:48:47.883365 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m17:48:47.884109 [info ] [MainThread]: Configuration:
[0m17:48:48.301426 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m17:48:48.358421 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m17:48:48.358421 [info ] [MainThread]: Required dependencies:
[0m17:48:48.358421 [debug] [MainThread]: Executing "git --help"
[0m17:48:48.425036 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:48:48.425036 [debug] [MainThread]: STDERR: "b''"
[0m17:48:48.437086 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m17:48:48.437086 [info ] [MainThread]: Connection:
[0m17:48:48.437086 [info ] [MainThread]:   host: localhost
[0m17:48:48.437086 [info ] [MainThread]:   port: 10000
[0m17:48:48.437086 [info ] [MainThread]:   cluster: None
[0m17:48:48.437086 [info ] [MainThread]:   endpoint: None
[0m17:48:48.437086 [info ] [MainThread]:   schema: ndb
[0m17:48:48.437086 [info ] [MainThread]:   organization: 0
[0m17:48:48.437086 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m17:48:48.437086 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m17:48:48.437086 [debug] [MainThread]: Using spark connection "debug"
[0m17:48:48.437086 [debug] [MainThread]: On debug: select 1 as id
[0m17:48:48.453113 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:48:54.772977 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m17:48:54.778007 [debug] [MainThread]: SQL status: OK in 6.0 seconds
[0m17:48:54.780259 [debug] [MainThread]: On debug: Close
[0m17:48:54.829920 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m17:48:54.831831 [info ] [MainThread]: [32mAll checks passed![0m
[0m17:48:54.835312 [debug] [MainThread]: Command `dbt debug` succeeded at 17:48:54.831831 after 7.05 seconds
[0m17:48:54.835312 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m17:48:54.836835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C4971FE860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C493C77310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C4975A2950>]}
[0m17:48:54.836835 [debug] [MainThread]: Flushing usage events
[0m22:19:16.115903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001779740DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017799B1EC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017799B1E9E0>]}


============================== 22:19:16.126380 | 73498742-3b23-46f6-bfa2-e1131ffab8fc ==============================
[0m22:19:16.126380 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:19:16.126380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:19:16.126380 [info ] [MainThread]: dbt version: 1.5.2
[0m22:19:16.131914 [info ] [MainThread]: python version: 3.10.11
[0m22:19:16.131914 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:19:16.131914 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:19:16.136934 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:19:16.136934 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:19:16.139526 [info ] [MainThread]: Configuration:
[0m22:19:16.480943 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:19:16.526864 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:19:16.528370 [info ] [MainThread]: Required dependencies:
[0m22:19:16.530708 [debug] [MainThread]: Executing "git --help"
[0m22:19:16.585630 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:19:16.590945 [debug] [MainThread]: STDERR: "b''"
[0m22:19:16.590945 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:19:16.590945 [info ] [MainThread]: Connection:
[0m22:19:16.590945 [info ] [MainThread]:   host: localhost
[0m22:19:16.590945 [info ] [MainThread]:   port: 10000
[0m22:19:16.595451 [info ] [MainThread]:   cluster: None
[0m22:19:16.596753 [info ] [MainThread]:   endpoint: None
[0m22:19:16.596753 [info ] [MainThread]:   schema: ndb
[0m22:19:16.596753 [info ] [MainThread]:   organization: 0
[0m22:19:16.596753 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:19:16.596753 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:19:16.596753 [debug] [MainThread]: Using spark connection "debug"
[0m22:19:16.596753 [debug] [MainThread]: On debug: select 1 as id
[0m22:19:16.596753 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:16.626669 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m22:19:16.627965 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m22:19:16.627965 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m22:19:16.630894 [info ] [MainThread]: [31m1 check failed:[0m
[0m22:19:16.631972 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m22:19:16.634122 [debug] [MainThread]: Command `dbt debug` failed at 22:19:16.633588 after 0.58 seconds
[0m22:19:16.634654 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:19:16.635732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001779740DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017799EC0C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017799EC2E90>]}
[0m22:19:16.636263 [debug] [MainThread]: Flushing usage events
[0m22:20:53.176196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2677DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F28E8EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F28E8E9E0>]}


============================== 22:20:53.176196 | df942ba6-abdd-4ee4-9c11-920d00279fa1 ==============================
[0m22:20:53.176196 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:20:53.176196 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:20:53.176196 [info ] [MainThread]: dbt version: 1.5.2
[0m22:20:53.176196 [info ] [MainThread]: python version: 3.10.11
[0m22:20:53.176196 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:20:53.176196 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:20:53.176196 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:20:53.176196 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:20:53.191830 [info ] [MainThread]: Configuration:
[0m22:20:53.302357 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:20:53.334019 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:20:53.334528 [info ] [MainThread]: Required dependencies:
[0m22:20:53.334528 [debug] [MainThread]: Executing "git --help"
[0m22:20:53.365425 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:20:53.365425 [debug] [MainThread]: STDERR: "b''"
[0m22:20:53.365425 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:20:53.365425 [info ] [MainThread]: Connection:
[0m22:20:53.365425 [info ] [MainThread]:   host: localhost
[0m22:20:53.365425 [info ] [MainThread]:   port: 10000
[0m22:20:53.365425 [info ] [MainThread]:   cluster: None
[0m22:20:53.381090 [info ] [MainThread]:   endpoint: None
[0m22:20:53.381090 [info ] [MainThread]:   schema: ndb
[0m22:20:53.381090 [info ] [MainThread]:   organization: 0
[0m22:20:53.381090 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:20:53.381090 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:20:53.381090 [debug] [MainThread]: Using spark connection "debug"
[0m22:20:53.381090 [debug] [MainThread]: On debug: select 1 as id
[0m22:20:53.381090 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:55.273089 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:20:55.273089 [debug] [MainThread]: SQL status: OK in 2.0 seconds
[0m22:20:55.277637 [debug] [MainThread]: On debug: Close
[0m22:20:55.332263 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:20:55.336135 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:20:55.338272 [debug] [MainThread]: Command `dbt debug` succeeded at 22:20:55.338272 after 2.17 seconds
[0m22:20:55.338272 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:20:55.338272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2677DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F259079D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F25907A00>]}
[0m22:20:55.340421 [debug] [MainThread]: Flushing usage events
[0m22:21:08.453816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A2F6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A567EC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A567E980>]}


============================== 22:21:08.453816 | 21f3a3b1-0289-4353-b759-65b88f9bed13 ==============================
[0m22:21:08.453816 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:21:08.453816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:21:08.465573 [info ] [MainThread]: dbt version: 1.5.2
[0m22:21:08.465573 [info ] [MainThread]: python version: 3.10.11
[0m22:21:08.465573 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m22:21:08.465573 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m22:21:08.465573 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m22:21:08.465573 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m22:21:08.470426 [info ] [MainThread]: Configuration:
[0m22:21:08.565335 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m22:21:08.596567 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m22:21:08.596567 [info ] [MainThread]: Required dependencies:
[0m22:21:08.596567 [debug] [MainThread]: Executing "git --help"
[0m22:21:08.644435 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:21:08.644435 [debug] [MainThread]: STDERR: "b''"
[0m22:21:08.644435 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m22:21:08.644435 [info ] [MainThread]: Connection:
[0m22:21:08.644435 [info ] [MainThread]:   host: localhost
[0m22:21:08.644435 [info ] [MainThread]:   port: 10000
[0m22:21:08.644435 [info ] [MainThread]:   cluster: None
[0m22:21:08.644435 [info ] [MainThread]:   endpoint: None
[0m22:21:08.644435 [info ] [MainThread]:   schema: ndb
[0m22:21:08.644435 [info ] [MainThread]:   organization: 0
[0m22:21:08.644435 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m22:21:08.644435 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m22:21:08.644435 [debug] [MainThread]: Using spark connection "debug"
[0m22:21:08.644435 [debug] [MainThread]: On debug: select 1 as id
[0m22:21:08.644435 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:21:08.848681 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:21:08.849736 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m22:21:08.849736 [debug] [MainThread]: On debug: Close
[0m22:21:08.865554 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m22:21:08.865554 [info ] [MainThread]: [32mAll checks passed![0m
[0m22:21:08.869473 [debug] [MainThread]: Command `dbt debug` succeeded at 22:21:08.869473 after 0.43 seconds
[0m22:21:08.869473 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m22:21:08.869473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A2F6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A20F79D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261A20F7A00>]}
[0m22:21:08.873401 [debug] [MainThread]: Flushing usage events
[0m14:50:19.232306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76888DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76AFA2CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76AFA2A10>]}


============================== 14:50:19.244604 | 10faa2e4-1c87-4c44-9f34-3acbb6dc8c1b ==============================
[0m14:50:19.244604 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:50:19.244604 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:50:19.244604 [info ] [MainThread]: dbt version: 1.5.2
[0m14:50:19.252925 [info ] [MainThread]: python version: 3.10.11
[0m14:50:19.252925 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m14:50:19.252925 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:50:19.252925 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:50:19.252925 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m14:50:19.258186 [info ] [MainThread]: Configuration:
[0m14:50:19.510170 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:50:19.530569 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:50:19.530569 [info ] [MainThread]: Required dependencies:
[0m14:50:19.530569 [debug] [MainThread]: Executing "git --help"
[0m14:50:19.584749 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:50:19.584749 [debug] [MainThread]: STDERR: "b''"
[0m14:50:19.584749 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:50:19.592466 [info ] [MainThread]: Connection:
[0m14:50:19.593472 [info ] [MainThread]:   host: localhost
[0m14:50:19.594471 [info ] [MainThread]:   port: 10000
[0m14:50:19.595479 [info ] [MainThread]:   cluster: None
[0m14:50:19.596451 [info ] [MainThread]:   endpoint: None
[0m14:50:19.596451 [info ] [MainThread]:   schema: ndb
[0m14:50:19.596451 [info ] [MainThread]:   organization: 0
[0m14:50:19.596451 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m14:50:19.603199 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m14:50:19.603199 [debug] [MainThread]: Using spark connection "debug"
[0m14:50:19.603199 [debug] [MainThread]: On debug: select 1 as id
[0m14:50:19.603199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:50:20.818003 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m14:50:20.818003 [debug] [MainThread]: SQL status: OK in 1.0 seconds
[0m14:50:20.821521 [debug] [MainThread]: On debug: Close
[0m14:50:20.850800 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:50:20.850800 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:50:20.856863 [debug] [MainThread]: Command `dbt debug` succeeded at 14:50:20.856863 after 1.64 seconds
[0m14:50:20.856863 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:50:20.856863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76AFA29B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C767A17310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76B350250>]}
[0m14:50:20.856863 [debug] [MainThread]: Flushing usage events
[0m15:15:35.550053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001489485DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014896F72BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014896F72A40>]}


============================== 15:15:35.565724 | 0aaf523b-37aa-4b95-89df-7e7262c825c1 ==============================
[0m15:15:35.565724 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:15:35.565724 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:15:35.565724 [info ] [MainThread]: dbt version: 1.5.2
[0m15:15:35.565724 [info ] [MainThread]: python version: 3.10.11
[0m15:15:35.565724 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m15:15:35.565724 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m15:15:35.574606 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m15:15:35.575658 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m15:15:35.577613 [info ] [MainThread]: Configuration:
[0m15:15:35.719468 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:15:35.735148 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:15:35.750154 [info ] [MainThread]: Required dependencies:
[0m15:15:35.751360 [debug] [MainThread]: Executing "git --help"
[0m15:15:35.794010 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:15:35.794010 [debug] [MainThread]: STDERR: "b''"
[0m15:15:35.795471 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:15:35.795471 [info ] [MainThread]: Connection:
[0m15:15:35.795471 [info ] [MainThread]:   host: localhost
[0m15:15:35.795471 [info ] [MainThread]:   port: 10000
[0m15:15:35.795471 [info ] [MainThread]:   cluster: None
[0m15:15:35.795471 [info ] [MainThread]:   endpoint: None
[0m15:15:35.795471 [info ] [MainThread]:   schema: ndb
[0m15:15:35.803859 [info ] [MainThread]:   organization: 0
[0m15:15:35.806945 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:15:35.808894 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m15:15:35.809889 [debug] [MainThread]: Using spark connection "debug"
[0m15:15:35.810888 [debug] [MainThread]: On debug: select 1 as id
[0m15:15:35.810888 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:15:39.385874 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:15:39.385874 [debug] [MainThread]: SQL status: OK in 4.0 seconds
[0m15:15:39.385874 [debug] [MainThread]: On debug: Close
[0m15:15:39.416829 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:15:39.416829 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:15:39.416829 [debug] [MainThread]: Command `dbt debug` succeeded at 15:15:39.416829 after 3.89 seconds
[0m15:15:39.416829 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:15:39.416829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001489485DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000148939E38E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000148939E1900>]}
[0m15:15:39.416829 [debug] [MainThread]: Flushing usage events
[0m15:19:53.150284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16C8DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A193A2DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A193A2B90>]}


============================== 15:19:53.150284 | 4930ad40-2449-433a-986b-1cdd3b08c8b6 ==============================
[0m15:19:53.150284 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:19:53.150284 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:19:53.274172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A193A2E00>]}
[0m15:19:53.289852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A1974ADA0>]}
[0m15:19:53.289852 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:19:53.356531 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:19:53.389268 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:19:53.390295 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m15:19:53.392320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19748820>]}
[0m15:19:56.631556 [debug] [MainThread]: 1699: static parser successfully parsed example\delta_lake.sql
[0m15:19:56.647183 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m15:19:56.647183 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m15:19:56.647183 [debug] [MainThread]: 1699: static parser successfully parsed example\iceberg_partition.sql
[0m15:19:56.647183 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m15:19:56.662801 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m15:19:56.662801 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\as_of_date.sql
[0m15:19:56.662801 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\h_cli.sql
[0m15:19:56.756406 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\h_cli.sql
[0m15:19:56.756406 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m15:19:56.814792 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m15:19:56.814792 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_h_cli.sql
[0m15:19:56.814792 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_s_address.sql
[0m15:19:56.819991 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_s_name.sql
[0m15:19:56.819991 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\s_address.sql
[0m15:19:56.866313 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\s_address.sql
[0m15:19:56.866313 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\s_name.sql
[0m15:19:56.881933 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\s_name.sql
[0m15:19:56.881933 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_h_cli.sql
[0m15:19:56.960721 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_h_cli.sql
[0m15:19:56.960721 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_s_address.sql
[0m15:19:56.976349 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_s_address.sql
[0m15:19:56.976349 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_s_name.sql
[0m15:19:56.991972 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_s_name.sql
[0m15:19:57.133796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19ABC4F0>]}
[0m15:19:57.149320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19C063E0>]}
[0m15:19:57.149320 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:19:57.149320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19ABEF20>]}
[0m15:19:57.165020 [info ] [MainThread]: 
[0m15:19:57.165020 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:19:57.165020 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:19:57.165020 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:19:57.165020 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:19:57.165020 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:57.316688 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:19:57.316688 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:19:57.323385 [debug] [ThreadPool]: On list_schemas: Close
[0m15:19:57.340222 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m15:19:57.340222 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:19:57.340222 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:19:57.340222 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:19:57.340222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:58.043742 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:19:58.043742 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m15:19:58.051749 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:19:58.051749 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:19:58.051749 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:19:58.060760 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m15:19:58.067861 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:19:58.067861 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:19:58.067861 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:19:58.067861 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:58.385452 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:19:58.391468 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:19:58.397499 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:19:58.397499 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:19:58.397499 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:19:58.401506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A199AF250>]}
[0m15:19:58.401506 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:19:58.401506 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:19:58.401506 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:19:58.401506 [info ] [MainThread]: 
[0m15:19:58.416833 [debug] [Thread-1 (]: Began running node model.poc_demo.raw_h_cli
[0m15:19:58.416833 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.raw_h_cli ...................................... [RUN]
[0m15:19:58.427050 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.raw_h_cli'
[0m15:19:58.427050 [debug] [Thread-1 (]: Began compiling node model.poc_demo.raw_h_cli
[0m15:19:58.431062 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.raw_h_cli"
[0m15:19:58.433290 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_h_cli (compile): 15:19:58.428060 => 15:19:58.433290
[0m15:19:58.433290 [debug] [Thread-1 (]: Began executing node model.poc_demo.raw_h_cli
[0m15:19:58.461857 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.raw_h_cli"
[0m15:19:58.461857 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:19:58.461857 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.raw_h_cli"
[0m15:19:58.461857 [debug] [Thread-1 (]: On model.poc_demo.raw_h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.raw_h_cli"} */
create or replace view ndb.raw_h_cli
  
  as
    with cli AS (
SELECT distinct trim(cli.cli_id) as cli_id,
'dummy' AS rcrd_src_nm
FROM ndb.cli_hub cli
WHERE 1=1
)

SELECT
cli.cli_id,
cli.rcrd_src_nm
FROM cli

[0m15:19:58.470951 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:19:59.049428 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:19:59.052559 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m15:19:59.052559 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_h_cli (execute): 15:19:58.433290 => 15:19:59.052559
[0m15:19:59.063734 [debug] [Thread-1 (]: On model.poc_demo.raw_h_cli: ROLLBACK
[0m15:19:59.063734 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:19:59.063734 [debug] [Thread-1 (]: On model.poc_demo.raw_h_cli: Close
[0m15:19:59.079748 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4930ad40-2449-433a-986b-1cdd3b08c8b6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19B03970>]}
[0m15:19:59.080728 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.raw_h_cli ................................. [[32mOK[0m in 0.66s]
[0m15:19:59.080728 [debug] [Thread-1 (]: Finished running node model.poc_demo.raw_h_cli
[0m15:19:59.080728 [debug] [MainThread]: On master: ROLLBACK
[0m15:19:59.084239 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:19:59.132894 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:19:59.132894 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:19:59.139310 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:19:59.139310 [debug] [MainThread]: On master: ROLLBACK
[0m15:19:59.140317 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:19:59.140317 [debug] [MainThread]: On master: Close
[0m15:19:59.149768 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:19:59.149768 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:19:59.149768 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m15:19:59.149768 [debug] [MainThread]: Connection 'model.poc_demo.raw_h_cli' was properly closed.
[0m15:19:59.149768 [info ] [MainThread]: 
[0m15:19:59.156142 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.98 seconds (1.98s).
[0m15:19:59.157149 [debug] [MainThread]: Command end result
[0m15:19:59.177782 [info ] [MainThread]: 
[0m15:19:59.178782 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:19:59.179781 [info ] [MainThread]: 
[0m15:19:59.181784 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:19:59.182708 [debug] [MainThread]: Command `dbt run` succeeded at 15:19:59.182708 after 6.05 seconds
[0m15:19:59.182708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16C8DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A18FF1060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A19C063E0>]}
[0m15:19:59.186217 [debug] [MainThread]: Flushing usage events
[0m15:20:35.236002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002558E6CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025590DE2CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025590DE2B30>]}


============================== 15:20:35.236002 | 2601fd2e-d37c-4169-b1a1-7778946f984f ==============================
[0m15:20:35.236002 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:20:35.236002 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:20:35.374644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025590DE2D40>]}
[0m15:20:35.374644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025591192F50>]}
[0m15:20:35.374644 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:20:35.410313 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:20:35.525508 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:20:35.525508 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:20:35.525508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255914C00D0>]}
[0m15:20:35.541874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002559147DF30>]}
[0m15:20:35.541874 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:20:35.541874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002559147E0B0>]}
[0m15:20:35.556943 [info ] [MainThread]: 
[0m15:20:35.556943 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:20:35.556943 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:20:35.556943 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:20:35.556943 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:20:35.556943 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:35.709728 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:35.709728 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:35.716248 [debug] [ThreadPool]: On list_schemas: Close
[0m15:20:35.731606 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m15:20:35.741305 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:35.741305 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:20:35.741305 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:20:35.741305 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:36.017616 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:36.017616 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:36.026171 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:20:36.026171 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:20:36.028421 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:20:36.041977 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m15:20:36.044599 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:36.044599 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:20:36.044599 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:20:36.044599 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:20:36.305713 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:36.305713 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:36.309228 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:20:36.309228 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:20:36.309228 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:20:36.319830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255914E3430>]}
[0m15:20:36.319830 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:36.319830 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:20:36.319830 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:20:36.319830 [info ] [MainThread]: 
[0m15:20:36.336160 [debug] [Thread-1 (]: Began running node model.poc_demo.v_stg_h_cli
[0m15:20:36.336907 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.v_stg_h_cli .................................... [RUN]
[0m15:20:36.338939 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.v_stg_h_cli'
[0m15:20:36.338939 [debug] [Thread-1 (]: Began compiling node model.poc_demo.v_stg_h_cli
[0m15:20:36.387085 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:36.387085 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_h_cli"
[0m15:20:36.387085 [debug] [Thread-1 (]: On model.poc_demo.v_stg_h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_h_cli"} */

      describe extended ndb.raw_h_cli
  
[0m15:20:36.387085 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:20:36.527724 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:20:36.527724 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:20:36.573674 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_h_cli"
[0m15:20:36.573674 [debug] [Thread-1 (]: On model.poc_demo.v_stg_h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_h_cli"} */

      describe extended ndb.raw_h_cli
  
[0m15:20:36.645000 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:20:36.645000 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:20:36.683439 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.v_stg_h_cli"
[0m15:20:36.683439 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_h_cli (compile): 15:20:36.340037 => 15:20:36.683439
[0m15:20:36.683439 [debug] [Thread-1 (]: Began executing node model.poc_demo.v_stg_h_cli
[0m15:20:36.717051 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.v_stg_h_cli"
[0m15:20:36.717974 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_h_cli"
[0m15:20:36.718976 [debug] [Thread-1 (]: On model.poc_demo.v_stg_h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_h_cli"} */
create or replace view ndb.v_stg_h_cli
  
  as
    







-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`

    FROM ndb.raw_h_cli
),

derived_columns AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`,
    `CURRENT_TIMESTAMP` AS `ld_dt_tm`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `LD_DT_TM`,

    CAST(MD5(NULLIF(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`cli_id` AS STRING))), ''), '^^')
    ), '^^')) AS VARCHAR(16)) AS `hk_cli_cd`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `LD_DT_TM`,
    `HK_CLI_CD`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

[0m15:20:36.953843 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:20:36.953843 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:20:36.970560 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_h_cli (execute): 15:20:36.690983 => 15:20:36.970560
[0m15:20:36.970560 [debug] [Thread-1 (]: On model.poc_demo.v_stg_h_cli: ROLLBACK
[0m15:20:36.970560 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:20:36.970560 [debug] [Thread-1 (]: On model.poc_demo.v_stg_h_cli: Close
[0m15:20:36.981523 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2601fd2e-d37c-4169-b1a1-7778946f984f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025591545AB0>]}
[0m15:20:36.981523 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.v_stg_h_cli ............................... [[32mOK[0m in 0.64s]
[0m15:20:36.981523 [debug] [Thread-1 (]: Finished running node model.poc_demo.v_stg_h_cli
[0m15:20:36.981523 [debug] [MainThread]: On master: ROLLBACK
[0m15:20:36.981523 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:20:37.042091 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:20:37.043100 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:37.043100 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:20:37.043100 [debug] [MainThread]: On master: ROLLBACK
[0m15:20:37.043100 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:20:37.044609 [debug] [MainThread]: On master: Close
[0m15:20:37.051617 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:20:37.051617 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:20:37.051617 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m15:20:37.051617 [debug] [MainThread]: Connection 'model.poc_demo.v_stg_h_cli' was properly closed.
[0m15:20:37.051617 [info ] [MainThread]: 
[0m15:20:37.051617 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.49 seconds (1.49s).
[0m15:20:37.051617 [debug] [MainThread]: Command end result
[0m15:20:37.071247 [info ] [MainThread]: 
[0m15:20:37.072248 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:20:37.073246 [info ] [MainThread]: 
[0m15:20:37.074244 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:20:37.076467 [debug] [MainThread]: Command `dbt run` succeeded at 15:20:37.075441 after 1.85 seconds
[0m15:20:37.077473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002558E6CDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025591192F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002559158CE80>]}
[0m15:20:37.077473 [debug] [MainThread]: Flushing usage events
[0m15:20:45.587108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B2CBDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B53D2D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B53D2BC0>]}


============================== 15:20:45.587108 | 31e66196-ada3-474b-97cf-1b168ad7044c ==============================
[0m15:20:45.587108 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:20:45.587108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:20:45.711006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B53D2DD0>]}
[0m15:20:45.733619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5782F20>]}
[0m15:20:45.734619 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:20:45.760924 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:20:45.860501 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:20:45.860501 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:20:45.860501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5AAC0D0>]}
[0m15:20:45.876155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5A6DFC0>]}
[0m15:20:45.876155 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:20:45.891783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5A6E140>]}
[0m15:20:45.891783 [info ] [MainThread]: 
[0m15:20:45.891783 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:20:45.891783 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:20:45.904881 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:20:45.904881 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:20:45.907959 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:46.018499 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:46.019483 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:46.028248 [debug] [ThreadPool]: On list_schemas: Close
[0m15:20:46.042652 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m15:20:46.048818 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:46.048818 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:20:46.048818 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:20:46.048818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:46.309794 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:46.309794 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:46.318308 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:20:46.318308 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:20:46.318308 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:20:46.330026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m15:20:46.333049 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:46.333049 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:20:46.333049 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:20:46.333049 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:20:46.535512 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:20:46.535512 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:20:46.547047 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:20:46.547047 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:20:46.547047 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:20:46.552055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5ADF6A0>]}
[0m15:20:46.552055 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:46.552055 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:20:46.552055 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:20:46.563060 [info ] [MainThread]: 
[0m15:20:46.563060 [debug] [Thread-1 (]: Began running node model.poc_demo.h_cli
[0m15:20:46.563060 [info ] [Thread-1 (]: 1 of 1 START sql incremental model ndb.h_cli ................................... [RUN]
[0m15:20:46.563060 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.h_cli'
[0m15:20:46.571755 [debug] [Thread-1 (]: Began compiling node model.poc_demo.h_cli
[0m15:20:46.677053 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.h_cli"
[0m15:20:46.677053 [debug] [Thread-1 (]: Timing info for model.poc_demo.h_cli (compile): 15:20:46.571755 => 15:20:46.677053
[0m15:20:46.677053 [debug] [Thread-1 (]: Began executing node model.poc_demo.h_cli
[0m15:20:46.761669 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.h_cli"
[0m15:20:46.764126 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:46.764126 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.h_cli"
[0m15:20:46.765172 [debug] [Thread-1 (]: On model.poc_demo.h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.h_cli"} */

  
    
        create table ndb.h_cli
      
      
    using hudi
      options (hoodie.datasource.write.storage.type "COPY_ON_WRITE" , hoodie.datasource.write.operation "upsert" , hoodie.datasource.write.precombine.field "ld_dt_tm" , hoodie.upsert.shuffle.parallelism "2" , hoodie.insert.shuffle.parallelism "2" , hoodie.table.base.file.format "PARQUET" , primaryKey "hk_cli_cd,ld_dt_tm" 
        )
      
      
      
      
      as
      










-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT * FROM (
    SELECT rr.`hk_cli_cd`, rr.`cli_id`, rr.`ld_dt_tm`, rr.`rcrd_src_nm`,
           ROW_NUMBER() OVER(
               PARTITION BY rr.`hk_cli_cd`
               ORDER BY rr.`ld_dt_tm`
           ) AS row_number
    FROM ndb.v_stg_h_cli AS rr
    WHERE rr.`hk_cli_cd` IS NOT NULL
     ) WHERE row_number = 1
),

records_to_insert AS (
    SELECT a.`hk_cli_cd`, a.`cli_id`, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM row_rank_1 AS a
)

SELECT * FROM records_to_insert
  
[0m15:20:46.765172 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:20:51.813243 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m15:20:56.417554 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:20:56.419092 [debug] [Thread-1 (]: SQL status: OK in 10.0 seconds
[0m15:20:56.436683 [debug] [Thread-1 (]: Timing info for model.poc_demo.h_cli (execute): 15:20:46.677053 => 15:20:56.421647
[0m15:20:56.436683 [debug] [Thread-1 (]: On model.poc_demo.h_cli: ROLLBACK
[0m15:20:56.436683 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:20:56.436683 [debug] [Thread-1 (]: On model.poc_demo.h_cli: Close
[0m15:20:56.440192 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31e66196-ada3-474b-97cf-1b168ad7044c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5A93760>]}
[0m15:20:56.440192 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model ndb.h_cli .............................. [[32mOK[0m in 9.88s]
[0m15:20:56.440192 [debug] [Thread-1 (]: Finished running node model.poc_demo.h_cli
[0m15:20:56.452704 [debug] [MainThread]: On master: ROLLBACK
[0m15:20:56.452704 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:20:56.498643 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:20:56.498643 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:20:56.498643 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:20:56.498643 [debug] [MainThread]: On master: ROLLBACK
[0m15:20:56.498643 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:20:56.498643 [debug] [MainThread]: On master: Close
[0m15:20:56.508268 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:20:56.508268 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:20:56.510197 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m15:20:56.511203 [debug] [MainThread]: Connection 'model.poc_demo.h_cli' was properly closed.
[0m15:20:56.511203 [info ] [MainThread]: 
[0m15:20:56.512209 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.62 seconds (10.62s).
[0m15:20:56.513469 [debug] [MainThread]: Command end result
[0m15:20:56.523805 [info ] [MainThread]: 
[0m15:20:56.523805 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:20:56.529356 [info ] [MainThread]: 
[0m15:20:56.530420 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:20:56.531442 [debug] [MainThread]: Command `dbt run` succeeded at 15:20:56.531442 after 10.95 seconds
[0m15:20:56.531442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B2CBDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B3003FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228B5B30C10>]}
[0m15:20:56.532445 [debug] [MainThread]: Flushing usage events
[0m15:23:31.687111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6C0FDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6E812DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6E812B90>]}


============================== 15:23:31.687111 | 34890a38-ab31-461c-bc63-44c90e78ca6a ==============================
[0m15:23:31.687111 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:23:31.687111 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:23:31.818725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6E812E00>]}
[0m15:23:31.826719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EBC2F20>]}
[0m15:23:31.827720 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:23:31.841262 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:23:31.955046 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:23:31.955046 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:23:31.970104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EEF00D0>]}
[0m15:23:31.986599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EEB1FF0>]}
[0m15:23:31.986599 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:23:31.986599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EEB2170>]}
[0m15:23:31.986599 [info ] [MainThread]: 
[0m15:23:31.986599 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:23:31.986599 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:23:32.005225 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:23:32.006322 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:23:32.006322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:23:32.112979 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:23:32.112979 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:23:32.121115 [debug] [ThreadPool]: On list_schemas: Close
[0m15:23:32.129090 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m15:23:32.129090 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:32.137697 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:23:32.137697 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:23:32.137697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:23:32.380046 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:23:32.381108 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:23:32.385125 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:23:32.385125 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:23:32.385125 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:23:32.400930 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m15:23:32.408951 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:32.408951 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:23:32.409997 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:23:32.409997 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:23:32.727729 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:23:32.729740 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:23:32.736248 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:23:32.736248 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:23:32.736248 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:23:32.750821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EF236D0>]}
[0m15:23:32.752324 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:32.752324 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:23:32.752324 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:23:32.752324 [info ] [MainThread]: 
[0m15:23:32.752324 [debug] [Thread-1 (]: Began running node model.poc_demo.raw_s_name
[0m15:23:32.752324 [info ] [Thread-1 (]: 1 of 3 START sql view model ndb.raw_s_name ..................................... [RUN]
[0m15:23:32.752324 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.raw_s_name'
[0m15:23:32.763870 [debug] [Thread-1 (]: Began compiling node model.poc_demo.raw_s_name
[0m15:23:32.766881 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.raw_s_name"
[0m15:23:32.768885 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_s_name (compile): 15:23:32.763870 => 15:23:32.767880
[0m15:23:32.769883 [debug] [Thread-1 (]: Began executing node model.poc_demo.raw_s_name
[0m15:23:32.795851 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.raw_s_name"
[0m15:23:32.799888 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:32.799888 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.raw_s_name"
[0m15:23:32.799888 [debug] [Thread-1 (]: On model.poc_demo.raw_s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.raw_s_name"} */
create or replace view ndb.raw_s_name
  
  as
    select distinct
v_h.cli_id,
v_h.rcrd_src_nm,
v_s_name.name,
v_s_name.ts
from ndb.raw_h_cli v_h
LEFT JOIN ndb.name2 v_s_name
ON v_h.cli_id = v_s_name.cli_id

[0m15:23:32.799888 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:23:33.044769 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:23:33.044769 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:23:33.053368 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_s_name (execute): 15:23:32.770399 => 15:23:33.053368
[0m15:23:33.053368 [debug] [Thread-1 (]: On model.poc_demo.raw_s_name: ROLLBACK
[0m15:23:33.053368 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:23:33.053368 [debug] [Thread-1 (]: On model.poc_demo.raw_s_name: Close
[0m15:23:33.075008 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EF73B50>]}
[0m15:23:33.077031 [info ] [Thread-1 (]: 1 of 3 OK created sql view model ndb.raw_s_name ................................ [[32mOK[0m in 0.32s]
[0m15:23:33.077031 [debug] [Thread-1 (]: Finished running node model.poc_demo.raw_s_name
[0m15:23:33.080142 [debug] [Thread-1 (]: Began running node model.poc_demo.v_stg_s_name
[0m15:23:33.080142 [info ] [Thread-1 (]: 2 of 3 START sql view model ndb.v_stg_s_name ................................... [RUN]
[0m15:23:33.080142 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.raw_s_name, now model.poc_demo.v_stg_s_name)
[0m15:23:33.083807 [debug] [Thread-1 (]: Began compiling node model.poc_demo.v_stg_s_name
[0m15:23:33.116036 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:33.116036 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_name"
[0m15:23:33.116036 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_name"} */

      describe extended ndb.raw_s_name
  
[0m15:23:33.116036 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:33.340709 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:23:33.340709 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:23:33.399468 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_name"
[0m15:23:33.399468 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_name"} */

      describe extended ndb.raw_s_name
  
[0m15:23:33.464163 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:23:33.464163 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:23:33.512716 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.v_stg_s_name"
[0m15:23:33.512716 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_s_name (compile): 15:23:33.083807 => 15:23:33.512716
[0m15:23:33.512716 [debug] [Thread-1 (]: Began executing node model.poc_demo.v_stg_s_name
[0m15:23:33.520810 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.v_stg_s_name"
[0m15:23:33.520810 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_name"
[0m15:23:33.520810 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_name"} */
create or replace view ndb.v_stg_s_name
  
  as
    







-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`,
    `name`,
    `ts`

    FROM ndb.raw_s_name
),

derived_columns AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`,
    `name`,
    `ts`,
    `CURRENT_TIMESTAMP` AS `ld_dt_tm`,
    `ts` AS `EFFECTIVE_FROM`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `NAME`,
    `TS`,
    `LD_DT_TM`,
    `EFFECTIVE_FROM`,

    CAST((MD5(NULLIF(UPPER(TRIM(CAST(`cli_id` AS STRING))), ''))) AS VARCHAR(16)) AS `hsh_ky_cli_cd`,
    CAST(MD5(NULLIF(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`name` AS STRING))), ''), '^^')
    ), '^^')) AS VARCHAR(16)) AS `rcrd_hsh_id`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `NAME`,
    `TS`,
    `LD_DT_TM`,
    `EFFECTIVE_FROM`,
    `HSH_KY_CLI_CD`,
    `RCRD_HSH_ID`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

[0m15:23:33.678762 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:23:33.686279 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:23:33.686279 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_s_name (execute): 15:23:33.512716 => 15:23:33.686279
[0m15:23:33.686279 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_name: ROLLBACK
[0m15:23:33.686279 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:23:33.686279 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_name: Close
[0m15:23:33.695474 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EECFA90>]}
[0m15:23:33.695474 [info ] [Thread-1 (]: 2 of 3 OK created sql view model ndb.v_stg_s_name .............................. [[32mOK[0m in 0.62s]
[0m15:23:33.701980 [debug] [Thread-1 (]: Finished running node model.poc_demo.v_stg_s_name
[0m15:23:33.701980 [debug] [Thread-1 (]: Began running node model.poc_demo.s_name
[0m15:23:33.701980 [info ] [Thread-1 (]: 3 of 3 START sql incremental model ndb.s_name .................................. [RUN]
[0m15:23:33.701980 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.v_stg_s_name, now model.poc_demo.s_name)
[0m15:23:33.701980 [debug] [Thread-1 (]: Began compiling node model.poc_demo.s_name
[0m15:23:33.821771 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.s_name"
[0m15:23:33.824318 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_name (compile): 15:23:33.701980 => 15:23:33.823312
[0m15:23:33.825323 [debug] [Thread-1 (]: Began executing node model.poc_demo.s_name
[0m15:23:33.918072 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.s_name"
[0m15:23:33.933807 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:33.933807 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_name"
[0m15:23:33.933807 [debug] [Thread-1 (]: On model.poc_demo.s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_name"} */

  
    
        create table ndb.s_name
      
      
    using hudi
      options (hoodie.datasource.write.storage.type "COPY_ON_WRITE" , hoodie.datasource.write.operation "upsert" , hoodie.datasource.write.precombine.field "ld_dt_tm" , hoodie.upsert.shuffle.parallelism "2" , hoodie.insert.shuffle.parallelism "2" , hoodie.table.base.file.format "PARQUET" , primaryKey "hsh_ky_cli_cd,ld_dt_tm" 
        )
      
      
      
      
      as
      














-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`name`, a.EFFECTIVE_FROM, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM ndb.v_stg_s_name AS a
    WHERE a.`hsh_ky_cli_cd` IS NOT NULL
),

records_to_insert AS (
    SELECT DISTINCT stage.`hsh_ky_cli_cd`, stage.`rcrd_hsh_id`, stage.`name`, stage.EFFECTIVE_FROM, stage.`ld_dt_tm` AS `ld_dt_tm`, stage.`rcrd_src_nm` AS `rcrd_src_nm`
    FROM source_data AS stage
)

SELECT * FROM records_to_insert
  
[0m15:23:33.933807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:37.432503 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:23:37.436095 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m15:23:37.444614 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_name (execute): 15:23:33.825323 => 15:23:37.444614
[0m15:23:37.444614 [debug] [Thread-1 (]: On model.poc_demo.s_name: ROLLBACK
[0m15:23:37.444614 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:23:37.444614 [debug] [Thread-1 (]: On model.poc_demo.s_name: Close
[0m15:23:37.459607 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34890a38-ab31-461c-bc63-44c90e78ca6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EECFD60>]}
[0m15:23:37.460609 [info ] [Thread-1 (]: 3 of 3 OK created sql incremental model ndb.s_name ............................. [[32mOK[0m in 3.76s]
[0m15:23:37.461605 [debug] [Thread-1 (]: Finished running node model.poc_demo.s_name
[0m15:23:37.463607 [debug] [MainThread]: On master: ROLLBACK
[0m15:23:37.463607 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:23:37.517112 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:23:37.517112 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:23:37.517112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:23:37.517112 [debug] [MainThread]: On master: ROLLBACK
[0m15:23:37.517112 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:23:37.517112 [debug] [MainThread]: On master: Close
[0m15:23:37.529631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:23:37.529631 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:23:37.529631 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m15:23:37.529631 [debug] [MainThread]: Connection 'model.poc_demo.s_name' was properly closed.
[0m15:23:37.529631 [info ] [MainThread]: 
[0m15:23:37.529631 [info ] [MainThread]: Finished running 2 view models, 1 incremental model in 0 hours 0 minutes and 5.54 seconds (5.54s).
[0m15:23:37.529631 [debug] [MainThread]: Command end result
[0m15:23:37.546145 [info ] [MainThread]: 
[0m15:23:37.548142 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:23:37.548270 [info ] [MainThread]: 
[0m15:23:37.549527 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:23:37.549527 [debug] [MainThread]: Command `dbt run` succeeded at 15:23:37.549527 after 5.88 seconds
[0m15:23:37.549527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6C0FDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6EBC1780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B6E812E00>]}
[0m15:23:37.549527 [debug] [MainThread]: Flushing usage events
[0m15:24:03.224361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288C4EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288EC02D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288EC02B00>]}


============================== 15:24:03.224361 | 28f7bf1e-0d15-45cd-9e93-8f220ffb1168 ==============================
[0m15:24:03.224361 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:24:03.224361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:24:03.370512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288EC02D70>]}
[0m15:24:03.374057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288EFB2C80>]}
[0m15:24:03.374057 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:24:03.407814 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:24:03.530724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:24:03.530724 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:24:03.539036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F2E00D0>]}
[0m15:24:03.542250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F29DF60>]}
[0m15:24:03.542250 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:24:03.542250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F29E0E0>]}
[0m15:24:03.557961 [info ] [MainThread]: 
[0m15:24:03.558869 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:24:03.558869 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:24:03.570462 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:24:03.570462 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:24:03.570462 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:24:03.674757 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:24:03.674757 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:24:03.683915 [debug] [ThreadPool]: On list_schemas: Close
[0m15:24:03.693998 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m15:24:03.693998 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:03.693998 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:24:03.702006 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:24:03.702006 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:24:03.970441 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:24:03.970972 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:24:03.979744 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:24:03.979744 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:24:03.981253 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:24:03.990262 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m15:24:03.992315 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:03.992315 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:24:03.992315 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:24:03.992315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:24:04.231078 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:24:04.231078 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:24:04.236619 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:24:04.236619 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:24:04.236619 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:24:04.252450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F30F640>]}
[0m15:24:04.252450 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:04.252450 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:24:04.252450 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:24:04.252450 [info ] [MainThread]: 
[0m15:24:04.263969 [debug] [Thread-1 (]: Began running node model.poc_demo.raw_s_address
[0m15:24:04.263969 [info ] [Thread-1 (]: 1 of 3 START sql view model ndb.raw_s_address .................................. [RUN]
[0m15:24:04.263969 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.raw_s_address'
[0m15:24:04.263969 [debug] [Thread-1 (]: Began compiling node model.poc_demo.raw_s_address
[0m15:24:04.274054 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.raw_s_address"
[0m15:24:04.281361 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_s_address (compile): 15:24:04.263969 => 15:24:04.281361
[0m15:24:04.281361 [debug] [Thread-1 (]: Began executing node model.poc_demo.raw_s_address
[0m15:24:04.307739 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.raw_s_address"
[0m15:24:04.311751 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:04.312550 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.raw_s_address"
[0m15:24:04.312550 [debug] [Thread-1 (]: On model.poc_demo.raw_s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.raw_s_address"} */
create or replace view ndb.raw_s_address
  
  as
    select distinct
v_h.cli_id,
v_h.rcrd_src_nm,
v_s_address.addr,
v_s_address.ts
from ndb.raw_h_cli v_h
LEFT JOIN ndb.address2 v_s_address
ON v_h.cli_id = v_s_address.cli_id

[0m15:24:04.312550 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:24:04.512035 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:24:04.512035 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:24:04.522634 [debug] [Thread-1 (]: Timing info for model.poc_demo.raw_s_address (execute): 15:24:04.281361 => 15:24:04.522634
[0m15:24:04.522634 [debug] [Thread-1 (]: On model.poc_demo.raw_s_address: ROLLBACK
[0m15:24:04.522634 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:24:04.522634 [debug] [Thread-1 (]: On model.poc_demo.raw_s_address: Close
[0m15:24:04.531144 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F35FCA0>]}
[0m15:24:04.531144 [info ] [Thread-1 (]: 1 of 3 OK created sql view model ndb.raw_s_address ............................. [[32mOK[0m in 0.27s]
[0m15:24:04.531144 [debug] [Thread-1 (]: Finished running node model.poc_demo.raw_s_address
[0m15:24:04.531144 [debug] [Thread-1 (]: Began running node model.poc_demo.v_stg_s_address
[0m15:24:04.531144 [info ] [Thread-1 (]: 2 of 3 START sql view model ndb.v_stg_s_address ................................ [RUN]
[0m15:24:04.538650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.raw_s_address, now model.poc_demo.v_stg_s_address)
[0m15:24:04.538650 [debug] [Thread-1 (]: Began compiling node model.poc_demo.v_stg_s_address
[0m15:24:04.576010 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:04.576010 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_address"
[0m15:24:04.576010 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_address"} */

      describe extended ndb.raw_s_address
  
[0m15:24:04.576010 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:24:04.699141 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:24:04.703153 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:24:04.761889 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_address"
[0m15:24:04.761889 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_address"} */

      describe extended ndb.raw_s_address
  
[0m15:24:04.827701 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:24:04.827701 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:24:04.872381 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.v_stg_s_address"
[0m15:24:04.872381 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_s_address (compile): 15:24:04.538650 => 15:24:04.872381
[0m15:24:04.872381 [debug] [Thread-1 (]: Began executing node model.poc_demo.v_stg_s_address
[0m15:24:04.883413 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.v_stg_s_address"
[0m15:24:04.883413 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.v_stg_s_address"
[0m15:24:04.883413 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.v_stg_s_address"} */
create or replace view ndb.v_stg_s_address
  
  as
    







-- Generated by dbtvault.



WITH source_data AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`,
    `addr`,
    `ts`

    FROM ndb.raw_s_address
),

derived_columns AS (

    SELECT

    `cli_id`,
    `rcrd_src_nm`,
    `addr`,
    `ts`,
    `CURRENT_TIMESTAMP` AS `ld_dt_tm`,
    `ts` AS `EFFECTIVE_FROM`

    FROM source_data
),

hashed_columns AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `ADDR`,
    `TS`,
    `LD_DT_TM`,
    `EFFECTIVE_FROM`,

    CAST((MD5(NULLIF(UPPER(TRIM(CAST(`cli_id` AS STRING))), ''))) AS VARCHAR(16)) AS `hsh_ky_cli_cd`,
    CAST(MD5(NULLIF(CONCAT_WS('||',
        IFNULL(NULLIF(UPPER(TRIM(CAST(`addr` AS STRING))), ''), '^^')
    ), '^^')) AS VARCHAR(16)) AS `rcrd_hsh_id`

    FROM derived_columns
),

columns_to_select AS (

    SELECT

    `CLI_ID`,
    `RCRD_SRC_NM`,
    `ADDR`,
    `TS`,
    `LD_DT_TM`,
    `EFFECTIVE_FROM`,
    `HSH_KY_CLI_CD`,
    `RCRD_HSH_ID`

    FROM hashed_columns
)

SELECT * FROM columns_to_select

[0m15:24:05.036995 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:24:05.037992 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:24:05.038991 [debug] [Thread-1 (]: Timing info for model.poc_demo.v_stg_s_address (execute): 15:24:04.872381 => 15:24:05.038991
[0m15:24:05.040498 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_address: ROLLBACK
[0m15:24:05.040498 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:24:05.041509 [debug] [Thread-1 (]: On model.poc_demo.v_stg_s_address: Close
[0m15:24:05.049691 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F2CC520>]}
[0m15:24:05.049691 [info ] [Thread-1 (]: 2 of 3 OK created sql view model ndb.v_stg_s_address ........................... [[32mOK[0m in 0.51s]
[0m15:24:05.049691 [debug] [Thread-1 (]: Finished running node model.poc_demo.v_stg_s_address
[0m15:24:05.049691 [debug] [Thread-1 (]: Began running node model.poc_demo.s_address
[0m15:24:05.049691 [info ] [Thread-1 (]: 3 of 3 START sql incremental model ndb.s_address ............................... [RUN]
[0m15:24:05.049691 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.v_stg_s_address, now model.poc_demo.s_address)
[0m15:24:05.057264 [debug] [Thread-1 (]: Began compiling node model.poc_demo.s_address
[0m15:24:05.144003 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.s_address"
[0m15:24:05.153668 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_address (compile): 15:24:05.057264 => 15:24:05.153021
[0m15:24:05.153668 [debug] [Thread-1 (]: Began executing node model.poc_demo.s_address
[0m15:24:05.261660 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.s_address"
[0m15:24:05.261660 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:05.261660 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_address"
[0m15:24:05.261660 [debug] [Thread-1 (]: On model.poc_demo.s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_address"} */

  
    
        create table ndb.s_address
      
      
    using hudi
      options (hoodie.datasource.write.storage.type "COPY_ON_WRITE" , hoodie.datasource.write.operation "upsert" , hoodie.datasource.write.precombine.field "ld_dt_tm" , hoodie.upsert.shuffle.parallelism "2" , hoodie.insert.shuffle.parallelism "2" , hoodie.table.base.file.format "PARQUET" , primaryKey "hsh_ky_cli_cd,ld_dt_tm" 
        )
      
      
      
      
      as
      














-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`addr`, a.EFFECTIVE_FROM, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM ndb.v_stg_s_address AS a
    WHERE a.`hsh_ky_cli_cd` IS NOT NULL
),

records_to_insert AS (
    SELECT DISTINCT stage.`hsh_ky_cli_cd`, stage.`rcrd_hsh_id`, stage.`addr`, stage.EFFECTIVE_FROM, stage.`ld_dt_tm` AS `ld_dt_tm`, stage.`rcrd_src_nm` AS `rcrd_src_nm`
    FROM source_data AS stage
)

SELECT * FROM records_to_insert
  
[0m15:24:05.261660 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:24:08.145175 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:24:08.145175 [debug] [Thread-1 (]: SQL status: OK in 3.0 seconds
[0m15:24:08.155740 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_address (execute): 15:24:05.153668 => 15:24:08.155740
[0m15:24:08.155740 [debug] [Thread-1 (]: On model.poc_demo.s_address: ROLLBACK
[0m15:24:08.155740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:24:08.155740 [debug] [Thread-1 (]: On model.poc_demo.s_address: Close
[0m15:24:08.176369 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28f7bf1e-0d15-45cd-9e93-8f220ffb1168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288EC02B60>]}
[0m15:24:08.176369 [info ] [Thread-1 (]: 3 of 3 OK created sql incremental model ndb.s_address .......................... [[32mOK[0m in 3.13s]
[0m15:24:08.176369 [debug] [Thread-1 (]: Finished running node model.poc_demo.s_address
[0m15:24:08.181386 [debug] [MainThread]: On master: ROLLBACK
[0m15:24:08.181386 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:24:08.219219 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:24:08.222853 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:24:08.222853 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:24:08.222853 [debug] [MainThread]: On master: ROLLBACK
[0m15:24:08.222853 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:24:08.224238 [debug] [MainThread]: On master: Close
[0m15:24:08.230331 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:24:08.230331 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:24:08.230331 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m15:24:08.230331 [debug] [MainThread]: Connection 'model.poc_demo.s_address' was properly closed.
[0m15:24:08.233387 [info ] [MainThread]: 
[0m15:24:08.233387 [info ] [MainThread]: Finished running 2 view models, 1 incremental model in 0 hours 0 minutes and 4.67 seconds (4.67s).
[0m15:24:08.233387 [debug] [MainThread]: Command end result
[0m15:24:08.244533 [info ] [MainThread]: 
[0m15:24:08.244533 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:24:08.251549 [info ] [MainThread]: 
[0m15:24:08.251549 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:24:08.251549 [debug] [MainThread]: Command `dbt run` succeeded at 15:24:08.251549 after 5.04 seconds
[0m15:24:08.251549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288C4EDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288C677EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002288F29E170>]}
[0m15:24:08.251549 [debug] [MainThread]: Flushing usage events
[0m15:25:58.307917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B5E2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8542DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8542BF0>]}


============================== 15:25:58.323551 | c8808ffe-aafb-4475-a596-9634a80bd1f6 ==============================
[0m15:25:58.323551 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:25:58.323551 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:25:58.448742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8542E00>]}
[0m15:25:58.464925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8902DA0>]}
[0m15:25:58.464925 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:25:58.495641 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:25:58.640423 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:25:58.641475 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\pit_test\pit_test\as_of_date.sql
[0m15:25:58.660636 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\as_of_date.sql
[0m15:25:58.681215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8CC00D0>]}
[0m15:25:58.699372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8BEE0B0>]}
[0m15:25:58.699372 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:25:58.699372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8BEE170>]}
[0m15:25:58.699372 [info ] [MainThread]: 
[0m15:25:58.699372 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:25:58.699372 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:25:58.722354 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:25:58.725107 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:25:58.725774 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:58.841151 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:25:58.842244 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:25:58.848159 [debug] [ThreadPool]: On list_schemas: Close
[0m15:25:58.849602 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m15:25:58.859924 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:25:58.865034 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:25:58.865034 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:25:58.865034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:59.121260 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:25:59.121260 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:25:59.134799 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:25:59.134799 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:25:59.134799 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:25:59.134799 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m15:25:59.150438 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:25:59.150438 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:25:59.151946 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:25:59.151946 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:25:59.403278 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:25:59.403278 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:25:59.412444 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:25:59.412444 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:25:59.412444 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:25:59.428731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8C97F10>]}
[0m15:25:59.428731 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:25:59.428731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:25:59.428731 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:25:59.428731 [info ] [MainThread]: 
[0m15:25:59.428731 [debug] [Thread-1 (]: Began running node model.poc_demo.as_of_date
[0m15:25:59.428731 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.as_of_date ..................................... [RUN]
[0m15:25:59.439446 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.as_of_date'
[0m15:25:59.439956 [debug] [Thread-1 (]: Began compiling node model.poc_demo.as_of_date
[0m15:25:59.441969 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.as_of_date"
[0m15:25:59.442974 [debug] [Thread-1 (]: Timing info for model.poc_demo.as_of_date (compile): 15:25:59.439956 => 15:25:59.442974
[0m15:25:59.443971 [debug] [Thread-1 (]: Began executing node model.poc_demo.as_of_date
[0m15:25:59.471072 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.as_of_date"
[0m15:25:59.472077 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:25:59.473072 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.as_of_date"
[0m15:25:59.473072 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.as_of_date"} */
create or replace view ndb.as_of_date
  
  as
    WITH as_of_date AS (
    select hsh_ky_cli_cd,ts from ndb.v_stg_s_address
    union
    select hsh_ky_cli_cd,ts from ndb.v_stg_s_name
)

 

SELECT distinct hsh_ky_cli_cd, ts  as AS_OF_DATE FROM as_of_date

[0m15:25:59.474068 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:25:59.818884 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:25:59.818884 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:25:59.828433 [debug] [Thread-1 (]: Timing info for model.poc_demo.as_of_date (execute): 15:25:59.443971 => 15:25:59.828433
[0m15:25:59.828433 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: ROLLBACK
[0m15:25:59.828433 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:25:59.828433 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: Close
[0m15:25:59.845091 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8808ffe-aafb-4475-a596-9634a80bd1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8D11FF0>]}
[0m15:25:59.845091 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.as_of_date ................................ [[32mOK[0m in 0.41s]
[0m15:25:59.845091 [debug] [Thread-1 (]: Finished running node model.poc_demo.as_of_date
[0m15:25:59.845091 [debug] [MainThread]: On master: ROLLBACK
[0m15:25:59.845091 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:25:59.896076 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:25:59.897170 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:25:59.897170 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:25:59.897170 [debug] [MainThread]: On master: ROLLBACK
[0m15:25:59.897170 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:25:59.899159 [debug] [MainThread]: On master: Close
[0m15:25:59.907060 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:25:59.907060 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:25:59.907060 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m15:25:59.907060 [debug] [MainThread]: Connection 'model.poc_demo.as_of_date' was properly closed.
[0m15:25:59.907060 [info ] [MainThread]: 
[0m15:25:59.907060 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.21 seconds (1.21s).
[0m15:25:59.907060 [debug] [MainThread]: Command end result
[0m15:25:59.925397 [info ] [MainThread]: 
[0m15:25:59.925397 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:25:59.925397 [info ] [MainThread]: 
[0m15:25:59.925397 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:25:59.925397 [debug] [MainThread]: Command `dbt run` succeeded at 15:25:59.925397 after 1.63 seconds
[0m15:25:59.925397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B5E2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B6165540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244B8D4E890>]}
[0m15:25:59.925397 [debug] [MainThread]: Flushing usage events
[0m15:37:39.226220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4741DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D49B32DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D49B32BC0>]}


============================== 15:37:39.226220 | c30a9c15-f533-4715-bb55-bfff4f977530 ==============================
[0m15:37:39.226220 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:37:39.226220 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:37:39.351110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D49B32E30>]}
[0m15:37:39.366181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D49ED6F50>]}
[0m15:37:39.366181 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:37:39.407423 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:37:39.547530 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:37:39.558093 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\pit_test\pit_test\as_of_date.sql
[0m15:37:39.558093 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m15:37:39.616007 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\as_of_date.sql
[0m15:37:39.625319 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m15:37:39.716051 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m15:37:39.732105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4A35C0D0>]}
[0m15:37:39.747192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4A1CE0B0>]}
[0m15:37:39.747192 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:37:39.747192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4A1CE380>]}
[0m15:37:39.747192 [info ] [MainThread]: 
[0m15:37:39.747192 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:37:39.762870 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:37:39.762870 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:37:39.762870 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:37:39.762870 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:37:39.886205 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:37:39.886205 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:37:39.897244 [debug] [ThreadPool]: On list_schemas: Close
[0m15:37:39.909304 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m15:37:39.919824 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:37:39.919824 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:37:39.919824 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:37:39.919824 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:37:40.200725 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:37:40.206329 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:37:40.208396 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:37:40.215988 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:37:40.216953 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:37:40.216953 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m15:37:40.226461 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:37:40.226461 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:37:40.231972 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:37:40.231972 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:37:40.523328 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:37:40.523328 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:37:40.532848 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:37:40.532848 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:37:40.532848 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:37:40.532848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4A3534C0>]}
[0m15:37:40.532848 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:37:40.532848 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:37:40.548527 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:37:40.549532 [info ] [MainThread]: 
[0m15:37:40.549532 [debug] [Thread-1 (]: Began running node model.poc_demo.as_of_date
[0m15:37:40.549532 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.as_of_date ..................................... [RUN]
[0m15:37:40.549532 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.as_of_date'
[0m15:37:40.549532 [debug] [Thread-1 (]: Began compiling node model.poc_demo.as_of_date
[0m15:37:40.608609 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.as_of_date"
[0m15:37:40.608609 [debug] [Thread-1 (]: Timing info for model.poc_demo.as_of_date (compile): 15:37:40.549532 => 15:37:40.608609
[0m15:37:40.608609 [debug] [Thread-1 (]: Began executing node model.poc_demo.as_of_date
[0m15:37:40.647922 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.as_of_date"
[0m15:37:40.649926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:37:40.650901 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.as_of_date"
[0m15:37:40.650901 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.as_of_date"} */
create or replace view ndb.as_of_date
  
  as
    WITH as_of_date AS (
    select hsh_ky_cli_cd,ts from ndb.v_stg_s_address
    union
    select hsh_ky_cli_cd,ts from ndb.v_stg_s_name
)

 

SELECT distinct hsh_ky_cli_cd as hk_cli_cd, ts  as AS_OF_DATE FROM as_of_date

[0m15:37:40.650901 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:37:41.882032 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:37:41.882032 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m15:37:41.895236 [debug] [Thread-1 (]: Timing info for model.poc_demo.as_of_date (execute): 15:37:40.608609 => 15:37:41.895236
[0m15:37:41.895236 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: ROLLBACK
[0m15:37:41.895236 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:37:41.895236 [debug] [Thread-1 (]: On model.poc_demo.as_of_date: Close
[0m15:37:41.911073 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c30a9c15-f533-4715-bb55-bfff4f977530', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D49B32C20>]}
[0m15:37:41.911073 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.as_of_date ................................ [[32mOK[0m in 1.36s]
[0m15:37:41.911073 [debug] [Thread-1 (]: Finished running node model.poc_demo.as_of_date
[0m15:37:41.911073 [debug] [MainThread]: On master: ROLLBACK
[0m15:37:41.911073 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:37:41.953346 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:37:41.953346 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:37:41.953346 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:37:41.953346 [debug] [MainThread]: On master: ROLLBACK
[0m15:37:41.960856 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:37:41.960856 [debug] [MainThread]: On master: Close
[0m15:37:41.963057 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:37:41.963057 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:37:41.967439 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m15:37:41.968487 [debug] [MainThread]: Connection 'model.poc_demo.as_of_date' was properly closed.
[0m15:37:41.969446 [info ] [MainThread]: 
[0m15:37:41.970446 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.22 seconds (2.22s).
[0m15:37:41.970446 [debug] [MainThread]: Command end result
[0m15:37:41.983181 [info ] [MainThread]: 
[0m15:37:41.984969 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:37:41.984969 [info ] [MainThread]: 
[0m15:37:41.985965 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:37:41.986969 [debug] [MainThread]: Command `dbt run` succeeded at 15:37:41.986969 after 2.77 seconds
[0m15:37:41.986969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4741DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D4A277D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D477141C0>]}
[0m15:37:41.987965 [debug] [MainThread]: Flushing usage events
[0m15:41:20.143095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C165E3DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168552DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168552BF0>]}


============================== 15:41:20.143095 | 9799a6c2-2309-43ea-b0be-ad064b372128 ==============================
[0m15:41:20.143095 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:41:20.143095 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:41:20.282164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168552E00>]}
[0m15:41:20.282164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168902DA0>]}
[0m15:41:20.282164 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m15:41:20.321533 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:41:20.450555 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:41:20.450555 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m15:41:20.450555 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\pit_test\pit_test\pit_client.sql
[0m15:41:20.497427 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m15:41:20.607534 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m15:41:20.623142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168D780D0>]}
[0m15:41:20.643792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168BEE080>]}
[0m15:41:20.643792 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:41:20.643792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168BEE350>]}
[0m15:41:20.643792 [info ] [MainThread]: 
[0m15:41:20.643792 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:41:20.643792 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:41:20.667018 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:41:20.668034 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:41:20.669071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:20.769362 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:41:20.769362 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:41:20.780372 [debug] [ThreadPool]: On list_schemas: Close
[0m15:41:20.785972 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m15:41:20.785972 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:41:20.785972 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m15:41:20.785972 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m15:41:20.785972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:41:21.105992 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:41:21.107505 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:41:21.113800 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m15:41:21.113800 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:41:21.113800 [debug] [ThreadPool]: On list_None_ndb: Close
[0m15:41:21.127009 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m15:41:21.127009 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:41:21.127009 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m15:41:21.127009 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m15:41:21.127009 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:41:21.395913 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:41:21.396682 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:41:21.399175 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m15:41:21.409182 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:41:21.409182 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m15:41:21.415572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168D775E0>]}
[0m15:41:21.415572 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:41:21.415572 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:41:21.426100 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:41:21.426100 [info ] [MainThread]: 
[0m15:41:21.427610 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client
[0m15:41:21.427610 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client ..................................... [RUN]
[0m15:41:21.427610 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client'
[0m15:41:21.438776 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client
[0m15:41:21.503068 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client"
[0m15:41:21.503068 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client (compile): 15:41:21.438776 => 15:41:21.503068
[0m15:41:21.503068 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client
[0m15:41:21.539883 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client"
[0m15:41:21.541880 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:41:21.541880 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client"
[0m15:41:21.542919 [debug] [Thread-1 (]: On model.poc_demo.pit_client: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client"} */
create or replace view ndb.pit_client
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),

pit AS (
    SELECT * FROM new_rows
)

SELECT DISTINCT * FROM pit

[0m15:41:21.542919 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:41:22.007674 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:41:22.010800 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:41:22.022171 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client (execute): 15:41:21.503068 => 15:41:22.022171
[0m15:41:22.023178 [debug] [Thread-1 (]: On model.poc_demo.pit_client: ROLLBACK
[0m15:41:22.024159 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:41:22.024159 [debug] [Thread-1 (]: On model.poc_demo.pit_client: Close
[0m15:41:22.033962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9799a6c2-2309-43ea-b0be-ad064b372128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168D12170>]}
[0m15:41:22.035472 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.pit_client ................................ [[32mOK[0m in 0.61s]
[0m15:41:22.035472 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client
[0m15:41:22.035472 [debug] [MainThread]: On master: ROLLBACK
[0m15:41:22.035472 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:41:22.079753 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:41:22.079753 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:41:22.079753 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:41:22.079753 [debug] [MainThread]: On master: ROLLBACK
[0m15:41:22.079753 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:41:22.079753 [debug] [MainThread]: On master: Close
[0m15:41:22.088575 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:41:22.089956 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m15:41:22.089956 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m15:41:22.089956 [debug] [MainThread]: Connection 'model.poc_demo.pit_client' was properly closed.
[0m15:41:22.089956 [info ] [MainThread]: 
[0m15:41:22.089956 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.45 seconds (1.45s).
[0m15:41:22.089956 [debug] [MainThread]: Command end result
[0m15:41:22.106551 [info ] [MainThread]: 
[0m15:41:22.106551 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:41:22.106551 [info ] [MainThread]: 
[0m15:41:22.106551 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:41:22.106551 [debug] [MainThread]: Command `dbt run` succeeded at 15:41:22.106551 after 1.99 seconds
[0m15:41:22.106551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C165E3DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1681A5060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C168552E00>]}
[0m15:41:22.106551 [debug] [MainThread]: Flushing usage events
[0m20:29:39.704275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7235DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB74A72BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB74A72A10>]}


============================== 20:29:39.705979 | c49950e9-7a5d-410c-9fb5-7017f2dddeb9 ==============================
[0m20:29:39.705979 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:29:39.705979 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:29:39.705979 [info ] [MainThread]: dbt version: 1.5.2
[0m20:29:39.705979 [info ] [MainThread]: python version: 3.10.11
[0m20:29:39.705979 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m20:29:39.705979 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m20:29:39.705979 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m20:29:39.705979 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\poc_demo\dbt_project.yml
[0m20:29:39.705979 [info ] [MainThread]: Configuration:
[0m20:29:39.844172 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:29:39.869606 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:29:39.869606 [info ] [MainThread]: Required dependencies:
[0m20:29:39.872266 [debug] [MainThread]: Executing "git --help"
[0m20:29:39.913203 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:29:39.916711 [debug] [MainThread]: STDERR: "b''"
[0m20:29:39.916711 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:29:39.916711 [info ] [MainThread]: Connection:
[0m20:29:39.920770 [info ] [MainThread]:   host: localhost
[0m20:29:39.921705 [info ] [MainThread]:   port: 10000
[0m20:29:39.921705 [info ] [MainThread]:   cluster: None
[0m20:29:39.921705 [info ] [MainThread]:   endpoint: None
[0m20:29:39.921705 [info ] [MainThread]:   schema: ndb
[0m20:29:39.921705 [info ] [MainThread]:   organization: 0
[0m20:29:39.930100 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m20:29:39.931065 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m20:29:39.931065 [debug] [MainThread]: Using spark connection "debug"
[0m20:29:39.931065 [debug] [MainThread]: On debug: select 1 as id
[0m20:29:39.931065 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:29:40.272180 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:29:40.273201 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m20:29:40.275211 [debug] [MainThread]: On debug: Close
[0m20:29:40.313733 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:29:40.317254 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:29:40.317254 [debug] [MainThread]: Command `dbt debug` succeeded at 20:29:40.317254 after 0.63 seconds
[0m20:29:40.317254 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:29:40.317254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7235DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB714E7A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB714E6470>]}
[0m20:29:40.320922 [debug] [MainThread]: Flushing usage events
[0m20:30:40.712968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C5F3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8652D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8652B30>]}


============================== 20:30:40.719096 | c01dfcdf-633d-4f1f-a236-7c7dac98b401 ==============================
[0m20:30:40.719096 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:30:40.720827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:30:40.863920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c01dfcdf-633d-4f1f-a236-7c7dac98b401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8652DA0>]}
[0m20:30:40.873812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c01dfcdf-633d-4f1f-a236-7c7dac98b401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8A02CB0>]}
[0m20:30:40.873812 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m20:30:40.904144 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:30:41.118479 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:41.118479 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:41.126137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c01dfcdf-633d-4f1f-a236-7c7dac98b401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8D300D0>]}
[0m20:30:41.142074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c01dfcdf-633d-4f1f-a236-7c7dac98b401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8CEDF90>]}
[0m20:30:41.143582 [info ] [MainThread]: Found 17 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m20:30:41.143582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c01dfcdf-633d-4f1f-a236-7c7dac98b401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8CEE110>]}
[0m20:30:41.143582 [warn ] [MainThread]: The selection criterion 'final_table' does not match any nodes
[0m20:30:41.143582 [info ] [MainThread]: 
[0m20:30:41.143582 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m20:30:41.143582 [debug] [MainThread]: Command end result
[0m20:30:41.162205 [debug] [MainThread]: Command `dbt run` succeeded at 20:30:41.159783 after 0.47 seconds
[0m20:30:41.162205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C5F3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C60C7EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C8CEE1A0>]}
[0m20:30:41.162205 [debug] [MainThread]: Flushing usage events
[0m20:31:12.867920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED6C2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9342DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9342BC0>]}


============================== 20:31:12.873426 | ddf013be-fb67-4b79-b7b5-4a100a7ba834 ==============================
[0m20:31:12.873426 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:31:12.874452 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:31:13.013696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9342E30>]}
[0m20:31:13.023850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED96E6F50>]}
[0m20:31:13.023850 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m20:31:13.053410 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:31:13.163824 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:31:13.163824 [debug] [MainThread]: Partial parsing: added file: poc_demo://models\pit_test\pit_test\final_table.sql
[0m20:31:13.190256 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\final_table.sql
[0m20:31:13.213653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9AAC0D0>]}
[0m20:31:13.223513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED97F35E0>]}
[0m20:31:13.223513 [info ] [MainThread]: Found 18 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m20:31:13.231388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED97F36A0>]}
[0m20:31:13.231388 [info ] [MainThread]: 
[0m20:31:13.233414 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:31:13.233414 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:31:13.245001 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:31:13.245001 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:31:13.245001 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:31:13.413275 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:31:13.413275 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:31:13.426203 [debug] [ThreadPool]: On list_schemas: Close
[0m20:31:13.445638 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m20:31:13.448160 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:13.448160 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m20:31:13.453687 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m20:31:13.453687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:31:14.304097 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:31:14.304097 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m20:31:14.313643 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m20:31:14.313643 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:31:14.313643 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m20:31:14.323309 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m20:31:14.335359 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:14.335359 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m20:31:14.335359 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m20:31:14.335359 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:31:14.969543 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:31:14.970054 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m20:31:14.980072 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m20:31:14.980072 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:31:14.980072 [debug] [ThreadPool]: On list_None_ndb: Close
[0m20:31:15.002215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9A7FA90>]}
[0m20:31:15.003234 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:15.004252 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:31:15.004252 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:31:15.006268 [info ] [MainThread]: 
[0m20:31:15.013300 [debug] [Thread-1 (]: Began running node model.poc_demo.final_table
[0m20:31:15.013300 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.final_table .................................... [RUN]
[0m20:31:15.017810 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.final_table'
[0m20:31:15.017810 [debug] [Thread-1 (]: Began compiling node model.poc_demo.final_table
[0m20:31:15.017810 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.final_table"
[0m20:31:15.023321 [debug] [Thread-1 (]: Timing info for model.poc_demo.final_table (compile): 20:31:15.017810 => 20:31:15.017810
[0m20:31:15.023321 [debug] [Thread-1 (]: Began executing node model.poc_demo.final_table
[0m20:31:15.054508 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.final_table"
[0m20:31:15.059696 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:15.059696 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.final_table"
[0m20:31:15.059696 [debug] [Thread-1 (]: On model.poc_demo.final_table: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.final_table"} */
create or replace view ndb.final_table
  
  as
    select 
hk_cli_cd,
start_date,
end_date,
addr,
name
from (
select distinct p.hk_cli_cd, p.as_of_date as start_date,
lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
a.addr, b.name
from ndb.pit_client p
LEFT JOIN s_address a
    ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
LEFT JOIN s_name b
    ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
order by 1 asc
)

[0m20:31:15.059696 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:31:16.697389 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;\n'CreateViewCommand `ndb`.`final_table`, select \r\nhk_cli_cd,\r\nstart_date,\r\nend_date,\r\naddr,\r\nname\r\nfrom (\r\nselect distinct p.hk_cli_cd, p.as_of_date as start_date,\r\nlead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,\r\na.addr, b.name\r\nfrom ndb.pit_client p\r\nLEFT JOIN s_address a\r\n    ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from\r\nLEFT JOIN s_name b\r\n    ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd\r\norder by 1 asc\r\n), false, true, PersistedView, false\n+- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]\n   +- 'SubqueryAlias __auto_generated_subquery_name\n      +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true\n         +- 'Distinct\n            +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]\n               +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))\n                  :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))\n                  :  :- SubqueryAlias p\n                  :  :  +- SubqueryAlias spark_catalog.ndb.pit_client\n                  :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])\n                  :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]\n                  :  :           +- WithCTE\n                  :  :              :- CTERelationDef 0, false\n                  :  :              :  +- SubqueryAlias as_of_dates\n                  :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]\n                  :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date\n                  :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])\n                  :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]\n                  :  :              :                 +- WithCTE\n                  :  :              :                    :- CTERelationDef 4, false\n                  :  :              :                    :  +- SubqueryAlias as_of_date\n                  :  :              :                    :     +- Distinct\n                  :  :              :                    :        +- Union false, false\n                  :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]\n                  :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                  :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])\n                  :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]\n                  :  :              :                    :           :           +- WithCTE\n                  :  :              :                    :           :              :- CTERelationDef 5, false\n                  :  :              :                    :           :              :  +- SubqueryAlias source_data\n                  :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]\n                  :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                  :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])\n                  :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]\n                  :  :              :                    :           :              :                 +- Distinct\n                  :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]\n                  :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)\n                  :  :              :                    :           :              :                          :- SubqueryAlias v_h\n                  :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                  :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])\n                  :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]\n                  :  :              :                    :           :              :                          :           +- WithCTE\n                  :  :              :                    :           :              :                          :              :- CTERelationDef 9, false\n                  :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :              :     +- Distinct\n                  :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)\n                  :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                  :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]\n                  :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          :                 +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          +- SubqueryAlias v_s_address\n                  :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                  :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]\n                  :  :              :                    :           :              :- CTERelationDef 6, false\n                  :  :              :                    :           :              :  +- SubqueryAlias derived_columns\n                  :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]\n                  :  :              :                    :           :              :        +- SubqueryAlias source_data\n                  :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]\n                  :  :              :                    :           :              :- CTERelationDef 7, false\n                  :  :              :                    :           :              :  +- SubqueryAlias hashed_columns\n                  :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]\n                  :  :              :                    :           :              :        +- SubqueryAlias derived_columns\n                  :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]\n                  :  :              :                    :           :              :- CTERelationDef 8, false\n                  :  :              :                    :           :              :  +- SubqueryAlias columns_to_select\n                  :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           :              :        +- SubqueryAlias hashed_columns\n                  :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]\n                  :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           :                 +- SubqueryAlias columns_to_select\n                  :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]\n                  :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                  :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])\n                  :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]\n                  :  :              :                    :                       +- WithCTE\n                  :  :              :                    :                          :- CTERelationDef 10, false\n                  :  :              :                    :                          :  +- SubqueryAlias source_data\n                  :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]\n                  :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                  :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])\n                  :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]\n                  :  :              :                    :                          :                 +- Distinct\n                  :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]\n                  :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)\n                  :  :              :                    :                          :                          :- SubqueryAlias v_h\n                  :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                  :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])\n                  :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]\n                  :  :              :                    :                          :                          :           +- WithCTE\n                  :  :              :                    :                          :                          :              :- CTERelationDef 14, false\n                  :  :              :                    :                          :                          :              :  +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :              :     +- Distinct\n                  :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]\n                  :  :              :                    :                          :                          :              :           +- Filter (1 = 1)\n                  :  :              :                    :                          :                          :              :              +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                  :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]\n                  :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :                          :                          :                 +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :                          :                          +- SubqueryAlias v_s_name\n                  :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                  :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]\n                  :  :              :                    :                          :- CTERelationDef 11, false\n                  :  :              :                    :                          :  +- SubqueryAlias derived_columns\n                  :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]\n                  :  :              :                    :                          :        +- SubqueryAlias source_data\n                  :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]\n                  :  :              :                    :                          :- CTERelationDef 12, false\n                  :  :              :                    :                          :  +- SubqueryAlias hashed_columns\n                  :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]\n                  :  :              :                    :                          :        +- SubqueryAlias derived_columns\n                  :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]\n                  :  :              :                    :                          :- CTERelationDef 13, false\n                  :  :              :                    :                          :  +- SubqueryAlias columns_to_select\n                  :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    :                          :        +- SubqueryAlias hashed_columns\n                  :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]\n                  :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    :                             +- SubqueryAlias columns_to_select\n                  :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    +- Distinct\n                  :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]\n                  :  :              :                          +- SubqueryAlias as_of_date\n                  :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]\n                  :  :              :- CTERelationDef 1, false\n                  :  :              :  +- SubqueryAlias new_rows_as_of_dates\n                  :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]\n                  :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)\n                  :  :              :           :- SubqueryAlias a\n                  :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                  :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet\n                  :  :              :           +- SubqueryAlias b\n                  :  :              :              +- SubqueryAlias as_of_dates\n                  :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]\n                  :  :              :- CTERelationDef 2, false\n                  :  :              :  +- SubqueryAlias new_rows\n                  :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]\n                  :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))\n                  :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))\n                  :  :              :           :  :- SubqueryAlias a\n                  :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                  :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]\n                  :  :              :           :  +- SubqueryAlias s_address_src\n                  :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                  :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet\n                  :  :              :           +- SubqueryAlias s_name_src\n                  :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name\n                  :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet\n                  :  :              :- CTERelationDef 3, false\n                  :  :              :  +- SubqueryAlias pit\n                  :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :              :        +- SubqueryAlias new_rows\n                  :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :              +- Distinct\n                  :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :                    +- SubqueryAlias pit\n                  :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  +- 'SubqueryAlias a\n                  :     +- 'UnresolvedRelation [s_address], [], false\n                  +- 'SubqueryAlias b\n                     +- 'UnresolvedRelation [s_name], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;\n'CreateViewCommand `ndb`.`final_table`, select \r\nhk_cli_cd,\r\nstart_date,\r\nend_date,\r\naddr,\r\nname\r\nfrom (\r\nselect distinct p.hk_cli_cd, p.as_of_date as start_date,\r\nlead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,\r\na.addr, b.name\r\nfrom ndb.pit_client p\r\nLEFT JOIN s_address a\r\n    ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from\r\nLEFT JOIN s_name b\r\n    ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd\r\norder by 1 asc\r\n), false, true, PersistedView, false\n+- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]\n   +- 'SubqueryAlias __auto_generated_subquery_name\n      +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true\n         +- 'Distinct\n            +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]\n               +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))\n                  :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))\n                  :  :- SubqueryAlias p\n                  :  :  +- SubqueryAlias spark_catalog.ndb.pit_client\n                  :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])\n                  :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]\n                  :  :           +- WithCTE\n                  :  :              :- CTERelationDef 0, false\n                  :  :              :  +- SubqueryAlias as_of_dates\n                  :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]\n                  :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date\n                  :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])\n                  :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]\n                  :  :              :                 +- WithCTE\n                  :  :              :                    :- CTERelationDef 4, false\n                  :  :              :                    :  +- SubqueryAlias as_of_date\n                  :  :              :                    :     +- Distinct\n                  :  :              :                    :        +- Union false, false\n                  :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]\n                  :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                  :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])\n                  :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]\n                  :  :              :                    :           :           +- WithCTE\n                  :  :              :                    :           :              :- CTERelationDef 5, false\n                  :  :              :                    :           :              :  +- SubqueryAlias source_data\n                  :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]\n                  :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                  :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])\n                  :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]\n                  :  :              :                    :           :              :                 +- Distinct\n                  :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]\n                  :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)\n                  :  :              :                    :           :              :                          :- SubqueryAlias v_h\n                  :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                  :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])\n                  :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]\n                  :  :              :                    :           :              :                          :           +- WithCTE\n                  :  :              :                    :           :              :                          :              :- CTERelationDef 9, false\n                  :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :              :     +- Distinct\n                  :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)\n                  :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                  :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]\n                  :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          :                 +- SubqueryAlias cli\n                  :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :           :              :                          +- SubqueryAlias v_s_address\n                  :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                  :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]\n                  :  :              :                    :           :              :- CTERelationDef 6, false\n                  :  :              :                    :           :              :  +- SubqueryAlias derived_columns\n                  :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]\n                  :  :              :                    :           :              :        +- SubqueryAlias source_data\n                  :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]\n                  :  :              :                    :           :              :- CTERelationDef 7, false\n                  :  :              :                    :           :              :  +- SubqueryAlias hashed_columns\n                  :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]\n                  :  :              :                    :           :              :        +- SubqueryAlias derived_columns\n                  :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]\n                  :  :              :                    :           :              :- CTERelationDef 8, false\n                  :  :              :                    :           :              :  +- SubqueryAlias columns_to_select\n                  :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           :              :        +- SubqueryAlias hashed_columns\n                  :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]\n                  :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           :                 +- SubqueryAlias columns_to_select\n                  :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]\n                  :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]\n                  :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                  :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])\n                  :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]\n                  :  :              :                    :                       +- WithCTE\n                  :  :              :                    :                          :- CTERelationDef 10, false\n                  :  :              :                    :                          :  +- SubqueryAlias source_data\n                  :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]\n                  :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                  :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])\n                  :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]\n                  :  :              :                    :                          :                 +- Distinct\n                  :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]\n                  :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)\n                  :  :              :                    :                          :                          :- SubqueryAlias v_h\n                  :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                  :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])\n                  :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]\n                  :  :              :                    :                          :                          :           +- WithCTE\n                  :  :              :                    :                          :                          :              :- CTERelationDef 14, false\n                  :  :              :                    :                          :                          :              :  +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :              :     +- Distinct\n                  :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]\n                  :  :              :                    :                          :                          :              :           +- Filter (1 = 1)\n                  :  :              :                    :                          :                          :              :              +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                  :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]\n                  :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :                          :                          :                 +- SubqueryAlias cli\n                  :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]\n                  :  :              :                    :                          :                          +- SubqueryAlias v_s_name\n                  :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                  :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]\n                  :  :              :                    :                          :- CTERelationDef 11, false\n                  :  :              :                    :                          :  +- SubqueryAlias derived_columns\n                  :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]\n                  :  :              :                    :                          :        +- SubqueryAlias source_data\n                  :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]\n                  :  :              :                    :                          :- CTERelationDef 12, false\n                  :  :              :                    :                          :  +- SubqueryAlias hashed_columns\n                  :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]\n                  :  :              :                    :                          :        +- SubqueryAlias derived_columns\n                  :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]\n                  :  :              :                    :                          :- CTERelationDef 13, false\n                  :  :              :                    :                          :  +- SubqueryAlias columns_to_select\n                  :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    :                          :        +- SubqueryAlias hashed_columns\n                  :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]\n                  :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    :                             +- SubqueryAlias columns_to_select\n                  :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]\n                  :  :              :                    +- Distinct\n                  :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]\n                  :  :              :                          +- SubqueryAlias as_of_date\n                  :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]\n                  :  :              :- CTERelationDef 1, false\n                  :  :              :  +- SubqueryAlias new_rows_as_of_dates\n                  :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]\n                  :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)\n                  :  :              :           :- SubqueryAlias a\n                  :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                  :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet\n                  :  :              :           +- SubqueryAlias b\n                  :  :              :              +- SubqueryAlias as_of_dates\n                  :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]\n                  :  :              :- CTERelationDef 2, false\n                  :  :              :  +- SubqueryAlias new_rows\n                  :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]\n                  :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))\n                  :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))\n                  :  :              :           :  :- SubqueryAlias a\n                  :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                  :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]\n                  :  :              :           :  +- SubqueryAlias s_address_src\n                  :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                  :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet\n                  :  :              :           +- SubqueryAlias s_name_src\n                  :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name\n                  :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet\n                  :  :              :- CTERelationDef 3, false\n                  :  :              :  +- SubqueryAlias pit\n                  :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :              :        +- SubqueryAlias new_rows\n                  :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :              +- Distinct\n                  :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  :                    +- SubqueryAlias pit\n                  :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]\n                  :  +- 'SubqueryAlias a\n                  :     +- 'UnresolvedRelation [s_address], [], false\n                  +- 'SubqueryAlias b\n                     +- 'UnresolvedRelation [s_name], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m20:31:16.699902 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m20:31:16.699902 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.final_table"} */
create or replace view ndb.final_table
  
  as
    select 
hk_cli_cd,
start_date,
end_date,
addr,
name
from (
select distinct p.hk_cli_cd, p.as_of_date as start_date,
lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
a.addr, b.name
from ndb.pit_client p
LEFT JOIN s_address a
    ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
LEFT JOIN s_name b
    ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
order by 1 asc
)

[0m20:31:16.705258 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
  'CreateViewCommand `ndb`.`final_table`, select 
  hk_cli_cd,
  start_date,
  end_date,
  addr,
  name
  from (
  select distinct p.hk_cli_cd, p.as_of_date as start_date,
  lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
  a.addr, b.name
  from ndb.pit_client p
  LEFT JOIN s_address a
      ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
  LEFT JOIN s_name b
      ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
  order by 1 asc
  ), false, true, PersistedView, false
  +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
     +- 'SubqueryAlias __auto_generated_subquery_name
        +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
           +- 'Distinct
              +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
                 +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
                    :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
                    :  :- SubqueryAlias p
                    :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
                    :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
                    :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
                    :  :           +- WithCTE
                    :  :              :- CTERelationDef 0, false
                    :  :              :  +- SubqueryAlias as_of_dates
                    :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
                    :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
                    :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
                    :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
                    :  :              :                 +- WithCTE
                    :  :              :                    :- CTERelationDef 4, false
                    :  :              :                    :  +- SubqueryAlias as_of_date
                    :  :              :                    :     +- Distinct
                    :  :              :                    :        +- Union false, false
                    :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
                    :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                    :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
                    :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
                    :  :              :                    :           :           +- WithCTE
                    :  :              :                    :           :              :- CTERelationDef 5, false
                    :  :              :                    :           :              :  +- SubqueryAlias source_data
                    :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                    :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                    :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
                    :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
                    :  :              :                    :           :              :                 +- Distinct
                    :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
                    :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
                    :  :              :                    :           :              :                          :- SubqueryAlias v_h
                    :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                    :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                    :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                    :  :              :                    :           :              :                          :           +- WithCTE
                    :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
                    :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :              :     +- Distinct
                    :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                    :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
                    :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                    :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
                    :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
                    :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                    :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
                    :  :              :                    :           :              :- CTERelationDef 6, false
                    :  :              :                    :           :              :  +- SubqueryAlias derived_columns
                    :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
                    :  :              :                    :           :              :        +- SubqueryAlias source_data
                    :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                    :  :              :                    :           :              :- CTERelationDef 7, false
                    :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
                    :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
                    :  :              :                    :           :              :        +- SubqueryAlias derived_columns
                    :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
                    :  :              :                    :           :              :- CTERelationDef 8, false
                    :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
                    :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
                    :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
                    :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           :                 +- SubqueryAlias columns_to_select
                    :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
                    :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                    :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
                    :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
                    :  :              :                    :                       +- WithCTE
                    :  :              :                    :                          :- CTERelationDef 10, false
                    :  :              :                    :                          :  +- SubqueryAlias source_data
                    :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                    :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                    :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
                    :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
                    :  :              :                    :                          :                 +- Distinct
                    :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
                    :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
                    :  :              :                    :                          :                          :- SubqueryAlias v_h
                    :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                    :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                    :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                    :  :              :                    :                          :                          :           +- WithCTE
                    :  :              :                    :                          :                          :              :- CTERelationDef 14, false
                    :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
                    :  :              :                    :                          :                          :              :     +- Distinct
                    :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                    :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
                    :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
                    :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                    :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
                    :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :                          :                          :                 +- SubqueryAlias cli
                    :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :                          :                          +- SubqueryAlias v_s_name
                    :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                    :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
                    :  :              :                    :                          :- CTERelationDef 11, false
                    :  :              :                    :                          :  +- SubqueryAlias derived_columns
                    :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
                    :  :              :                    :                          :        +- SubqueryAlias source_data
                    :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                    :  :              :                    :                          :- CTERelationDef 12, false
                    :  :              :                    :                          :  +- SubqueryAlias hashed_columns
                    :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
                    :  :              :                    :                          :        +- SubqueryAlias derived_columns
                    :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
                    :  :              :                    :                          :- CTERelationDef 13, false
                    :  :              :                    :                          :  +- SubqueryAlias columns_to_select
                    :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    :                          :        +- SubqueryAlias hashed_columns
                    :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
                    :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    :                             +- SubqueryAlias columns_to_select
                    :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    +- Distinct
                    :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
                    :  :              :                          +- SubqueryAlias as_of_date
                    :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
                    :  :              :- CTERelationDef 1, false
                    :  :              :  +- SubqueryAlias new_rows_as_of_dates
                    :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
                    :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
                    :  :              :           :- SubqueryAlias a
                    :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                    :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
                    :  :              :           +- SubqueryAlias b
                    :  :              :              +- SubqueryAlias as_of_dates
                    :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
                    :  :              :- CTERelationDef 2, false
                    :  :              :  +- SubqueryAlias new_rows
                    :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
                    :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
                    :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
                    :  :              :           :  :- SubqueryAlias a
                    :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
                    :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
                    :  :              :           :  +- SubqueryAlias s_address_src
                    :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                    :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
                    :  :              :           +- SubqueryAlias s_name_src
                    :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
                    :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
                    :  :              :- CTERelationDef 3, false
                    :  :              :  +- SubqueryAlias pit
                    :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :              :        +- SubqueryAlias new_rows
                    :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :              +- Distinct
                    :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :                    +- SubqueryAlias pit
                    :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  +- 'SubqueryAlias a
                    :     +- 'UnresolvedRelation [s_address], [], false
                    +- 'SubqueryAlias b
                       +- 'UnresolvedRelation [s_name], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
  'CreateViewCommand `ndb`.`final_table`, select 
  hk_cli_cd,
  start_date,
  end_date,
  addr,
  name
  from (
  select distinct p.hk_cli_cd, p.as_of_date as start_date,
  lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
  a.addr, b.name
  from ndb.pit_client p
  LEFT JOIN s_address a
      ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
  LEFT JOIN s_name b
      ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
  order by 1 asc
  ), false, true, PersistedView, false
  +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
     +- 'SubqueryAlias __auto_generated_subquery_name
        +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
           +- 'Distinct
              +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
                 +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
                    :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
                    :  :- SubqueryAlias p
                    :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
                    :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
                    :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
                    :  :           +- WithCTE
                    :  :              :- CTERelationDef 0, false
                    :  :              :  +- SubqueryAlias as_of_dates
                    :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
                    :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
                    :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
                    :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
                    :  :              :                 +- WithCTE
                    :  :              :                    :- CTERelationDef 4, false
                    :  :              :                    :  +- SubqueryAlias as_of_date
                    :  :              :                    :     +- Distinct
                    :  :              :                    :        +- Union false, false
                    :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
                    :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                    :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
                    :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
                    :  :              :                    :           :           +- WithCTE
                    :  :              :                    :           :              :- CTERelationDef 5, false
                    :  :              :                    :           :              :  +- SubqueryAlias source_data
                    :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                    :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                    :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
                    :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
                    :  :              :                    :           :              :                 +- Distinct
                    :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
                    :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
                    :  :              :                    :           :              :                          :- SubqueryAlias v_h
                    :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                    :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                    :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                    :  :              :                    :           :              :                          :           +- WithCTE
                    :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
                    :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :              :     +- Distinct
                    :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                    :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
                    :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                    :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
                    :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
                    :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
                    :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                    :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
                    :  :              :                    :           :              :- CTERelationDef 6, false
                    :  :              :                    :           :              :  +- SubqueryAlias derived_columns
                    :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
                    :  :              :                    :           :              :        +- SubqueryAlias source_data
                    :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                    :  :              :                    :           :              :- CTERelationDef 7, false
                    :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
                    :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
                    :  :              :                    :           :              :        +- SubqueryAlias derived_columns
                    :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
                    :  :              :                    :           :              :- CTERelationDef 8, false
                    :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
                    :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
                    :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
                    :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           :                 +- SubqueryAlias columns_to_select
                    :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                    :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
                    :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                    :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
                    :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
                    :  :              :                    :                       +- WithCTE
                    :  :              :                    :                          :- CTERelationDef 10, false
                    :  :              :                    :                          :  +- SubqueryAlias source_data
                    :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                    :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                    :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
                    :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
                    :  :              :                    :                          :                 +- Distinct
                    :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
                    :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
                    :  :              :                    :                          :                          :- SubqueryAlias v_h
                    :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                    :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                    :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                    :  :              :                    :                          :                          :           +- WithCTE
                    :  :              :                    :                          :                          :              :- CTERelationDef 14, false
                    :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
                    :  :              :                    :                          :                          :              :     +- Distinct
                    :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                    :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
                    :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
                    :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                    :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
                    :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :                          :                          :                 +- SubqueryAlias cli
                    :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
                    :  :              :                    :                          :                          +- SubqueryAlias v_s_name
                    :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                    :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
                    :  :              :                    :                          :- CTERelationDef 11, false
                    :  :              :                    :                          :  +- SubqueryAlias derived_columns
                    :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
                    :  :              :                    :                          :        +- SubqueryAlias source_data
                    :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                    :  :              :                    :                          :- CTERelationDef 12, false
                    :  :              :                    :                          :  +- SubqueryAlias hashed_columns
                    :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
                    :  :              :                    :                          :        +- SubqueryAlias derived_columns
                    :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
                    :  :              :                    :                          :- CTERelationDef 13, false
                    :  :              :                    :                          :  +- SubqueryAlias columns_to_select
                    :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    :                          :        +- SubqueryAlias hashed_columns
                    :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
                    :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    :                             +- SubqueryAlias columns_to_select
                    :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                    :  :              :                    +- Distinct
                    :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
                    :  :              :                          +- SubqueryAlias as_of_date
                    :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
                    :  :              :- CTERelationDef 1, false
                    :  :              :  +- SubqueryAlias new_rows_as_of_dates
                    :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
                    :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
                    :  :              :           :- SubqueryAlias a
                    :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                    :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
                    :  :              :           +- SubqueryAlias b
                    :  :              :              +- SubqueryAlias as_of_dates
                    :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
                    :  :              :- CTERelationDef 2, false
                    :  :              :  +- SubqueryAlias new_rows
                    :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
                    :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
                    :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
                    :  :              :           :  :- SubqueryAlias a
                    :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
                    :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
                    :  :              :           :  +- SubqueryAlias s_address_src
                    :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                    :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
                    :  :              :           +- SubqueryAlias s_name_src
                    :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
                    :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
                    :  :              :- CTERelationDef 3, false
                    :  :              :  +- SubqueryAlias pit
                    :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :              :        +- SubqueryAlias new_rows
                    :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :              +- Distinct
                    :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  :                    +- SubqueryAlias pit
                    :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                    :  +- 'SubqueryAlias a
                    :     +- 'UnresolvedRelation [s_address], [], false
                    +- 'SubqueryAlias b
                       +- 'UnresolvedRelation [s_name], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m20:31:16.713274 [debug] [Thread-1 (]: Timing info for model.poc_demo.final_table (execute): 20:31:15.023321 => 20:31:16.705258
[0m20:31:16.713274 [debug] [Thread-1 (]: On model.poc_demo.final_table: ROLLBACK
[0m20:31:16.713274 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:31:16.715299 [debug] [Thread-1 (]: On model.poc_demo.final_table: Close
[0m20:31:16.843484 [debug] [Thread-1 (]: Runtime Error in model final_table (models\pit_test\pit_test\final_table.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
    'CreateViewCommand `ndb`.`final_table`, select 
    hk_cli_cd,
    start_date,
    end_date,
    addr,
    name
    from (
    select distinct p.hk_cli_cd, p.as_of_date as start_date,
    lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
    a.addr, b.name
    from ndb.pit_client p
    LEFT JOIN s_address a
        ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
    LEFT JOIN s_name b
        ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
    order by 1 asc
    ), false, true, PersistedView, false
    +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
             +- 'Distinct
                +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
                   +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
                      :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
                      :  :- SubqueryAlias p
                      :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
                      :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
                      :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
                      :  :           +- WithCTE
                      :  :              :- CTERelationDef 0, false
                      :  :              :  +- SubqueryAlias as_of_dates
                      :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
                      :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
                      :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
                      :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
                      :  :              :                 +- WithCTE
                      :  :              :                    :- CTERelationDef 4, false
                      :  :              :                    :  +- SubqueryAlias as_of_date
                      :  :              :                    :     +- Distinct
                      :  :              :                    :        +- Union false, false
                      :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
                      :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                      :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
                      :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
                      :  :              :                    :           :           +- WithCTE
                      :  :              :                    :           :              :- CTERelationDef 5, false
                      :  :              :                    :           :              :  +- SubqueryAlias source_data
                      :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                      :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                      :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
                      :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
                      :  :              :                    :           :              :                 +- Distinct
                      :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
                      :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
                      :  :              :                    :           :              :                          :- SubqueryAlias v_h
                      :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                      :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                      :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                      :  :              :                    :           :              :                          :           +- WithCTE
                      :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
                      :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :              :     +- Distinct
                      :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                      :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
                      :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                      :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
                      :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
                      :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                      :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
                      :  :              :                    :           :              :- CTERelationDef 6, false
                      :  :              :                    :           :              :  +- SubqueryAlias derived_columns
                      :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
                      :  :              :                    :           :              :        +- SubqueryAlias source_data
                      :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                      :  :              :                    :           :              :- CTERelationDef 7, false
                      :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
                      :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
                      :  :              :                    :           :              :        +- SubqueryAlias derived_columns
                      :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
                      :  :              :                    :           :              :- CTERelationDef 8, false
                      :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
                      :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
                      :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
                      :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           :                 +- SubqueryAlias columns_to_select
                      :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
                      :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                      :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
                      :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
                      :  :              :                    :                       +- WithCTE
                      :  :              :                    :                          :- CTERelationDef 10, false
                      :  :              :                    :                          :  +- SubqueryAlias source_data
                      :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                      :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                      :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
                      :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
                      :  :              :                    :                          :                 +- Distinct
                      :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
                      :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
                      :  :              :                    :                          :                          :- SubqueryAlias v_h
                      :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                      :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                      :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                      :  :              :                    :                          :                          :           +- WithCTE
                      :  :              :                    :                          :                          :              :- CTERelationDef 14, false
                      :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
                      :  :              :                    :                          :                          :              :     +- Distinct
                      :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                      :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
                      :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
                      :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                      :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
                      :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :                          :                          :                 +- SubqueryAlias cli
                      :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :                          :                          +- SubqueryAlias v_s_name
                      :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                      :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
                      :  :              :                    :                          :- CTERelationDef 11, false
                      :  :              :                    :                          :  +- SubqueryAlias derived_columns
                      :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
                      :  :              :                    :                          :        +- SubqueryAlias source_data
                      :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                      :  :              :                    :                          :- CTERelationDef 12, false
                      :  :              :                    :                          :  +- SubqueryAlias hashed_columns
                      :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
                      :  :              :                    :                          :        +- SubqueryAlias derived_columns
                      :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
                      :  :              :                    :                          :- CTERelationDef 13, false
                      :  :              :                    :                          :  +- SubqueryAlias columns_to_select
                      :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    :                          :        +- SubqueryAlias hashed_columns
                      :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
                      :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    :                             +- SubqueryAlias columns_to_select
                      :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    +- Distinct
                      :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
                      :  :              :                          +- SubqueryAlias as_of_date
                      :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
                      :  :              :- CTERelationDef 1, false
                      :  :              :  +- SubqueryAlias new_rows_as_of_dates
                      :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
                      :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
                      :  :              :           :- SubqueryAlias a
                      :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                      :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
                      :  :              :           +- SubqueryAlias b
                      :  :              :              +- SubqueryAlias as_of_dates
                      :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
                      :  :              :- CTERelationDef 2, false
                      :  :              :  +- SubqueryAlias new_rows
                      :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
                      :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
                      :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
                      :  :              :           :  :- SubqueryAlias a
                      :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
                      :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
                      :  :              :           :  +- SubqueryAlias s_address_src
                      :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                      :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
                      :  :              :           +- SubqueryAlias s_name_src
                      :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
                      :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
                      :  :              :- CTERelationDef 3, false
                      :  :              :  +- SubqueryAlias pit
                      :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :              :        +- SubqueryAlias new_rows
                      :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :              +- Distinct
                      :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :                    +- SubqueryAlias pit
                      :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  +- 'SubqueryAlias a
                      :     +- 'UnresolvedRelation [s_address], [], false
                      +- 'SubqueryAlias b
                         +- 'UnresolvedRelation [s_name], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
    'CreateViewCommand `ndb`.`final_table`, select 
    hk_cli_cd,
    start_date,
    end_date,
    addr,
    name
    from (
    select distinct p.hk_cli_cd, p.as_of_date as start_date,
    lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
    a.addr, b.name
    from ndb.pit_client p
    LEFT JOIN s_address a
        ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
    LEFT JOIN s_name b
        ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
    order by 1 asc
    ), false, true, PersistedView, false
    +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
             +- 'Distinct
                +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
                   +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
                      :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
                      :  :- SubqueryAlias p
                      :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
                      :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
                      :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
                      :  :           +- WithCTE
                      :  :              :- CTERelationDef 0, false
                      :  :              :  +- SubqueryAlias as_of_dates
                      :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
                      :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
                      :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
                      :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
                      :  :              :                 +- WithCTE
                      :  :              :                    :- CTERelationDef 4, false
                      :  :              :                    :  +- SubqueryAlias as_of_date
                      :  :              :                    :     +- Distinct
                      :  :              :                    :        +- Union false, false
                      :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
                      :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                      :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
                      :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
                      :  :              :                    :           :           +- WithCTE
                      :  :              :                    :           :              :- CTERelationDef 5, false
                      :  :              :                    :           :              :  +- SubqueryAlias source_data
                      :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                      :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                      :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
                      :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
                      :  :              :                    :           :              :                 +- Distinct
                      :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
                      :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
                      :  :              :                    :           :              :                          :- SubqueryAlias v_h
                      :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                      :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                      :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                      :  :              :                    :           :              :                          :           +- WithCTE
                      :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
                      :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :              :     +- Distinct
                      :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                      :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
                      :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                      :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
                      :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
                      :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
                      :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                      :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
                      :  :              :                    :           :              :- CTERelationDef 6, false
                      :  :              :                    :           :              :  +- SubqueryAlias derived_columns
                      :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
                      :  :              :                    :           :              :        +- SubqueryAlias source_data
                      :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
                      :  :              :                    :           :              :- CTERelationDef 7, false
                      :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
                      :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
                      :  :              :                    :           :              :        +- SubqueryAlias derived_columns
                      :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
                      :  :              :                    :           :              :- CTERelationDef 8, false
                      :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
                      :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
                      :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
                      :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           :                 +- SubqueryAlias columns_to_select
                      :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
                      :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
                      :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                      :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
                      :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
                      :  :              :                    :                       +- WithCTE
                      :  :              :                    :                          :- CTERelationDef 10, false
                      :  :              :                    :                          :  +- SubqueryAlias source_data
                      :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                      :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                      :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
                      :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
                      :  :              :                    :                          :                 +- Distinct
                      :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
                      :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
                      :  :              :                    :                          :                          :- SubqueryAlias v_h
                      :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                      :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
                      :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
                      :  :              :                    :                          :                          :           +- WithCTE
                      :  :              :                    :                          :                          :              :- CTERelationDef 14, false
                      :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
                      :  :              :                    :                          :                          :              :     +- Distinct
                      :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
                      :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
                      :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
                      :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                      :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
                      :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :                          :                          :                 +- SubqueryAlias cli
                      :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
                      :  :              :                    :                          :                          +- SubqueryAlias v_s_name
                      :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                      :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
                      :  :              :                    :                          :- CTERelationDef 11, false
                      :  :              :                    :                          :  +- SubqueryAlias derived_columns
                      :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
                      :  :              :                    :                          :        +- SubqueryAlias source_data
                      :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
                      :  :              :                    :                          :- CTERelationDef 12, false
                      :  :              :                    :                          :  +- SubqueryAlias hashed_columns
                      :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
                      :  :              :                    :                          :        +- SubqueryAlias derived_columns
                      :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
                      :  :              :                    :                          :- CTERelationDef 13, false
                      :  :              :                    :                          :  +- SubqueryAlias columns_to_select
                      :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    :                          :        +- SubqueryAlias hashed_columns
                      :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
                      :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    :                             +- SubqueryAlias columns_to_select
                      :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
                      :  :              :                    +- Distinct
                      :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
                      :  :              :                          +- SubqueryAlias as_of_date
                      :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
                      :  :              :- CTERelationDef 1, false
                      :  :              :  +- SubqueryAlias new_rows_as_of_dates
                      :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
                      :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
                      :  :              :           :- SubqueryAlias a
                      :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                      :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
                      :  :              :           +- SubqueryAlias b
                      :  :              :              +- SubqueryAlias as_of_dates
                      :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
                      :  :              :- CTERelationDef 2, false
                      :  :              :  +- SubqueryAlias new_rows
                      :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
                      :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
                      :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
                      :  :              :           :  :- SubqueryAlias a
                      :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
                      :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
                      :  :              :           :  +- SubqueryAlias s_address_src
                      :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                      :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
                      :  :              :           +- SubqueryAlias s_name_src
                      :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
                      :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
                      :  :              :- CTERelationDef 3, false
                      :  :              :  +- SubqueryAlias pit
                      :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :              :        +- SubqueryAlias new_rows
                      :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :              +- Distinct
                      :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  :                    +- SubqueryAlias pit
                      :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
                      :  +- 'SubqueryAlias a
                      :     +- 'UnresolvedRelation [s_address], [], false
                      +- 'SubqueryAlias b
                         +- 'UnresolvedRelation [s_name], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m20:31:16.843484 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddf013be-fb67-4b79-b7b5-4a100a7ba834', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED9B3F7F0>]}
[0m20:31:16.843484 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.final_table ........................... [[31mERROR[0m in 1.83s]
[0m20:31:16.843484 [debug] [Thread-1 (]: Finished running node model.poc_demo.final_table
[0m20:31:16.843484 [debug] [MainThread]: On master: ROLLBACK
[0m20:31:16.843484 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:31:16.923586 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:31:16.923586 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:31:16.923586 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:31:16.923586 [debug] [MainThread]: On master: ROLLBACK
[0m20:31:16.923586 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:31:16.923586 [debug] [MainThread]: On master: Close
[0m20:31:16.939785 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:31:16.939785 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m20:31:16.943295 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m20:31:16.943295 [debug] [MainThread]: Connection 'model.poc_demo.final_table' was properly closed.
[0m20:31:16.943295 [info ] [MainThread]: 
[0m20:31:16.946820 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.71 seconds (3.71s).
[0m20:31:16.946820 [debug] [MainThread]: Command end result
[0m20:31:16.963520 [info ] [MainThread]: 
[0m20:31:16.970282 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:31:16.971741 [info ] [MainThread]: 
[0m20:31:16.973273 [error] [MainThread]: [33mRuntime Error in model final_table (models\pit_test\pit_test\final_table.sql)[0m
[0m20:31:16.975014 [error] [MainThread]:   Database Error
[0m20:31:16.976021 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
[0m20:31:16.979041 [error] [MainThread]:     'CreateViewCommand `ndb`.`final_table`, select 
[0m20:31:16.979041 [error] [MainThread]:     hk_cli_cd,
[0m20:31:16.979041 [error] [MainThread]:     start_date,
[0m20:31:16.979041 [error] [MainThread]:     end_date,
[0m20:31:16.979041 [error] [MainThread]:     addr,
[0m20:31:16.983556 [error] [MainThread]:     name
[0m20:31:16.983556 [error] [MainThread]:     from (
[0m20:31:16.983556 [error] [MainThread]:     select distinct p.hk_cli_cd, p.as_of_date as start_date,
[0m20:31:16.986084 [error] [MainThread]:     lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
[0m20:31:16.986914 [error] [MainThread]:     a.addr, b.name
[0m20:31:16.986914 [error] [MainThread]:     from ndb.pit_client p
[0m20:31:16.986914 [error] [MainThread]:     LEFT JOIN s_address a
[0m20:31:16.986914 [error] [MainThread]:         ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
[0m20:31:16.986914 [error] [MainThread]:     LEFT JOIN s_name b
[0m20:31:16.993426 [error] [MainThread]:         ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
[0m20:31:16.993426 [error] [MainThread]:     order by 1 asc
[0m20:31:16.994444 [error] [MainThread]:     ), false, true, PersistedView, false
[0m20:31:16.995453 [error] [MainThread]:     +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
[0m20:31:16.995453 [error] [MainThread]:        +- 'SubqueryAlias __auto_generated_subquery_name
[0m20:31:16.997760 [error] [MainThread]:           +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
[0m20:31:16.998768 [error] [MainThread]:              +- 'Distinct
[0m20:31:16.998768 [error] [MainThread]:                 +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
[0m20:31:17.001135 [error] [MainThread]:                    +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
[0m20:31:17.001135 [error] [MainThread]:                       :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
[0m20:31:17.004511 [error] [MainThread]:                       :  :- SubqueryAlias p
[0m20:31:17.004511 [error] [MainThread]:                       :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
[0m20:31:17.004511 [error] [MainThread]:                       :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
[0m20:31:17.004511 [error] [MainThread]:                       :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
[0m20:31:17.004511 [error] [MainThread]:                       :  :           +- WithCTE
[0m20:31:17.004511 [error] [MainThread]:                       :  :              :- CTERelationDef 0, false
[0m20:31:17.004511 [error] [MainThread]:                       :  :              :  +- SubqueryAlias as_of_dates
[0m20:31:17.012211 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
[0m20:31:17.013218 [error] [MainThread]:                       :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :                 +- WithCTE
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :                    :- CTERelationDef 4, false
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :                    :  +- SubqueryAlias as_of_date
[0m20:31:17.015330 [error] [MainThread]:                       :  :              :                    :     +- Distinct
[0m20:31:17.020235 [error] [MainThread]:                       :  :              :                    :        +- Union false, false
[0m20:31:17.020235 [error] [MainThread]:                       :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
[0m20:31:17.023247 [error] [MainThread]:                       :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m20:31:17.023247 [error] [MainThread]:                       :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
[0m20:31:17.023247 [error] [MainThread]:                       :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
[0m20:31:17.023247 [error] [MainThread]:                       :  :              :                    :           :           +- WithCTE
[0m20:31:17.028276 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 5, false
[0m20:31:17.028840 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias source_data
[0m20:31:17.028840 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
[0m20:31:17.028840 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m20:31:17.033353 [error] [MainThread]:                       :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
[0m20:31:17.033877 [error] [MainThread]:                       :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
[0m20:31:17.033877 [error] [MainThread]:                       :  :              :                    :           :              :                 +- Distinct
[0m20:31:17.036898 [error] [MainThread]:                       :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
[0m20:31:17.036898 [error] [MainThread]:                       :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
[0m20:31:17.036898 [error] [MainThread]:                       :  :              :                    :           :              :                          :- SubqueryAlias v_h
[0m20:31:17.036898 [error] [MainThread]:                       :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m20:31:17.036898 [error] [MainThread]:                       :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
[0m20:31:17.043408 [error] [MainThread]:                       :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
[0m20:31:17.044937 [error] [MainThread]:                       :  :              :                    :           :              :                          :           +- WithCTE
[0m20:31:17.046210 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
[0m20:31:17.046210 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
[0m20:31:17.046210 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :     +- Distinct
[0m20:31:17.051720 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
[0m20:31:17.053596 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
[0m20:31:17.053596 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
[0m20:31:17.055183 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m20:31:17.055183 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
[0m20:31:17.055183 [error] [MainThread]:                       :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.055183 [error] [MainThread]:                       :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
[0m20:31:17.062095 [error] [MainThread]:                       :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.063496 [error] [MainThread]:                       :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
[0m20:31:17.063496 [error] [MainThread]:                       :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m20:31:17.063496 [error] [MainThread]:                       :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
[0m20:31:17.063496 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 6, false
[0m20:31:17.063496 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias derived_columns
[0m20:31:17.070025 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
[0m20:31:17.070277 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias source_data
[0m20:31:17.070277 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
[0m20:31:17.073353 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 7, false
[0m20:31:17.073860 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
[0m20:31:17.073860 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
[0m20:31:17.073860 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias derived_columns
[0m20:31:17.078386 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
[0m20:31:17.079386 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 8, false
[0m20:31:17.079386 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
[0m20:31:17.079386 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.083395 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
[0m20:31:17.083395 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
[0m20:31:17.083395 [error] [MainThread]:                       :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.087258 [error] [MainThread]:                       :  :              :                    :           :                 +- SubqueryAlias columns_to_select
[0m20:31:17.089795 [error] [MainThread]:                       :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.089795 [error] [MainThread]:                       :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
[0m20:31:17.093325 [error] [MainThread]:                       :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m20:31:17.093325 [error] [MainThread]:                       :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
[0m20:31:17.095355 [error] [MainThread]:                       :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                       +- WithCTE
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 10, false
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias source_data
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m20:31:17.096402 [error] [MainThread]:                       :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
[0m20:31:17.103641 [error] [MainThread]:                       :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
[0m20:31:17.103641 [error] [MainThread]:                       :  :              :                    :                          :                 +- Distinct
[0m20:31:17.103641 [error] [MainThread]:                       :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
[0m20:31:17.107339 [error] [MainThread]:                       :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
[0m20:31:17.107339 [error] [MainThread]:                       :  :              :                    :                          :                          :- SubqueryAlias v_h
[0m20:31:17.109844 [error] [MainThread]:                       :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m20:31:17.109844 [error] [MainThread]:                       :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
[0m20:31:17.111860 [error] [MainThread]:                       :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
[0m20:31:17.113025 [error] [MainThread]:                       :  :              :                    :                          :                          :           +- WithCTE
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :- CTERelationDef 14, false
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :     +- Distinct
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
[0m20:31:17.113533 [error] [MainThread]:                       :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m20:31:17.120387 [error] [MainThread]:                       :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
[0m20:31:17.120387 [error] [MainThread]:                       :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.123476 [error] [MainThread]:                       :  :              :                    :                          :                          :                 +- SubqueryAlias cli
[0m20:31:17.123476 [error] [MainThread]:                       :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.123476 [error] [MainThread]:                       :  :              :                    :                          :                          +- SubqueryAlias v_s_name
[0m20:31:17.126485 [error] [MainThread]:                       :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m20:31:17.126485 [error] [MainThread]:                       :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
[0m20:31:17.128493 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 11, false
[0m20:31:17.128493 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias derived_columns
[0m20:31:17.128493 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
[0m20:31:17.130818 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias source_data
[0m20:31:17.130818 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
[0m20:31:17.133915 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 12, false
[0m20:31:17.133915 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias hashed_columns
[0m20:31:17.133915 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
[0m20:31:17.133915 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias derived_columns
[0m20:31:17.136990 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
[0m20:31:17.140431 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 13, false
[0m20:31:17.140431 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias columns_to_select
[0m20:31:17.140431 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.143441 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias hashed_columns
[0m20:31:17.143441 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
[0m20:31:17.145463 [error] [MainThread]:                       :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                    :                             +- SubqueryAlias columns_to_select
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                    +- Distinct
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                          +- SubqueryAlias as_of_date
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
[0m20:31:17.146811 [error] [MainThread]:                       :  :              :- CTERelationDef 1, false
[0m20:31:17.153321 [error] [MainThread]:                       :  :              :  +- SubqueryAlias new_rows_as_of_dates
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :           :- SubqueryAlias a
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :           +- SubqueryAlias b
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :              +- SubqueryAlias as_of_dates
[0m20:31:17.153581 [error] [MainThread]:                       :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
[0m20:31:17.161789 [error] [MainThread]:                       :  :              :- CTERelationDef 2, false
[0m20:31:17.186920 [error] [MainThread]:                       :  :              :  +- SubqueryAlias new_rows
[0m20:31:17.195018 [error] [MainThread]:                       :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
[0m20:31:17.195018 [error] [MainThread]:                       :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :  :- SubqueryAlias a
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :  +- SubqueryAlias s_address_src
[0m20:31:17.197554 [error] [MainThread]:                       :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m20:31:17.203621 [error] [MainThread]:                       :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
[0m20:31:17.203621 [error] [MainThread]:                       :  :              :           +- SubqueryAlias s_name_src
[0m20:31:17.210424 [error] [MainThread]:                       :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m20:31:17.210424 [error] [MainThread]:                       :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
[0m20:31:17.213436 [error] [MainThread]:                       :  :              :- CTERelationDef 3, false
[0m20:31:17.213436 [error] [MainThread]:                       :  :              :  +- SubqueryAlias pit
[0m20:31:17.213436 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.213436 [error] [MainThread]:                       :  :              :        +- SubqueryAlias new_rows
[0m20:31:17.218468 [error] [MainThread]:                       :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.219476 [error] [MainThread]:                       :  :              +- Distinct
[0m20:31:17.220249 [error] [MainThread]:                       :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.220249 [error] [MainThread]:                       :  :                    +- SubqueryAlias pit
[0m20:31:17.223259 [error] [MainThread]:                       :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.223259 [error] [MainThread]:                       :  +- 'SubqueryAlias a
[0m20:31:17.223259 [error] [MainThread]:                       :     +- 'UnresolvedRelation [s_address], [], false
[0m20:31:17.227801 [error] [MainThread]:                       +- 'SubqueryAlias b
[0m20:31:17.227801 [error] [MainThread]:                          +- 'UnresolvedRelation [s_name], [], false
[0m20:31:17.227801 [error] [MainThread]:     
[0m20:31:17.227801 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m20:31:17.231036 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m20:31:17.234070 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m20:31:17.234070 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m20:31:17.234070 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m20:31:17.236922 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m20:31:17.236922 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m20:31:17.236922 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m20:31:17.236922 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m20:31:17.236922 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m20:31:17.244185 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m20:31:17.245193 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m20:31:17.246189 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m20:31:17.247458 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m20:31:17.248440 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m20:31:17.249448 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m20:31:17.249448 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m20:31:17.249448 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m20:31:17.253581 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: s_address; line 16 pos 10;
[0m20:31:17.253581 [error] [MainThread]:     'CreateViewCommand `ndb`.`final_table`, select 
[0m20:31:17.253581 [error] [MainThread]:     hk_cli_cd,
[0m20:31:17.253581 [error] [MainThread]:     start_date,
[0m20:31:17.258590 [error] [MainThread]:     end_date,
[0m20:31:17.258590 [error] [MainThread]:     addr,
[0m20:31:17.258590 [error] [MainThread]:     name
[0m20:31:17.261600 [error] [MainThread]:     from (
[0m20:31:17.261600 [error] [MainThread]:     select distinct p.hk_cli_cd, p.as_of_date as start_date,
[0m20:31:17.263864 [error] [MainThread]:     lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
[0m20:31:17.263864 [error] [MainThread]:     a.addr, b.name
[0m20:31:17.263864 [error] [MainThread]:     from ndb.pit_client p
[0m20:31:17.263864 [error] [MainThread]:     LEFT JOIN s_address a
[0m20:31:17.263864 [error] [MainThread]:         ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
[0m20:31:17.263864 [error] [MainThread]:     LEFT JOIN s_name b
[0m20:31:17.263864 [error] [MainThread]:         ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
[0m20:31:17.270242 [error] [MainThread]:     order by 1 asc
[0m20:31:17.270242 [error] [MainThread]:     ), false, true, PersistedView, false
[0m20:31:17.273367 [error] [MainThread]:     +- 'Project ['hk_cli_cd, 'start_date, 'end_date, 'addr, 'name]
[0m20:31:17.273367 [error] [MainThread]:        +- 'SubqueryAlias __auto_generated_subquery_name
[0m20:31:17.273367 [error] [MainThread]:           +- 'Sort [unresolvedordinal(1) ASC NULLS FIRST], true
[0m20:31:17.273367 [error] [MainThread]:              +- 'Distinct
[0m20:31:17.273367 [error] [MainThread]:                 +- 'Project ['p.hk_cli_cd, 'p.as_of_date AS start_date#131, 'lead('p.as_of_date) windowspecdefinition('p.hk_cli_cd, 'p.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#132, 'a.addr, 'b.name]
[0m20:31:17.278374 [error] [MainThread]:                    +- 'Join LeftOuter, (('p.s_name_ldts = 'b.effective_from) AND ('p.hk_cli_cd = 'b.hsh_ky_cli_cd))
[0m20:31:17.278374 [error] [MainThread]:                       :- 'Join LeftOuter, (('p.s_address_pk = 'a.hsh_ky_cli_cd) AND ('p.s_address_ldts = 'a.effective_from))
[0m20:31:17.280865 [error] [MainThread]:                       :  :- SubqueryAlias p
[0m20:31:17.280865 [error] [MainThread]:                       :  :  +- SubqueryAlias spark_catalog.ndb.pit_client
[0m20:31:17.283373 [error] [MainThread]:                       :  :     +- View (`ndb`.`pit_client`, [hk_cli_cd#137,AS_OF_DATE#138,S_ADDRESS_PK#139,S_ADDRESS_LDTS#140,S_NAME_PK#141,S_NAME_LDTS#142])
[0m20:31:17.283373 [error] [MainThread]:                       :  :        +- Project [cast(hk_cli_cd#198 as string) AS hk_cli_cd#137, cast(AS_OF_DATE#146 as timestamp) AS AS_OF_DATE#138, cast(S_ADDRESS_PK#133 as string) AS S_ADDRESS_PK#139, cast(S_ADDRESS_LDTS#134 as timestamp) AS S_ADDRESS_LDTS#140, cast(S_NAME_PK#135 as string) AS S_NAME_PK#141, cast(S_NAME_LDTS#136 as timestamp) AS S_NAME_LDTS#142]
[0m20:31:17.283373 [error] [MainThread]:                       :  :           +- WithCTE
[0m20:31:17.286925 [error] [MainThread]:                       :  :              :- CTERelationDef 0, false
[0m20:31:17.287842 [error] [MainThread]:                       :  :              :  +- SubqueryAlias as_of_dates
[0m20:31:17.287842 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#145, AS_OF_DATE#146]
[0m20:31:17.287842 [error] [MainThread]:                       :  :              :        +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m20:31:17.287842 [error] [MainThread]:                       :  :              :           +- View (`ndb`.`as_of_date`, [hk_cli_cd#145,AS_OF_DATE#146])
[0m20:31:17.287842 [error] [MainThread]:                       :  :              :              +- Project [cast(hk_cli_cd#143 as string) AS hk_cli_cd#145, cast(AS_OF_DATE#144 as timestamp) AS AS_OF_DATE#146]
[0m20:31:17.293357 [error] [MainThread]:                       :  :              :                 +- WithCTE
[0m20:31:17.293357 [error] [MainThread]:                       :  :              :                    :- CTERelationDef 4, false
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :  +- SubqueryAlias as_of_date
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :     +- Distinct
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :        +- Union false, false
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :           :- Project [hsh_ky_cli_cd#157, ts#154]
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#151,RCRD_SRC_NM#152,ADDR#153,TS#154,LD_DT_TM#155,EFFECTIVE_FROM#156,HSH_KY_CLI_CD#157,RCRD_HSH_ID#158])
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :           :        +- Project [cast(CLI_ID#159 as string) AS CLI_ID#151, cast(RCRD_SRC_NM#160 as string) AS RCRD_SRC_NM#152, cast(ADDR#161 as string) AS ADDR#153, cast(TS#162 as timestamp) AS TS#154, cast(LD_DT_TM#147 as timestamp) AS LD_DT_TM#155, cast(EFFECTIVE_FROM#148 as timestamp) AS EFFECTIVE_FROM#156, cast(HSH_KY_CLI_CD#149 as string) AS HSH_KY_CLI_CD#157, cast(RCRD_HSH_ID#150 as string) AS RCRD_HSH_ID#158]
[0m20:31:17.296054 [error] [MainThread]:                       :  :              :                    :           :           +- WithCTE
[0m20:31:17.303741 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 5, false
[0m20:31:17.303741 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias source_data
[0m20:31:17.303741 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
[0m20:31:17.303741 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m20:31:17.303741 [error] [MainThread]:                       :  :              :                    :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#159,rcrd_src_nm#160,addr#161,ts#162])
[0m20:31:17.308256 [error] [MainThread]:                       :  :              :                    :           :              :              +- Project [cast(cli_id#165 as string) AS cli_id#159, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#160, cast(addr#169 as string) AS addr#161, cast(ts#170 as timestamp) AS ts#162]
[0m20:31:17.308256 [error] [MainThread]:                       :  :              :                    :           :              :                 +- Distinct
[0m20:31:17.308256 [error] [MainThread]:                       :  :              :                    :           :              :                    +- Project [cli_id#165, rcrd_src_nm#166, addr#169, ts#170]
[0m20:31:17.308256 [error] [MainThread]:                       :  :              :                    :           :              :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#168)
[0m20:31:17.312139 [error] [MainThread]:                       :  :              :                    :           :              :                          :- SubqueryAlias v_h
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :           +- WithCTE
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :- CTERelationDef 9, false
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :  +- SubqueryAlias cli
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :     +- Distinct
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :        +- Project [trim(cast(cli_id#167 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :           +- Filter (1 = 1)
[0m20:31:17.313658 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :              +- SubqueryAlias cli
[0m20:31:17.323365 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m20:31:17.323365 [error] [MainThread]:                       :  :              :                    :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#167], Partition Cols: []]
[0m20:31:17.323365 [error] [MainThread]:                       :  :              :                    :           :              :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.323365 [error] [MainThread]:                       :  :              :                    :           :              :                          :                 +- SubqueryAlias cli
[0m20:31:17.323365 [error] [MainThread]:                       :  :              :                    :           :              :                          :                    +- CTERelationRef 9, true, [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.328628 [error] [MainThread]:                       :  :              :                    :           :              :                          +- SubqueryAlias v_s_address
[0m20:31:17.328628 [error] [MainThread]:                       :  :              :                    :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m20:31:17.328628 [error] [MainThread]:                       :  :              :                    :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#168, addr#169, ts#170], Partition Cols: []]
[0m20:31:17.328628 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 6, false
[0m20:31:17.328628 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias derived_columns
[0m20:31:17.333646 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, current_timestamp() AS ld_dt_tm#147, ts#162 AS EFFECTIVE_FROM#148]
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias source_data
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 5, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162]
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 7, false
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias hashed_columns
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, cast(md5(cast(nullif(upper(trim(cast(cli_id#159 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#149, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#161 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#150]
[0m20:31:17.334172 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias derived_columns
[0m20:31:17.339969 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 6, true, [cli_id#159, rcrd_src_nm#160, addr#161, ts#162, ld_dt_tm#147, EFFECTIVE_FROM#148]
[0m20:31:17.339969 [error] [MainThread]:                       :  :              :                    :           :              :- CTERelationDef 8, false
[0m20:31:17.339969 [error] [MainThread]:                       :  :              :                    :           :              :  +- SubqueryAlias columns_to_select
[0m20:31:17.339969 [error] [MainThread]:                       :  :              :                    :           :              :     +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.339969 [error] [MainThread]:                       :  :              :                    :           :              :        +- SubqueryAlias hashed_columns
[0m20:31:17.343483 [error] [MainThread]:                       :  :              :                    :           :              :           +- CTERelationRef 7, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, hsh_ky_cli_cd#149, rcrd_hsh_id#150]
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :           :              +- Project [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :           :                 +- SubqueryAlias columns_to_select
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :           :                    +- CTERelationRef 8, true, [CLI_ID#159, RCRD_SRC_NM#160, ADDR#161, TS#162, LD_DT_TM#147, EFFECTIVE_FROM#148, HSH_KY_CLI_CD#149, RCRD_HSH_ID#150]
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :           +- Project [hsh_ky_cli_cd#182, ts#179]
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#176,RCRD_SRC_NM#177,NAME#178,TS#179,LD_DT_TM#180,EFFECTIVE_FROM#181,HSH_KY_CLI_CD#182,RCRD_HSH_ID#183])
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :                    +- Project [cast(CLI_ID#184 as string) AS CLI_ID#176, cast(RCRD_SRC_NM#185 as string) AS RCRD_SRC_NM#177, cast(NAME#186 as string) AS NAME#178, cast(TS#187 as timestamp) AS TS#179, cast(LD_DT_TM#172 as timestamp) AS LD_DT_TM#180, cast(EFFECTIVE_FROM#173 as timestamp) AS EFFECTIVE_FROM#181, cast(HSH_KY_CLI_CD#174 as string) AS HSH_KY_CLI_CD#182, cast(RCRD_HSH_ID#175 as string) AS RCRD_HSH_ID#183]
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :                       +- WithCTE
[0m20:31:17.345462 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 10, false
[0m20:31:17.353469 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias source_data
[0m20:31:17.355019 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#184,rcrd_src_nm#185,name#186,ts#187])
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :              +- Project [cast(cli_id#165 as string) AS cli_id#184, cast(rcrd_src_nm#166 as string) AS rcrd_src_nm#185, cast(name#190 as string) AS name#186, cast(ts#191 as timestamp) AS ts#187]
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :                 +- Distinct
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :                    +- Project [cli_id#165, rcrd_src_nm#166, name#190, ts#191]
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :                       +- Join LeftOuter, (cast(cli_id#165 as int) = cli_id#189)
[0m20:31:17.357238 [error] [MainThread]:                       :  :              :                    :                          :                          :- SubqueryAlias v_h
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#165,rcrd_src_nm#166])
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :        +- Project [cast(cli_id#163 as string) AS cli_id#165, cast(rcrd_src_nm#164 as string) AS rcrd_src_nm#166]
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :           +- WithCTE
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :              :- CTERelationDef 14, false
[0m20:31:17.363634 [error] [MainThread]:                       :  :              :                    :                          :                          :              :  +- SubqueryAlias cli
[0m20:31:17.370282 [error] [MainThread]:                       :  :              :                    :                          :                          :              :     +- Distinct
[0m20:31:17.371332 [error] [MainThread]:                       :  :              :                    :                          :                          :              :        +- Project [trim(cast(cli_id#188 as string), None) AS cli_id#163, dummy AS rcrd_src_nm#164]
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :              :           +- Filter (1 = 1)
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :              :              +- SubqueryAlias cli
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#188], Partition Cols: []]
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :              +- Project [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.373527 [error] [MainThread]:                       :  :              :                    :                          :                          :                 +- SubqueryAlias cli
[0m20:31:17.378935 [error] [MainThread]:                       :  :              :                    :                          :                          :                    +- CTERelationRef 14, true, [cli_id#163, rcrd_src_nm#164]
[0m20:31:17.378935 [error] [MainThread]:                       :  :              :                    :                          :                          +- SubqueryAlias v_s_name
[0m20:31:17.378935 [error] [MainThread]:                       :  :              :                    :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m20:31:17.383443 [error] [MainThread]:                       :  :              :                    :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#189, name#190, ts#191], Partition Cols: []]
[0m20:31:17.383443 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 11, false
[0m20:31:17.383443 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias derived_columns
[0m20:31:17.383443 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [cli_id#184, rcrd_src_nm#185, name#186, ts#187, current_timestamp() AS ld_dt_tm#172, ts#187 AS EFFECTIVE_FROM#173]
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias source_data
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 10, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187]
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 12, false
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias hashed_columns
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, cast(md5(cast(nullif(upper(trim(cast(cli_id#184 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#174, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#186 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#175]
[0m20:31:17.386977 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias derived_columns
[0m20:31:17.393618 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 11, true, [cli_id#184, rcrd_src_nm#185, name#186, ts#187, ld_dt_tm#172, EFFECTIVE_FROM#173]
[0m20:31:17.395144 [error] [MainThread]:                       :  :              :                    :                          :- CTERelationDef 13, false
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                          :  +- SubqueryAlias columns_to_select
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                          :     +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                          :        +- SubqueryAlias hashed_columns
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                          :           +- CTERelationRef 12, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, hsh_ky_cli_cd#174, rcrd_hsh_id#175]
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                          +- Project [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                             +- SubqueryAlias columns_to_select
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    :                                +- CTERelationRef 13, true, [CLI_ID#184, RCRD_SRC_NM#185, NAME#186, TS#187, LD_DT_TM#172, EFFECTIVE_FROM#173, HSH_KY_CLI_CD#174, RCRD_HSH_ID#175]
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                    +- Distinct
[0m20:31:17.396081 [error] [MainThread]:                       :  :              :                       +- Project [hsh_ky_cli_cd#157 AS hk_cli_cd#143, ts#154 AS AS_OF_DATE#144]
[0m20:31:17.403589 [error] [MainThread]:                       :  :              :                          +- SubqueryAlias as_of_date
[0m20:31:17.403589 [error] [MainThread]:                       :  :              :                             +- CTERelationRef 4, true, [hsh_ky_cli_cd#157, ts#154]
[0m20:31:17.403589 [error] [MainThread]:                       :  :              :- CTERelationDef 1, false
[0m20:31:17.407934 [error] [MainThread]:                       :  :              :  +- SubqueryAlias new_rows_as_of_dates
[0m20:31:17.407934 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146]
[0m20:31:17.407934 [error] [MainThread]:                       :  :              :        +- Join LeftOuter, (hk_cli_cd#198 = hk_cli_cd#145)
[0m20:31:17.407934 [error] [MainThread]:                       :  :              :           :- SubqueryAlias a
[0m20:31:17.407934 [error] [MainThread]:                       :  :              :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :           :     +- Relation ndb.h_cli[_hoodie_commit_time#193,_hoodie_commit_seqno#194,_hoodie_record_key#195,_hoodie_partition_path#196,_hoodie_file_name#197,hk_cli_cd#198,cli_id#199,ld_dt_tm#200,rcrd_src_nm#201] parquet
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :           +- SubqueryAlias b
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :              +- SubqueryAlias as_of_dates
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :                 +- CTERelationRef 0, true, [hk_cli_cd#145, AS_OF_DATE#146]
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :- CTERelationDef 2, false
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :  +- SubqueryAlias new_rows
[0m20:31:17.413454 [error] [MainThread]:                       :  :              :     +- Aggregate [hk_cli_cd#198, AS_OF_DATE#146], [hk_cli_cd#198, AS_OF_DATE#146, coalesce(max(hsh_ky_cli_cd#207), cast(0000000000000000 as string)) AS S_ADDRESS_PK#133, coalesce(max(EFFECTIVE_FROM#210), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#134, coalesce(max(hsh_ky_cli_cd#218), cast(0000000000000000 as string)) AS S_NAME_PK#135, coalesce(max(EFFECTIVE_FROM#221), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#136]
[0m20:31:17.419665 [error] [MainThread]:                       :  :              :        +- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#218) AND (EFFECTIVE_FROM#221 <= AS_OF_DATE#146))
[0m20:31:17.420269 [error] [MainThread]:                       :  :              :           :- Join LeftOuter, ((hk_cli_cd#198 = hsh_ky_cli_cd#207) AND (EFFECTIVE_FROM#210 <= AS_OF_DATE#146))
[0m20:31:17.420269 [error] [MainThread]:                       :  :              :           :  :- SubqueryAlias a
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           :  :     +- CTERelationRef 1, true, [hk_cli_cd#198, AS_OF_DATE#146]
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           :  +- SubqueryAlias s_address_src
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           :        +- Relation ndb.s_address[_hoodie_commit_time#202,_hoodie_commit_seqno#203,_hoodie_record_key#204,_hoodie_partition_path#205,_hoodie_file_name#206,hsh_ky_cli_cd#207,rcrd_hsh_id#208,addr#209,EFFECTIVE_FROM#210,ld_dt_tm#211,rcrd_src_nm#212] parquet
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :           +- SubqueryAlias s_name_src
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :                 +- Relation ndb.s_name[_hoodie_commit_time#213,_hoodie_commit_seqno#214,_hoodie_record_key#215,_hoodie_partition_path#216,_hoodie_file_name#217,hsh_ky_cli_cd#218,rcrd_hsh_id#219,name#220,EFFECTIVE_FROM#221,ld_dt_tm#222,rcrd_src_nm#223] parquet
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :- CTERelationDef 3, false
[0m20:31:17.424478 [error] [MainThread]:                       :  :              :  +- SubqueryAlias pit
[0m20:31:17.433272 [error] [MainThread]:                       :  :              :     +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.434356 [error] [MainThread]:                       :  :              :        +- SubqueryAlias new_rows
[0m20:31:17.434356 [error] [MainThread]:                       :  :              :           +- CTERelationRef 2, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.435362 [error] [MainThread]:                       :  :              +- Distinct
[0m20:31:17.436949 [error] [MainThread]:                       :  :                 +- Project [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.436949 [error] [MainThread]:                       :  :                    +- SubqueryAlias pit
[0m20:31:17.436949 [error] [MainThread]:                       :  :                       +- CTERelationRef 3, true, [hk_cli_cd#198, AS_OF_DATE#146, S_ADDRESS_PK#133, S_ADDRESS_LDTS#134, S_NAME_PK#135, S_NAME_LDTS#136]
[0m20:31:17.441141 [error] [MainThread]:                       :  +- 'SubqueryAlias a
[0m20:31:17.441141 [error] [MainThread]:                       :     +- 'UnresolvedRelation [s_address], [], false
[0m20:31:17.441141 [error] [MainThread]:                       +- 'SubqueryAlias b
[0m20:31:17.443654 [error] [MainThread]:                          +- 'UnresolvedRelation [s_name], [], false
[0m20:31:17.443654 [error] [MainThread]:     
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:131)
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.443654 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.443654 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.451282 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.451282 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.453660 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.453660 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.457671 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.457671 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.457671 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.457671 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.457671 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.457671 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.457671 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.463681 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.463681 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.463681 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.463681 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.463681 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.463681 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.467215 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.467215 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.467215 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.470393 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.470393 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.473413 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.474175 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.474175 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.483733 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.483733 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.483733 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.490290 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.491626 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.491626 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.493649 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.493649 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.493649 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.493649 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.493649 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.493649 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.493649 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.493649 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.499998 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.499998 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.499998 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.499998 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.499998 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.503613 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.503613 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.503613 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m20:31:17.507584 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m20:31:17.507584 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.507584 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m20:31:17.513593 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m20:31:17.513593 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m20:31:17.515125 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m20:31:17.520249 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m20:31:17.520249 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m20:31:17.523260 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m20:31:17.523819 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m20:31:17.533621 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m20:31:17.534298 [error] [MainThread]:     	... 16 more
[0m20:31:17.534298 [error] [MainThread]:     
[0m20:31:17.536931 [info ] [MainThread]: 
[0m20:31:17.536931 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:31:17.536931 [debug] [MainThread]: Command `dbt run` failed at 20:31:17.536931 after 4.69 seconds
[0m20:31:17.536931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED6C2DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED96E6F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024ED97F3040>]}
[0m20:31:17.540967 [debug] [MainThread]: Flushing usage events
[0m20:32:28.511431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746AE1DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746D532D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746D532BC0>]}


============================== 20:32:28.513484 | d22e40e0-4382-430b-9793-f90fc457e411 ==============================
[0m20:32:28.513484 [info ] [MainThread]: Running with dbt=1.5.2
[0m20:32:28.513484 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m20:32:28.638674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746D532DD0>]}
[0m20:32:28.653889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746D8E2F20>]}
[0m20:32:28.653889 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m20:32:28.673627 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m20:32:28.828860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:32:28.828860 [debug] [MainThread]: Partial parsing: updated file: poc_demo://models\pit_test\pit_test\final_table.sql
[0m20:32:28.844532 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\final_table.sql
[0m20:32:28.873946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DC980D0>]}
[0m20:32:28.893434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DBD2500>]}
[0m20:32:28.893434 [info ] [MainThread]: Found 18 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m20:32:28.893434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DBD25C0>]}
[0m20:32:28.903365 [info ] [MainThread]: 
[0m20:32:28.903365 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:32:28.908678 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:32:28.918882 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:32:28.918882 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:32:28.918882 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:32:29.043596 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:32:29.043596 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:32:29.053593 [debug] [ThreadPool]: On list_schemas: Close
[0m20:32:29.066900 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m20:32:29.075011 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:32:29.075011 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m20:32:29.075011 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m20:32:29.075011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:32:29.546408 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:32:29.546408 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:32:29.561036 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m20:32:29.561036 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:32:29.563572 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m20:32:29.578430 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m20:32:29.585306 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:32:29.591209 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m20:32:29.591209 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m20:32:29.592323 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:32:30.044390 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:32:30.044390 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:32:30.053507 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m20:32:30.053507 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:32:30.053507 [debug] [ThreadPool]: On list_None_ndb: Close
[0m20:32:30.075875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DC7FAF0>]}
[0m20:32:30.075875 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:32:30.079394 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:32:30.079394 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:32:30.079394 [info ] [MainThread]: 
[0m20:32:30.084455 [debug] [Thread-1 (]: Began running node model.poc_demo.final_table
[0m20:32:30.084455 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.final_table .................................... [RUN]
[0m20:32:30.089165 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.final_table'
[0m20:32:30.089165 [debug] [Thread-1 (]: Began compiling node model.poc_demo.final_table
[0m20:32:30.089165 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.final_table"
[0m20:32:30.093679 [debug] [Thread-1 (]: Timing info for model.poc_demo.final_table (compile): 20:32:30.089165 => 20:32:30.089165
[0m20:32:30.093679 [debug] [Thread-1 (]: Began executing node model.poc_demo.final_table
[0m20:32:30.117624 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.final_table"
[0m20:32:30.117624 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:32:30.117624 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.final_table"
[0m20:32:30.124008 [debug] [Thread-1 (]: On model.poc_demo.final_table: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.final_table"} */
create or replace view ndb.final_table
  
  as
    select 
hk_cli_cd,
start_date,
end_date,
addr,
name
from (
select distinct p.hk_cli_cd, p.as_of_date as start_date,
lead(p.as_of_date) over (partition by p.hk_cli_cd order by p.as_of_date asc) as end_date,
a.addr, b.name
from ndb.pit_client p
LEFT JOIN ndb.s_address a
    ON p.s_address_pk = a.hsh_ky_cli_cd and p.s_address_ldts = a.effective_from
LEFT JOIN ndb.s_name b
    ON p.s_name_ldts = b.effective_from and p.hk_cli_cd = b.hsh_ky_cli_cd
order by 1 asc
)

[0m20:32:30.124008 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:32:31.133647 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:32:31.138748 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m20:32:31.154671 [debug] [Thread-1 (]: Timing info for model.poc_demo.final_table (execute): 20:32:30.093679 => 20:32:31.154671
[0m20:32:31.154671 [debug] [Thread-1 (]: On model.poc_demo.final_table: ROLLBACK
[0m20:32:31.154671 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:32:31.154671 [debug] [Thread-1 (]: On model.poc_demo.final_table: Close
[0m20:32:31.173443 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd22e40e0-4382-430b-9793-f90fc457e411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DCFC550>]}
[0m20:32:31.175500 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.final_table ............................... [[32mOK[0m in 1.09s]
[0m20:32:31.175500 [debug] [Thread-1 (]: Finished running node model.poc_demo.final_table
[0m20:32:31.175500 [debug] [MainThread]: On master: ROLLBACK
[0m20:32:31.175500 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:32:31.233563 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:32:31.233563 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:32:31.233563 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:32:31.233563 [debug] [MainThread]: On master: ROLLBACK
[0m20:32:31.233563 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:32:31.238597 [debug] [MainThread]: On master: Close
[0m20:32:31.243246 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:32:31.243246 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m20:32:31.243246 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m20:32:31.249368 [debug] [MainThread]: Connection 'model.poc_demo.final_table' was properly closed.
[0m20:32:31.250373 [info ] [MainThread]: 
[0m20:32:31.250373 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.35 seconds (2.35s).
[0m20:32:31.250373 [debug] [MainThread]: Command end result
[0m20:32:31.266264 [info ] [MainThread]: 
[0m20:32:31.268372 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:32:31.268890 [info ] [MainThread]: 
[0m20:32:31.269964 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:32:31.271556 [debug] [MainThread]: Command `dbt run` succeeded at 20:32:31.271556 after 2.77 seconds
[0m20:32:31.272129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746AE1DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746B163FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001746DD32F50>]}
[0m20:32:31.272634 [debug] [MainThread]: Flushing usage events
[0m00:30:42.250704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53CE4DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F562DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F562C20>]}


============================== 00:30:42.254806 | 0965b804-dd51-4ed5-8378-d09c9b1422f4 ==============================
[0m00:30:42.254806 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:30:42.254806 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:30:42.370262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F562E30>]}
[0m00:30:42.378862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53F912F50>]}
[0m00:30:42.379878 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:30:42.398846 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:30:42.512831 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m00:30:42.528464 [debug] [MainThread]: Partial parsing: added file: poc_demo://models\pit_test\pit_test\pit_client2.sql
[0m00:30:42.528464 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:30:42.597938 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:30:42.734556 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:30:42.734556 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:30:42.766265 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:30:42.782290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53FD9D960>]}
[0m00:30:42.829403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53BF8BA30>]}
[0m00:30:42.829403 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:30:42.829403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53BF8B970>]}
[0m00:30:42.829403 [info ] [MainThread]: 
[0m00:30:42.829403 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:30:42.829403 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:30:42.852288 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:30:42.852288 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:30:42.852288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:30:43.247794 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:30:43.247794 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:30:43.265814 [debug] [ThreadPool]: On list_schemas: Close
[0m00:30:43.285963 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m00:30:43.292978 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:43.295497 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:30:43.295497 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:30:43.295497 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:30:43.955637 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:30:43.955637 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m00:30:43.970461 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:30:43.970461 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:30:43.970461 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:30:43.986309 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m00:30:43.993455 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:43.993455 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:30:43.993455 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:30:43.993455 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:30:44.521258 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:30:44.525830 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m00:30:44.526613 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:30:44.526613 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:30:44.526613 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:30:44.557744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53FD54A90>]}
[0m00:30:44.560253 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:44.560253 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:30:44.560253 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:30:44.560253 [info ] [MainThread]: 
[0m00:30:44.560253 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:30:44.560253 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:30:44.560253 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:30:44.571840 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:30:44.599207 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:30:44.599207 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:30:44.571840 => 00:30:44.599207
[0m00:30:44.599207 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:30:44.632328 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:30:44.633340 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:44.634371 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:30:44.634371 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `addr``name`from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    )
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:30:44.635363 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:30:44.740173 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'pit\'(line 66, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    `addr``name`from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    )\npit AS (\n^^^\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'pit\'(line 66, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    `addr``name`from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    )\npit AS (\n^^^\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:30:44.740173 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:30:44.740173 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `addr``name`from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    )
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:30:44.742684 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'pit'(line 66, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      `addr``name`from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      )
  pit AS (
  ^^^
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'pit'(line 66, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      `addr``name`from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      )
  pit AS (
  ^^^
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:30:44.742684 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:30:44.599207 => 00:30:44.742684
[0m00:30:44.742684 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:30:44.742684 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:30:44.742684 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:30:44.759507 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'pit'(line 66, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        `addr``name`from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        )
    pit AS (
    ^^^
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'pit'(line 66, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        `addr``name`from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        )
    pit AS (
    ^^^
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:30:44.759507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0965b804-dd51-4ed5-8378-d09c9b1422f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53FDA6E00>]}
[0m00:30:44.759507 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.20s]
[0m00:30:44.759507 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:30:44.759507 [debug] [MainThread]: On master: ROLLBACK
[0m00:30:44.759507 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:30:44.833383 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:30:44.833383 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:30:44.833383 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:30:44.833383 [debug] [MainThread]: On master: ROLLBACK
[0m00:30:44.833383 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:30:44.833383 [debug] [MainThread]: On master: Close
[0m00:30:44.853046 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:30:44.854050 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:30:44.854050 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m00:30:44.855044 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:30:44.855044 [info ] [MainThread]: 
[0m00:30:44.856045 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.03 seconds (2.03s).
[0m00:30:44.857046 [debug] [MainThread]: Command end result
[0m00:30:44.877479 [info ] [MainThread]: 
[0m00:30:44.878487 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:30:44.879384 [info ] [MainThread]: 
[0m00:30:44.880392 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:30:44.880392 [error] [MainThread]:   Database Error
[0m00:30:44.880392 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:30:44.881902 [error] [MainThread]:     Syntax error at or near 'pit'(line 66, pos 0)
[0m00:30:44.882909 [error] [MainThread]:     
[0m00:30:44.884064 [error] [MainThread]:     == SQL ==
[0m00:30:44.884757 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:30:44.884757 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:30:44.884757 [error] [MainThread]:       
[0m00:30:44.884757 [error] [MainThread]:       as
[0m00:30:44.884757 [error] [MainThread]:         
[0m00:30:44.884757 [error] [MainThread]:     
[0m00:30:44.884757 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     
[0m00:30:44.889946 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:30:44.895971 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:30:44.896981 [error] [MainThread]:     
[0m00:30:44.896981 [error] [MainThread]:     
[0m00:30:44.897986 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:30:44.898977 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:30:44.898977 [error] [MainThread]:     ),
[0m00:30:44.900487 [error] [MainThread]:     
[0m00:30:44.901690 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:30:44.901690 [error] [MainThread]:         SELECT
[0m00:30:44.901690 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:30:44.901690 [error] [MainThread]:             b.AS_OF_DATE
[0m00:30:44.901690 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:30:44.901690 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:30:44.901690 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:30:44.901690 [error] [MainThread]:     ),
[0m00:30:44.901690 [error] [MainThread]:     
[0m00:30:44.901690 [error] [MainThread]:     new_rows AS (
[0m00:30:44.901690 [error] [MainThread]:         SELECT
[0m00:30:44.901690 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:30:44.901690 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:30:44.901690 [error] [MainThread]:         timestamp
[0m00:30:44.901690 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:30:44.901690 [error] [MainThread]:         timestamp
[0m00:30:44.912599 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:30:44.912599 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:30:44.913609 [error] [MainThread]:     
[0m00:30:44.914609 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:30:44.914609 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:30:44.915608 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:30:44.915608 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:30:44.916606 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:30:44.916606 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:30:44.917858 [error] [MainThread]:         GROUP BY
[0m00:30:44.917858 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:30:44.917858 [error] [MainThread]:     ),
[0m00:30:44.917858 [error] [MainThread]:     temp as (
[0m00:30:44.917858 [error] [MainThread]:         select 
[0m00:30:44.917858 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:30:44.917858 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:30:44.917858 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:30:44.917858 [error] [MainThread]:         `addr``name`from new_rows a
[0m00:30:44.917858 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:30:44.917858 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:30:44.917858 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:30:44.917858 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:30:44.917858 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:30:44.917858 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:30:44.917858 [error] [MainThread]:         )
[0m00:30:44.917858 [error] [MainThread]:     pit AS (
[0m00:30:44.929268 [error] [MainThread]:     ^^^
[0m00:30:44.930049 [error] [MainThread]:         SELECT * FROM temp
[0m00:30:44.930049 [error] [MainThread]:     )
[0m00:30:44.931059 [error] [MainThread]:     
[0m00:30:44.932056 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:30:44.932056 [error] [MainThread]:     
[0m00:30:44.933056 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:30:44.934056 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:30:44.934876 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:30:44.934876 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:30:44.934876 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:30:44.934876 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:30:44.938885 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:30:44.938885 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:30:44.940521 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:30:44.940521 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:30:44.940521 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:30:44.940521 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:30:44.940521 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:30:44.940521 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:30:44.940521 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:30:44.940521 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:30:44.945985 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:30:44.945985 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:30:44.946997 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:30:44.946997 [error] [MainThread]:     Syntax error at or near 'pit'(line 66, pos 0)
[0m00:30:44.947996 [error] [MainThread]:     
[0m00:30:44.947996 [error] [MainThread]:     == SQL ==
[0m00:30:44.948995 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:30:44.949995 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:30:44.951095 [error] [MainThread]:       
[0m00:30:44.951095 [error] [MainThread]:       as
[0m00:30:44.951095 [error] [MainThread]:         
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.951095 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:30:44.951095 [error] [MainThread]:     
[0m00:30:44.958605 [error] [MainThread]:     
[0m00:30:44.958605 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:30:44.958605 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:30:44.958605 [error] [MainThread]:     
[0m00:30:44.958605 [error] [MainThread]:     
[0m00:30:44.958605 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:30:44.958605 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:30:44.958605 [error] [MainThread]:     ),
[0m00:30:44.962640 [error] [MainThread]:     
[0m00:30:44.962640 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:30:44.963648 [error] [MainThread]:         SELECT
[0m00:30:44.963648 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:30:44.964647 [error] [MainThread]:             b.AS_OF_DATE
[0m00:30:44.964647 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:30:44.965650 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:30:44.965650 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:30:44.966869 [error] [MainThread]:     ),
[0m00:30:44.967552 [error] [MainThread]:     
[0m00:30:44.967552 [error] [MainThread]:     new_rows AS (
[0m00:30:44.967552 [error] [MainThread]:         SELECT
[0m00:30:44.967552 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:30:44.967552 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:30:44.967552 [error] [MainThread]:         timestamp
[0m00:30:44.967552 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:30:44.967552 [error] [MainThread]:         timestamp
[0m00:30:44.967552 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:30:44.967552 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:30:44.967552 [error] [MainThread]:     
[0m00:30:44.967552 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:30:44.967552 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:30:44.967552 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:30:44.967552 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:30:44.977557 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:30:44.977557 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:30:44.977557 [error] [MainThread]:         GROUP BY
[0m00:30:44.977557 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:30:44.979302 [error] [MainThread]:     ),
[0m00:30:44.980309 [error] [MainThread]:     temp as (
[0m00:30:44.980309 [error] [MainThread]:         select 
[0m00:30:44.981310 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:30:44.981310 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:30:44.982309 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:30:44.983309 [error] [MainThread]:         `addr``name`from new_rows a
[0m00:30:44.983309 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:30:44.984484 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:30:44.984484 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:30:44.984484 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:30:44.984484 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:30:44.984484 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:30:44.984484 [error] [MainThread]:         )
[0m00:30:44.984484 [error] [MainThread]:     pit AS (
[0m00:30:44.984484 [error] [MainThread]:     ^^^
[0m00:30:44.984484 [error] [MainThread]:         SELECT * FROM temp
[0m00:30:44.984484 [error] [MainThread]:     )
[0m00:30:44.984484 [error] [MainThread]:     
[0m00:30:44.984484 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:30:44.984484 [error] [MainThread]:     
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:30:44.984484 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:30:44.995972 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:30:44.996981 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:30:44.996981 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:30:44.998354 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:30:44.998354 [error] [MainThread]:     	... 16 more
[0m00:30:44.998354 [error] [MainThread]:     
[0m00:30:44.998354 [info ] [MainThread]: 
[0m00:30:44.998354 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:30:44.998354 [debug] [MainThread]: Command `dbt run` failed at 00:30:44.998354 after 2.77 seconds
[0m00:30:44.998354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53CE4DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53EA09240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C53FBE3F70>]}
[0m00:30:44.998354 [debug] [MainThread]: Flushing usage events
[0m00:33:13.063111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C264DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C4D62DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C4D62C20>]}


============================== 00:33:13.078744 | 97f33f3d-3094-4d02-ba6c-7c134480da94 ==============================
[0m00:33:13.078744 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:33:13.078744 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:33:13.189799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C4D62E30>]}
[0m00:33:13.189799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C510AF50>]}
[0m00:33:13.205336 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:33:13.220956 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:33:13.338974 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:33:13.353986 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:33:13.401756 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:33:13.511552 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:33:13.511552 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:33:13.542802 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:33:13.563638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C5595930>]}
[0m00:33:13.605494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C178BA30>]}
[0m00:33:13.605494 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:33:13.605494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C178B970>]}
[0m00:33:13.621353 [info ] [MainThread]: 
[0m00:33:13.621353 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:33:13.621353 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:33:13.621353 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:33:13.637721 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:33:13.637721 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:13.825968 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:33:13.825968 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:33:13.834724 [debug] [ThreadPool]: On list_schemas: Close
[0m00:33:13.849842 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:33:13.849842 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:33:13.849842 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:33:13.849842 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:33:13.849842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:33:14.140209 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:33:14.140209 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:33:14.151254 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:33:14.151254 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:33:14.151254 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:33:14.167204 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:33:14.167204 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:33:14.167204 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:33:14.167204 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:33:14.167204 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:33:14.450372 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:33:14.450372 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:33:14.462244 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:33:14.462244 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:33:14.467750 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:33:14.484894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C5467100>]}
[0m00:33:14.485903 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:33:14.485903 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:33:14.485903 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:33:14.487525 [info ] [MainThread]: 
[0m00:33:14.493092 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:33:14.493092 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:33:14.496315 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:33:14.496315 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:33:14.508849 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:33:14.508849 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:33:14.496315 => 00:33:14.508849
[0m00:33:14.508849 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:33:14.542211 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:33:14.542211 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:33:14.542211 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:33:14.542211 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `addr`,`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:33:14.542211 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:33:14.602020 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 32)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    `addr`,`name`,from new_rows a\n--------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 32)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    `addr`,`name`,from new_rows a\n--------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:33:14.606530 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:33:14.606530 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `addr`,`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:33:14.606530 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 32)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      `addr`,`name`,from new_rows a
  --------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 32)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      `addr`,`name`,from new_rows a
  --------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:33:14.606530 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:33:14.508849 => 00:33:14.606530
[0m00:33:14.606530 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:33:14.606530 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:33:14.606530 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:33:14.622582 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 32)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        `addr`,`name`,from new_rows a
    --------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 32)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        `addr`,`name`,from new_rows a
    --------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:33:14.622582 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97f33f3d-3094-4d02-ba6c-7c134480da94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C5483760>]}
[0m00:33:14.622582 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.13s]
[0m00:33:14.622582 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:33:14.622582 [debug] [MainThread]: On master: ROLLBACK
[0m00:33:14.622582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:33:14.682815 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:33:14.682815 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:33:14.682815 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:33:14.682815 [debug] [MainThread]: On master: ROLLBACK
[0m00:33:14.682815 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:33:14.682815 [debug] [MainThread]: On master: Close
[0m00:33:14.694085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:33:14.695085 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:33:14.695085 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:33:14.696093 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:33:14.696093 [info ] [MainThread]: 
[0m00:33:14.697084 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.07 seconds (1.07s).
[0m00:33:14.698087 [debug] [MainThread]: Command end result
[0m00:33:14.710124 [info ] [MainThread]: 
[0m00:33:14.711122 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:33:14.712121 [info ] [MainThread]: 
[0m00:33:14.713119 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:33:14.713119 [error] [MainThread]:   Database Error
[0m00:33:14.714122 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:33:14.715119 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 32)
[0m00:33:14.715119 [error] [MainThread]:     
[0m00:33:14.716118 [error] [MainThread]:     == SQL ==
[0m00:33:14.716118 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:33:14.717123 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:33:14.718120 [error] [MainThread]:       
[0m00:33:14.719162 [error] [MainThread]:       as
[0m00:33:14.719162 [error] [MainThread]:         
[0m00:33:14.720709 [error] [MainThread]:     
[0m00:33:14.721777 [error] [MainThread]:     
[0m00:33:14.722757 [error] [MainThread]:     
[0m00:33:14.723714 [error] [MainThread]:     
[0m00:33:14.724715 [error] [MainThread]:     
[0m00:33:14.725721 [error] [MainThread]:     
[0m00:33:14.726722 [error] [MainThread]:     
[0m00:33:14.727715 [error] [MainThread]:     
[0m00:33:14.728666 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:33:14.729675 [error] [MainThread]:     
[0m00:33:14.730403 [error] [MainThread]:     
[0m00:33:14.730403 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:33:14.730403 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:33:14.730403 [error] [MainThread]:     
[0m00:33:14.730403 [error] [MainThread]:     
[0m00:33:14.730403 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:33:14.738336 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:33:14.738336 [error] [MainThread]:     ),
[0m00:33:14.738336 [error] [MainThread]:     
[0m00:33:14.740308 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:33:14.741324 [error] [MainThread]:         SELECT
[0m00:33:14.742315 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:33:14.743315 [error] [MainThread]:             b.AS_OF_DATE
[0m00:33:14.743315 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:33:14.744316 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:33:14.745940 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:33:14.745940 [error] [MainThread]:     ),
[0m00:33:14.745940 [error] [MainThread]:     
[0m00:33:14.749809 [error] [MainThread]:     new_rows AS (
[0m00:33:14.749809 [error] [MainThread]:         SELECT
[0m00:33:14.749809 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:33:14.749809 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:33:14.749809 [error] [MainThread]:         timestamp
[0m00:33:14.749809 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:33:14.749809 [error] [MainThread]:         timestamp
[0m00:33:14.757073 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:33:14.757073 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:33:14.758618 [error] [MainThread]:     
[0m00:33:14.759173 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:33:14.760184 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:33:14.760184 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:33:14.761180 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:33:14.762179 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:33:14.763179 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:33:14.764372 [error] [MainThread]:         GROUP BY
[0m00:33:14.764372 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:33:14.764372 [error] [MainThread]:     ),
[0m00:33:14.764372 [error] [MainThread]:     temp as (
[0m00:33:14.768311 [error] [MainThread]:         select 
[0m00:33:14.768311 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:33:14.768311 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:33:14.768311 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:33:14.768311 [error] [MainThread]:         `addr`,`name`,from new_rows a
[0m00:33:14.768311 [error] [MainThread]:     --------------------------------^^^
[0m00:33:14.768311 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:33:14.773633 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:33:14.774651 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:33:14.775640 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:33:14.776645 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:33:14.776645 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:33:14.778151 [error] [MainThread]:         ),
[0m00:33:14.778151 [error] [MainThread]:     pit AS (
[0m00:33:14.779158 [error] [MainThread]:         SELECT * FROM temp
[0m00:33:14.780176 [error] [MainThread]:     )
[0m00:33:14.781507 [error] [MainThread]:     
[0m00:33:14.781507 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:33:14.781507 [error] [MainThread]:     
[0m00:33:14.781507 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:33:14.781507 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:33:14.781507 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:33:14.787825 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:33:14.787825 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:33:14.787825 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:33:14.787825 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:33:14.790317 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:33:14.790317 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:33:14.791324 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:33:14.792323 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:33:14.792323 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:33:14.793324 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:33:14.794323 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:33:14.794323 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:33:14.795322 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:33:14.796775 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:33:14.796775 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:33:14.796775 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:33:14.796775 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 32)
[0m00:33:14.796775 [error] [MainThread]:     
[0m00:33:14.796775 [error] [MainThread]:     == SQL ==
[0m00:33:14.796775 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:33:14.796775 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:33:14.796775 [error] [MainThread]:       
[0m00:33:14.796775 [error] [MainThread]:       as
[0m00:33:14.796775 [error] [MainThread]:         
[0m00:33:14.796775 [error] [MainThread]:     
[0m00:33:14.806974 [error] [MainThread]:     
[0m00:33:14.807561 [error] [MainThread]:     
[0m00:33:14.807985 [error] [MainThread]:     
[0m00:33:14.808984 [error] [MainThread]:     
[0m00:33:14.809984 [error] [MainThread]:     
[0m00:33:14.809984 [error] [MainThread]:     
[0m00:33:14.810985 [error] [MainThread]:     
[0m00:33:14.811986 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:33:14.812985 [error] [MainThread]:     
[0m00:33:14.813743 [error] [MainThread]:     
[0m00:33:14.813743 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:33:14.813743 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:33:14.813743 [error] [MainThread]:     
[0m00:33:14.817754 [error] [MainThread]:     
[0m00:33:14.817754 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:33:14.817754 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:33:14.817754 [error] [MainThread]:     ),
[0m00:33:14.817754 [error] [MainThread]:     
[0m00:33:14.817754 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:33:14.817754 [error] [MainThread]:         SELECT
[0m00:33:14.817754 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:33:14.817754 [error] [MainThread]:             b.AS_OF_DATE
[0m00:33:14.817754 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:33:14.823671 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:33:14.823671 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:33:14.824682 [error] [MainThread]:     ),
[0m00:33:14.825687 [error] [MainThread]:     
[0m00:33:14.826649 [error] [MainThread]:     new_rows AS (
[0m00:33:14.826649 [error] [MainThread]:         SELECT
[0m00:33:14.827679 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:33:14.828678 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:33:14.829838 [error] [MainThread]:         timestamp
[0m00:33:14.829838 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:33:14.829838 [error] [MainThread]:         timestamp
[0m00:33:14.829838 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:33:14.829838 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:33:14.834849 [error] [MainThread]:     
[0m00:33:14.834849 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:33:14.834849 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:33:14.834849 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:33:14.834849 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:33:14.834849 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:33:14.834849 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:33:14.834849 [error] [MainThread]:         GROUP BY
[0m00:33:14.834849 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:33:14.834849 [error] [MainThread]:     ),
[0m00:33:14.834849 [error] [MainThread]:     temp as (
[0m00:33:14.834849 [error] [MainThread]:         select 
[0m00:33:14.834849 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:33:14.842443 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:33:14.842443 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:33:14.843630 [error] [MainThread]:         `addr`,`name`,from new_rows a
[0m00:33:14.843630 [error] [MainThread]:     --------------------------------^^^
[0m00:33:14.845484 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:33:14.845484 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:33:14.845484 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:33:14.845484 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:33:14.845484 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:33:14.845484 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:33:14.845484 [error] [MainThread]:         ),
[0m00:33:14.845484 [error] [MainThread]:     pit AS (
[0m00:33:14.845484 [error] [MainThread]:         SELECT * FROM temp
[0m00:33:14.845484 [error] [MainThread]:     )
[0m00:33:14.851047 [error] [MainThread]:     
[0m00:33:14.851047 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:33:14.851047 [error] [MainThread]:     
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:33:14.851047 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:33:14.859068 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:33:14.859908 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:33:14.859908 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:33:14.861025 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:33:14.861025 [error] [MainThread]:     	... 16 more
[0m00:33:14.861025 [error] [MainThread]:     
[0m00:33:14.863690 [info ] [MainThread]: 
[0m00:33:14.863690 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:33:14.863690 [debug] [MainThread]: Command `dbt run` failed at 00:33:14.863690 after 1.80 seconds
[0m00:33:14.863690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C264DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C17D1A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000281C178B910>]}
[0m00:33:14.863690 [debug] [MainThread]: Flushing usage events
[0m00:34:35.979481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D151D2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154442D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154442BC0>]}


============================== 00:34:35.995695 | 9edaf6b8-917a-419b-9c63-9ebd50bc273b ==============================
[0m00:34:35.995695 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:34:35.995695 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:34:36.129044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154442DD0>]}
[0m00:34:36.129044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1547E2F20>]}
[0m00:34:36.129044 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:34:36.160394 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:34:36.264256 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:34:36.279790 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:34:36.327057 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:34:36.437112 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:34:36.437112 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:34:36.468746 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:34:36.485153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154C79CF0>]}
[0m00:34:36.532090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D152089FF0>]}
[0m00:34:36.532090 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:34:36.532090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D15208A980>]}
[0m00:34:36.532090 [info ] [MainThread]: 
[0m00:34:36.532090 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:34:36.547749 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:34:36.547749 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:34:36.547749 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:34:36.547749 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:34:36.662683 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:34:36.662683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:34:36.681932 [debug] [ThreadPool]: On list_schemas: Close
[0m00:34:36.700045 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:34:36.710560 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:36.710560 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:34:36.710560 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:34:36.710560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:34:37.068721 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:34:37.072654 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:34:37.081711 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:34:37.081711 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:34:37.081711 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:34:37.081711 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:34:37.099819 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:37.099819 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:34:37.099819 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:34:37.099819 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:34:37.382010 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:34:37.382010 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:34:37.391130 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:34:37.391130 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:34:37.391130 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:34:37.407210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154B1BBB0>]}
[0m00:34:37.407210 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:37.407210 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:34:37.407210 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:34:37.415230 [info ] [MainThread]: 
[0m00:34:37.415230 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:34:37.423517 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:34:37.423517 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:34:37.423517 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:34:37.441587 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:34:37.441587 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:34:37.423517 => 00:34:37.441587
[0m00:34:37.441587 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:34:37.481004 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:34:37.483046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:37.483976 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:34:37.483976 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:34:37.485005 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:34:37.539356 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:34:37.539860 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:34:37.539860 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:34:37.539860 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:34:37.539860 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:34:37.441587 => 00:34:37.539860
[0m00:34:37.539860 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:34:37.545465 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:34:37.546207 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:34:37.557573 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:34:37.557573 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9edaf6b8-917a-419b-9c63-9ebd50bc273b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D154BCF700>]}
[0m00:34:37.557573 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.13s]
[0m00:34:37.561607 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:34:37.562554 [debug] [MainThread]: On master: ROLLBACK
[0m00:34:37.562554 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:34:37.614969 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:34:37.614969 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:34:37.614969 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:34:37.614969 [debug] [MainThread]: On master: ROLLBACK
[0m00:34:37.619484 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:34:37.619484 [debug] [MainThread]: On master: Close
[0m00:34:37.626048 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:34:37.626048 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:34:37.626048 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:34:37.629055 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:34:37.629853 [info ] [MainThread]: 
[0m00:34:37.630961 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.10 seconds (1.10s).
[0m00:34:37.632856 [debug] [MainThread]: Command end result
[0m00:34:37.653381 [info ] [MainThread]: 
[0m00:34:37.654375 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:34:37.655391 [info ] [MainThread]: 
[0m00:34:37.656376 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:34:37.656376 [error] [MainThread]:   Database Error
[0m00:34:37.657384 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:34:37.658373 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:34:37.659374 [error] [MainThread]:     
[0m00:34:37.660374 [error] [MainThread]:     == SQL ==
[0m00:34:37.661371 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:34:37.662406 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:34:37.663915 [error] [MainThread]:       
[0m00:34:37.664938 [error] [MainThread]:       as
[0m00:34:37.665947 [error] [MainThread]:         
[0m00:34:37.666921 [error] [MainThread]:     
[0m00:34:37.667950 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.669623 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:34:37.669623 [error] [MainThread]:     
[0m00:34:37.679813 [error] [MainThread]:     
[0m00:34:37.680821 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:34:37.681820 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:34:37.682820 [error] [MainThread]:     
[0m00:34:37.683822 [error] [MainThread]:     
[0m00:34:37.684823 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:34:37.685823 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:34:37.686822 [error] [MainThread]:     ),
[0m00:34:37.688187 [error] [MainThread]:     
[0m00:34:37.688187 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:34:37.688187 [error] [MainThread]:         SELECT
[0m00:34:37.688187 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:34:37.688187 [error] [MainThread]:             b.AS_OF_DATE
[0m00:34:37.688187 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:34:37.688187 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:34:37.688187 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:34:37.696506 [error] [MainThread]:     ),
[0m00:34:37.698593 [error] [MainThread]:     
[0m00:34:37.699188 [error] [MainThread]:     new_rows AS (
[0m00:34:37.700494 [error] [MainThread]:         SELECT
[0m00:34:37.702829 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:34:37.703896 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:34:37.704947 [error] [MainThread]:         timestamp
[0m00:34:37.706040 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:34:37.706740 [error] [MainThread]:         timestamp
[0m00:34:37.708034 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:34:37.708987 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:34:37.709563 [error] [MainThread]:     
[0m00:34:37.710157 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:34:37.710659 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:34:37.711751 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:34:37.712274 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:34:37.712798 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:34:37.713027 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:34:37.713554 [error] [MainThread]:         GROUP BY
[0m00:34:37.713554 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:34:37.715067 [error] [MainThread]:     ),
[0m00:34:37.715379 [error] [MainThread]:     temp as (
[0m00:34:37.715379 [error] [MainThread]:         select 
[0m00:34:37.715379 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:34:37.715379 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:34:37.715379 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:34:37.715379 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:34:37.715379 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:34:37.715379 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:34:37.715379 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:34:37.715379 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:34:37.715379 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:34:37.715379 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:34:37.715379 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:34:37.715379 [error] [MainThread]:         ),
[0m00:34:37.715379 [error] [MainThread]:     pit AS (
[0m00:34:37.715379 [error] [MainThread]:         SELECT * FROM temp
[0m00:34:37.715379 [error] [MainThread]:     )
[0m00:34:37.715379 [error] [MainThread]:     
[0m00:34:37.715379 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:34:37.715379 [error] [MainThread]:     
[0m00:34:37.715379 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:34:37.715379 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:34:37.715379 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:34:37.715379 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:34:37.715379 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:34:37.729756 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:34:37.729756 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:34:37.730762 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:34:37.731764 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:34:37.732353 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:34:37.732353 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:34:37.732353 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     == SQL ==
[0m00:34:37.732353 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:34:37.732353 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:34:37.732353 [error] [MainThread]:       
[0m00:34:37.732353 [error] [MainThread]:       as
[0m00:34:37.732353 [error] [MainThread]:         
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.732353 [error] [MainThread]:     
[0m00:34:37.746461 [error] [MainThread]:     
[0m00:34:37.747468 [error] [MainThread]:     
[0m00:34:37.747468 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:34:37.748832 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:34:37.748832 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:34:37.748832 [error] [MainThread]:     ),
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:34:37.748832 [error] [MainThread]:         SELECT
[0m00:34:37.748832 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:34:37.748832 [error] [MainThread]:             b.AS_OF_DATE
[0m00:34:37.748832 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:34:37.748832 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:34:37.748832 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:34:37.748832 [error] [MainThread]:     ),
[0m00:34:37.748832 [error] [MainThread]:     
[0m00:34:37.748832 [error] [MainThread]:     new_rows AS (
[0m00:34:37.748832 [error] [MainThread]:         SELECT
[0m00:34:37.748832 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:34:37.748832 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:34:37.748832 [error] [MainThread]:         timestamp
[0m00:34:37.763063 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:34:37.764071 [error] [MainThread]:         timestamp
[0m00:34:37.764071 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:34:37.765080 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:34:37.765823 [error] [MainThread]:     
[0m00:34:37.765823 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:34:37.765823 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:34:37.765823 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:34:37.765823 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:34:37.765823 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:34:37.765823 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:34:37.765823 [error] [MainThread]:         GROUP BY
[0m00:34:37.765823 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:34:37.765823 [error] [MainThread]:     ),
[0m00:34:37.765823 [error] [MainThread]:     temp as (
[0m00:34:37.765823 [error] [MainThread]:         select 
[0m00:34:37.765823 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:34:37.765823 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:34:37.765823 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:34:37.765823 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:34:37.765823 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:34:37.765823 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:34:37.765823 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:34:37.765823 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:34:37.765823 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:34:37.765823 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:34:37.765823 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:34:37.765823 [error] [MainThread]:         ),
[0m00:34:37.779753 [error] [MainThread]:     pit AS (
[0m00:34:37.780760 [error] [MainThread]:         SELECT * FROM temp
[0m00:34:37.781760 [error] [MainThread]:     )
[0m00:34:37.782118 [error] [MainThread]:     
[0m00:34:37.782118 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:34:37.782118 [error] [MainThread]:     
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:34:37.782118 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:34:37.782118 [error] [MainThread]:     	... 16 more
[0m00:34:37.782118 [error] [MainThread]:     
[0m00:34:37.782118 [info ] [MainThread]: 
[0m00:34:37.782118 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:34:37.796395 [debug] [MainThread]: Command `dbt run` failed at 00:34:37.796395 after 1.82 seconds
[0m00:34:37.796395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D151D2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D152089FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D150E67B50>]}
[0m00:34:37.797407 [debug] [MainThread]: Flushing usage events
[0m00:37:42.287784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0C69DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0EDB2DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0EDB2BF0>]}


============================== 00:37:42.287784 | 46163c2d-652c-4f64-aba3-ff1fe54bba6b ==============================
[0m00:37:42.287784 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:37:42.287784 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:37:42.414504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0EDB2E00>]}
[0m00:37:42.422517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0F162DA0>]}
[0m00:37:42.423426 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:37:42.445512 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:37:42.560920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:37:42.560920 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:37:42.634960 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:37:42.753392 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:37:42.753392 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:37:42.789361 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:37:42.800872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0F5F1DE0>]}
[0m00:37:42.847814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0C9F8D60>]}
[0m00:37:42.847814 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:37:42.847814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0C9FA260>]}
[0m00:37:42.847814 [info ] [MainThread]: 
[0m00:37:42.847814 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:37:42.847814 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:37:42.867852 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:37:42.868853 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:37:42.869853 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:37:42.976606 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:37:42.976606 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:37:42.982748 [debug] [ThreadPool]: On list_schemas: Close
[0m00:37:42.998801 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m00:37:42.998801 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:42.998801 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:37:42.998801 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:37:42.998801 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:37:43.279388 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:37:43.279388 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:37:43.294757 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:37:43.294757 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:37:43.294757 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:37:43.305274 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m00:37:43.317635 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:43.317635 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:37:43.319176 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:37:43.319176 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:37:43.601683 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:37:43.601683 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:37:43.605692 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:37:43.605692 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:37:43.605692 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:37:43.622877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0F487BB0>]}
[0m00:37:43.622877 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:43.622877 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:43.622877 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:37:43.622877 [info ] [MainThread]: 
[0m00:37:43.622877 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:37:43.622877 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:37:43.635631 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:37:43.635631 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:37:43.664090 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:37:43.666090 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:37:43.636649 => 00:37:43.665090
[0m00:37:43.667213 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:37:43.696667 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:37:43.701181 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:43.701181 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:37:43.701181 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:37:43.701181 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:37:43.760232 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \',\': extra input \',\'(line 58, pos 25)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a\n-------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \',\': extra input \',\'(line 58, pos 25)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a\n-------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:37:43.760232 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:37:43.760232 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:37:43.760232 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near ',': extra input ','(line 58, pos 25)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
  -------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near ',': extra input ','(line 58, pos 25)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
  -------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:37:43.760232 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:37:43.667213 => 00:37:43.760232
[0m00:37:43.760232 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:37:43.760232 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:37:43.760232 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:37:43.779572 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near ',': extra input ','(line 58, pos 25)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
    -------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near ',': extra input ','(line 58, pos 25)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
    -------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:37:43.779572 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46163c2d-652c-4f64-aba3-ff1fe54bba6b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0F533700>]}
[0m00:37:43.779572 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.16s]
[0m00:37:43.779572 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:37:43.779572 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:43.779572 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:37:43.827872 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:37:43.827872 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:43.830382 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:43.830382 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:43.830382 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:37:43.830382 [debug] [MainThread]: On master: Close
[0m00:37:43.836399 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:37:43.836399 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:37:43.836399 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m00:37:43.836399 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:37:43.836399 [info ] [MainThread]: 
[0m00:37:43.836399 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.99 seconds (0.99s).
[0m00:37:43.836399 [debug] [MainThread]: Command end result
[0m00:37:43.854375 [info ] [MainThread]: 
[0m00:37:43.854375 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:37:43.854375 [info ] [MainThread]: 
[0m00:37:43.860090 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:37:43.860090 [error] [MainThread]:   Database Error
[0m00:37:43.861107 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:37:43.862127 [error] [MainThread]:     Syntax error at or near ',': extra input ','(line 58, pos 25)
[0m00:37:43.863100 [error] [MainThread]:     
[0m00:37:43.864101 [error] [MainThread]:     == SQL ==
[0m00:37:43.865097 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:37:43.866399 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:37:43.866399 [error] [MainThread]:       
[0m00:37:43.866399 [error] [MainThread]:       as
[0m00:37:43.866399 [error] [MainThread]:         
[0m00:37:43.866399 [error] [MainThread]:     
[0m00:37:43.866399 [error] [MainThread]:     
[0m00:37:43.866399 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     
[0m00:37:43.871407 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:37:43.876789 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:37:43.877799 [error] [MainThread]:     
[0m00:37:43.878809 [error] [MainThread]:     
[0m00:37:43.878809 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:37:43.879796 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:37:43.880800 [error] [MainThread]:     ),
[0m00:37:43.881796 [error] [MainThread]:     
[0m00:37:43.881796 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:37:43.882799 [error] [MainThread]:         SELECT
[0m00:37:43.883723 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:37:43.883723 [error] [MainThread]:             b.AS_OF_DATE
[0m00:37:43.883723 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:37:43.883723 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:37:43.883723 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:37:43.883723 [error] [MainThread]:     ),
[0m00:37:43.883723 [error] [MainThread]:     
[0m00:37:43.883723 [error] [MainThread]:     new_rows AS (
[0m00:37:43.883723 [error] [MainThread]:         SELECT
[0m00:37:43.883723 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:37:43.891232 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:37:43.891232 [error] [MainThread]:         timestamp
[0m00:37:43.891232 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:37:43.893412 [error] [MainThread]:         timestamp
[0m00:37:43.893412 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:37:43.894421 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:37:43.895419 [error] [MainThread]:     
[0m00:37:43.895419 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:37:43.896423 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:37:43.897421 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:37:43.898422 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:37:43.898570 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:37:43.900878 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:37:43.900878 [error] [MainThread]:         GROUP BY
[0m00:37:43.900878 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:37:43.900878 [error] [MainThread]:     ),
[0m00:37:43.900878 [error] [MainThread]:     temp as (
[0m00:37:43.900878 [error] [MainThread]:         select 
[0m00:37:43.900878 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:37:43.900878 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:37:43.900878 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:37:43.900878 [error] [MainThread]:         ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
[0m00:37:43.900878 [error] [MainThread]:     -------------------------^^^
[0m00:37:43.900878 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:37:43.900878 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:37:43.900878 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:37:43.900878 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:37:43.910077 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:37:43.910077 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:37:43.911085 [error] [MainThread]:         ),
[0m00:37:43.911085 [error] [MainThread]:     pit AS (
[0m00:37:43.912084 [error] [MainThread]:         SELECT * FROM temp
[0m00:37:43.913084 [error] [MainThread]:     )
[0m00:37:43.913084 [error] [MainThread]:     
[0m00:37:43.914083 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:37:43.915321 [error] [MainThread]:     
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:37:43.915321 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:37:43.915321 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:37:43.921624 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:37:43.921624 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:37:43.921624 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:37:43.921624 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:37:43.921624 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:37:43.926760 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:37:43.926760 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:37:43.927768 [error] [MainThread]:     Syntax error at or near ',': extra input ','(line 58, pos 25)
[0m00:37:43.928767 [error] [MainThread]:     
[0m00:37:43.929277 [error] [MainThread]:     == SQL ==
[0m00:37:43.930303 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:37:43.930303 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:37:43.930303 [error] [MainThread]:       
[0m00:37:43.930303 [error] [MainThread]:       as
[0m00:37:43.930303 [error] [MainThread]:         
[0m00:37:43.930303 [error] [MainThread]:     
[0m00:37:43.930303 [error] [MainThread]:     
[0m00:37:43.935929 [error] [MainThread]:     
[0m00:37:43.939130 [error] [MainThread]:     
[0m00:37:43.939130 [error] [MainThread]:     
[0m00:37:43.939130 [error] [MainThread]:     
[0m00:37:43.939130 [error] [MainThread]:     
[0m00:37:43.939130 [error] [MainThread]:     
[0m00:37:43.943384 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:37:43.944410 [error] [MainThread]:     
[0m00:37:43.945412 [error] [MainThread]:     
[0m00:37:43.946055 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:37:43.946055 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:37:43.946055 [error] [MainThread]:     
[0m00:37:43.946055 [error] [MainThread]:     
[0m00:37:43.946055 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:37:43.946055 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:37:43.946055 [error] [MainThread]:     ),
[0m00:37:43.946055 [error] [MainThread]:     
[0m00:37:43.951566 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:37:43.951566 [error] [MainThread]:         SELECT
[0m00:37:43.951566 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:37:43.951566 [error] [MainThread]:             b.AS_OF_DATE
[0m00:37:43.951566 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:37:43.951566 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:37:43.951566 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:37:43.951566 [error] [MainThread]:     ),
[0m00:37:43.951566 [error] [MainThread]:     
[0m00:37:43.951566 [error] [MainThread]:     new_rows AS (
[0m00:37:43.951566 [error] [MainThread]:         SELECT
[0m00:37:43.951566 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:37:43.951566 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:37:43.951566 [error] [MainThread]:         timestamp
[0m00:37:43.951566 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:37:43.951566 [error] [MainThread]:         timestamp
[0m00:37:43.951566 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:37:43.960070 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:37:43.960070 [error] [MainThread]:     
[0m00:37:43.961081 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:37:43.962355 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:37:43.962355 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:37:43.962355 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:37:43.962355 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:37:43.962355 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:37:43.962355 [error] [MainThread]:         GROUP BY
[0m00:37:43.962355 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:37:43.962355 [error] [MainThread]:     ),
[0m00:37:43.962355 [error] [MainThread]:     temp as (
[0m00:37:43.962355 [error] [MainThread]:         select 
[0m00:37:43.962355 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:37:43.962355 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:37:43.962355 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:37:43.962355 [error] [MainThread]:         ndb.s_address.`addr`,,ndb.s_name.`name`,from new_rows a
[0m00:37:43.962355 [error] [MainThread]:     -------------------------^^^
[0m00:37:43.962355 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:37:43.962355 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:37:43.962355 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:37:43.962355 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:37:43.962355 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:37:43.962355 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:37:43.962355 [error] [MainThread]:         ),
[0m00:37:43.962355 [error] [MainThread]:     pit AS (
[0m00:37:43.962355 [error] [MainThread]:         SELECT * FROM temp
[0m00:37:43.962355 [error] [MainThread]:     )
[0m00:37:43.962355 [error] [MainThread]:     
[0m00:37:43.962355 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:37:43.962355 [error] [MainThread]:     
[0m00:37:43.976740 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:37:43.976740 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:37:43.977749 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:37:43.978868 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:37:43.979266 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:37:43.979266 [error] [MainThread]:     	... 16 more
[0m00:37:43.979266 [error] [MainThread]:     
[0m00:37:43.979266 [info ] [MainThread]: 
[0m00:37:43.979266 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:37:43.979266 [debug] [MainThread]: Command `dbt run` failed at 00:37:43.979266 after 1.71 seconds
[0m00:37:43.979266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0C69DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0C9F8D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC0B7DBDF0>]}
[0m00:37:43.979266 [debug] [MainThread]: Flushing usage events
[0m00:38:52.470919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC7DDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCA4F2DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCA4F2BF0>]}


============================== 00:38:52.487742 | 69e7eb89-ccaa-4ffb-996c-c6615ad2126c ==============================
[0m00:38:52.487742 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:38:52.487742 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:38:52.599857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCA4F2E00>]}
[0m00:38:52.615572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCA892DA0>]}
[0m00:38:52.624082 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:38:52.647433 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:38:52.766239 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:38:52.766239 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:38:52.828504 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:38:52.939358 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:38:52.939358 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:38:52.969993 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:38:52.985628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCAD29D80>]}
[0m00:38:53.034021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC8138D60>]}
[0m00:38:53.034021 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:38:53.034021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC813A260>]}
[0m00:38:53.034021 [info ] [MainThread]: 
[0m00:38:53.034021 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:38:53.034021 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:38:53.057172 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:38:53.057172 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:38:53.058130 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:38:53.150915 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:38:53.150915 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:38:53.161015 [debug] [ThreadPool]: On list_schemas: Close
[0m00:38:53.170014 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:38:53.181708 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:38:53.181708 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:38:53.181708 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:38:53.181708 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:38:53.434131 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:38:53.434131 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:38:53.452364 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:38:53.452364 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:38:53.452364 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:38:53.466381 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:38:53.470751 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:38:53.470751 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:38:53.470751 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:38:53.470751 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:38:53.738090 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:38:53.739028 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:38:53.748538 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:38:53.748538 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:38:53.748538 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:38:53.767527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCABC7790>]}
[0m00:38:53.767527 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:38:53.770328 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:38:53.770328 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:38:53.770328 [info ] [MainThread]: 
[0m00:38:53.778220 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:38:53.778220 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:38:53.778220 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:38:53.778220 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:38:53.809301 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:38:53.809301 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:38:53.778220 => 00:38:53.809301
[0m00:38:53.809301 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:38:53.837792 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:38:53.838713 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:38:53.839698 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:38:53.839698 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:38:53.840696 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:38:53.883870 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:38:53.883870 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:38:53.883870 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:38:53.897883 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:38:53.897883 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:38:53.815287 => 00:38:53.897883
[0m00:38:53.897883 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:38:53.897883 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:38:53.897883 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:38:53.905999 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:38:53.914006 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69e7eb89-ccaa-4ffb-996c-c6615ad2126c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CCAC7F820>]}
[0m00:38:53.914898 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.13s]
[0m00:38:53.914898 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:38:53.914898 [debug] [MainThread]: On master: ROLLBACK
[0m00:38:53.914898 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:38:53.968747 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:38:53.968747 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:38:53.968747 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:38:53.968747 [debug] [MainThread]: On master: ROLLBACK
[0m00:38:53.968747 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:38:53.968747 [debug] [MainThread]: On master: Close
[0m00:38:53.968747 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:38:53.968747 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:38:53.968747 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:38:53.968747 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:38:53.980253 [info ] [MainThread]: 
[0m00:38:53.980253 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.93 seconds (0.93s).
[0m00:38:53.981942 [debug] [MainThread]: Command end result
[0m00:38:53.994954 [info ] [MainThread]: 
[0m00:38:53.995952 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:38:53.996998 [info ] [MainThread]: 
[0m00:38:53.997989 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:38:53.999193 [error] [MainThread]:   Database Error
[0m00:38:54.000200 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:38:54.001130 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:38:54.002098 [error] [MainThread]:     
[0m00:38:54.003157 [error] [MainThread]:     == SQL ==
[0m00:38:54.003157 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:38:54.005107 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:38:54.006106 [error] [MainThread]:       
[0m00:38:54.006106 [error] [MainThread]:       as
[0m00:38:54.007108 [error] [MainThread]:         
[0m00:38:54.008149 [error] [MainThread]:     
[0m00:38:54.009156 [error] [MainThread]:     
[0m00:38:54.010119 [error] [MainThread]:     
[0m00:38:54.011126 [error] [MainThread]:     
[0m00:38:54.012123 [error] [MainThread]:     
[0m00:38:54.012123 [error] [MainThread]:     
[0m00:38:54.013123 [error] [MainThread]:     
[0m00:38:54.013123 [error] [MainThread]:     
[0m00:38:54.014124 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:38:54.015123 [error] [MainThread]:     
[0m00:38:54.015123 [error] [MainThread]:     
[0m00:38:54.016125 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:38:54.017125 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:38:54.017125 [error] [MainThread]:     
[0m00:38:54.018124 [error] [MainThread]:     
[0m00:38:54.019123 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:38:54.020060 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:38:54.020060 [error] [MainThread]:     ),
[0m00:38:54.020060 [error] [MainThread]:     
[0m00:38:54.020060 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:38:54.020060 [error] [MainThread]:         SELECT
[0m00:38:54.020060 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:38:54.020060 [error] [MainThread]:             b.AS_OF_DATE
[0m00:38:54.029195 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:38:54.033003 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:38:54.034001 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:38:54.035005 [error] [MainThread]:     ),
[0m00:38:54.035561 [error] [MainThread]:     
[0m00:38:54.036210 [error] [MainThread]:     new_rows AS (
[0m00:38:54.036210 [error] [MainThread]:         SELECT
[0m00:38:54.036210 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:38:54.036210 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:38:54.036210 [error] [MainThread]:         timestamp
[0m00:38:54.036210 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:38:54.036210 [error] [MainThread]:         timestamp
[0m00:38:54.036210 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:38:54.036210 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:38:54.036210 [error] [MainThread]:     
[0m00:38:54.048779 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:38:54.049792 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:38:54.050790 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:38:54.052026 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:38:54.052026 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:38:54.052026 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:38:54.052026 [error] [MainThread]:         GROUP BY
[0m00:38:54.052026 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:38:54.052026 [error] [MainThread]:     ),
[0m00:38:54.052026 [error] [MainThread]:     temp as (
[0m00:38:54.052026 [error] [MainThread]:         select 
[0m00:38:54.052026 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:38:54.065482 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:38:54.066489 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:38:54.067490 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:38:54.069185 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:38:54.069185 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:38:54.069185 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:38:54.069185 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:38:54.069185 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:38:54.069185 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:38:54.069185 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:38:54.069185 [error] [MainThread]:         ),
[0m00:38:54.069185 [error] [MainThread]:     pit AS (
[0m00:38:54.069185 [error] [MainThread]:         SELECT * FROM temp
[0m00:38:54.069185 [error] [MainThread]:     )
[0m00:38:54.082241 [error] [MainThread]:     
[0m00:38:54.083309 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:38:54.084284 [error] [MainThread]:     
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:38:54.085857 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:38:54.085857 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:38:54.098805 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:38:54.099811 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:38:54.100811 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:38:54.102340 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:38:54.102340 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:38:54.102340 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:38:54.102340 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:38:54.102340 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:38:54.102340 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:38:54.102340 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:38:54.102340 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:38:54.102340 [error] [MainThread]:     
[0m00:38:54.102340 [error] [MainThread]:     == SQL ==
[0m00:38:54.115249 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:38:54.116257 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:38:54.117259 [error] [MainThread]:       
[0m00:38:54.118257 [error] [MainThread]:       as
[0m00:38:54.119060 [error] [MainThread]:         
[0m00:38:54.119060 [error] [MainThread]:     
[0m00:38:54.119060 [error] [MainThread]:     
[0m00:38:54.119060 [error] [MainThread]:     
[0m00:38:54.119060 [error] [MainThread]:     
[0m00:38:54.126075 [error] [MainThread]:     
[0m00:38:54.126075 [error] [MainThread]:     
[0m00:38:54.126075 [error] [MainThread]:     
[0m00:38:54.126075 [error] [MainThread]:     
[0m00:38:54.132004 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:38:54.133010 [error] [MainThread]:     
[0m00:38:54.134012 [error] [MainThread]:     
[0m00:38:54.135012 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:38:54.135965 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:38:54.135965 [error] [MainThread]:     
[0m00:38:54.135965 [error] [MainThread]:     
[0m00:38:54.135965 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:38:54.135965 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:38:54.135965 [error] [MainThread]:     ),
[0m00:38:54.135965 [error] [MainThread]:     
[0m00:38:54.135965 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:38:54.135965 [error] [MainThread]:         SELECT
[0m00:38:54.149707 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:38:54.150701 [error] [MainThread]:             b.AS_OF_DATE
[0m00:38:54.151704 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:38:54.153676 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:38:54.153676 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:38:54.153676 [error] [MainThread]:     ),
[0m00:38:54.153676 [error] [MainThread]:     
[0m00:38:54.153676 [error] [MainThread]:     new_rows AS (
[0m00:38:54.153676 [error] [MainThread]:         SELECT
[0m00:38:54.153676 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:38:54.153676 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:38:54.165487 [error] [MainThread]:         timestamp
[0m00:38:54.166495 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:38:54.167495 [error] [MainThread]:         timestamp
[0m00:38:54.169253 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:38:54.169253 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:38:54.169253 [error] [MainThread]:     
[0m00:38:54.169253 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:38:54.169253 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:38:54.169253 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:38:54.169253 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:38:54.169253 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:38:54.169253 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:38:54.169253 [error] [MainThread]:         GROUP BY
[0m00:38:54.169253 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:38:54.182190 [error] [MainThread]:     ),
[0m00:38:54.183198 [error] [MainThread]:     temp as (
[0m00:38:54.184197 [error] [MainThread]:         select 
[0m00:38:54.186196 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:38:54.187200 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:38:54.189197 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:38:54.189275 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:38:54.189275 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:38:54.189275 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:38:54.189275 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:38:54.189275 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:38:54.189275 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:38:54.189275 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:38:54.189275 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:38:54.189275 [error] [MainThread]:         ),
[0m00:38:54.202799 [error] [MainThread]:     pit AS (
[0m00:38:54.203785 [error] [MainThread]:         SELECT * FROM temp
[0m00:38:54.203785 [error] [MainThread]:     )
[0m00:38:54.203785 [error] [MainThread]:     
[0m00:38:54.203785 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:38:54.203785 [error] [MainThread]:     
[0m00:38:54.203785 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:38:54.203785 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:38:54.203785 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:38:54.203785 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:38:54.203785 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:38:54.215425 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:38:54.216434 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:38:54.217431 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:38:54.218538 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:38:54.226288 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:38:54.226288 [error] [MainThread]:     	... 16 more
[0m00:38:54.226288 [error] [MainThread]:     
[0m00:38:54.226288 [info ] [MainThread]: 
[0m00:38:54.226288 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:38:54.233137 [debug] [MainThread]: Command `dbt run` failed at 00:38:54.233137 after 1.76 seconds
[0m00:38:54.234112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC7DDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC8138D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018CC6F17DF0>]}
[0m00:38:54.234986 [debug] [MainThread]: Flushing usage events
[0m00:40:01.708046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1CE7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1F592D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1F592B30>]}


============================== 00:40:01.708046 | 2c0730f5-866c-4e88-b2a2-fea01537faea ==============================
[0m00:40:01.708046 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:40:01.708046 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:40:01.836152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1F592DA0>]}
[0m00:40:01.855266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1F93ACB0>]}
[0m00:40:01.861275 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:40:01.873595 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:40:02.004881 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:40:02.004881 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:40:02.061711 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:40:02.163877 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:40:02.163877 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:40:02.195234 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:40:02.210844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1FDD5D20>]}
[0m00:40:02.262026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1D1F2E90>]}
[0m00:40:02.262026 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:40:02.262026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1D1F34C0>]}
[0m00:40:02.262026 [info ] [MainThread]: 
[0m00:40:02.262026 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:40:02.262026 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:40:02.280081 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:40:02.280081 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:40:02.280081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:40:02.387014 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:02.387014 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:02.401037 [debug] [ThreadPool]: On list_schemas: Close
[0m00:40:02.409603 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:40:02.417212 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:02.421473 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:40:02.421473 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:40:02.421473 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:40:02.685749 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:02.685749 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:02.685749 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:40:02.695754 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:40:02.695754 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:40:02.698264 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:40:02.716329 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:02.716943 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:40:02.717515 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:40:02.718086 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:40:02.962948 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:02.962948 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:02.967515 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:40:02.967515 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:40:02.967515 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:40:02.984172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1FC67760>]}
[0m00:40:02.984172 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:02.984172 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:40:02.984172 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:40:02.984172 [info ] [MainThread]: 
[0m00:40:02.999184 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:40:03.000818 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:40:03.002824 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:40:03.002824 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:40:03.029035 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:40:03.030045 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:40:03.003825 => 00:40:03.030045
[0m00:40:03.031036 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:40:03.056951 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:40:03.056951 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:03.056951 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:40:03.056951 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ,ndb.s_name.`name`,
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:40:03.056951 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:40:03.109725 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \',\': extra input \',\'(line 59, pos 8)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ,ndb.s_name.`name`,\n--------^^^\n        from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \',\': extra input \',\'(line 59, pos 8)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ,ndb.s_name.`name`,\n--------^^^\n        from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:40:03.110785 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:40:03.111747 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ,ndb.s_name.`name`,
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:40:03.112685 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near ',': extra input ','(line 59, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ,ndb.s_name.`name`,
  --------^^^
          from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near ',': extra input ','(line 59, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ,ndb.s_name.`name`,
  --------^^^
          from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:40:03.113685 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:40:03.031036 => 00:40:03.113685
[0m00:40:03.113685 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:40:03.114682 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:40:03.114682 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:40:03.123367 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near ',': extra input ','(line 59, pos 8)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ,ndb.s_name.`name`,
    --------^^^
            from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near ',': extra input ','(line 59, pos 8)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ,ndb.s_name.`name`,
    --------^^^
            from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:40:03.123367 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c0730f5-866c-4e88-b2a2-fea01537faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1FD1F6D0>]}
[0m00:40:03.123367 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.12s]
[0m00:40:03.123367 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:40:03.123367 [debug] [MainThread]: On master: ROLLBACK
[0m00:40:03.123367 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:40:03.181556 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:40:03.181556 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:03.181556 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:40:03.181556 [debug] [MainThread]: On master: ROLLBACK
[0m00:40:03.181556 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:40:03.181556 [debug] [MainThread]: On master: Close
[0m00:40:03.192322 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:40:03.192322 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:40:03.192322 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:40:03.192322 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:40:03.194177 [info ] [MainThread]: 
[0m00:40:03.194177 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.93 seconds (0.93s).
[0m00:40:03.196692 [debug] [MainThread]: Command end result
[0m00:40:03.208857 [info ] [MainThread]: 
[0m00:40:03.210364 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:40:03.211588 [info ] [MainThread]: 
[0m00:40:03.211588 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:40:03.211588 [error] [MainThread]:   Database Error
[0m00:40:03.211588 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:40:03.211588 [error] [MainThread]:     Syntax error at or near ',': extra input ','(line 59, pos 8)
[0m00:40:03.211588 [error] [MainThread]:     
[0m00:40:03.211588 [error] [MainThread]:     == SQL ==
[0m00:40:03.211588 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:40:03.220361 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:40:03.221372 [error] [MainThread]:       
[0m00:40:03.222403 [error] [MainThread]:       as
[0m00:40:03.223432 [error] [MainThread]:         
[0m00:40:03.224407 [error] [MainThread]:     
[0m00:40:03.225397 [error] [MainThread]:     
[0m00:40:03.226394 [error] [MainThread]:     
[0m00:40:03.227372 [error] [MainThread]:     
[0m00:40:03.228370 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.229884 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:40:03.229884 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:40:03.229884 [error] [MainThread]:     
[0m00:40:03.236980 [error] [MainThread]:     
[0m00:40:03.236980 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:40:03.237989 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:40:03.238986 [error] [MainThread]:     ),
[0m00:40:03.239988 [error] [MainThread]:     
[0m00:40:03.240989 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:40:03.242078 [error] [MainThread]:         SELECT
[0m00:40:03.242990 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:03.243534 [error] [MainThread]:             b.AS_OF_DATE
[0m00:40:03.243534 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:40:03.243534 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:40:03.247544 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:40:03.247544 [error] [MainThread]:     ),
[0m00:40:03.247544 [error] [MainThread]:     
[0m00:40:03.247544 [error] [MainThread]:     new_rows AS (
[0m00:40:03.247544 [error] [MainThread]:         SELECT
[0m00:40:03.247544 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:03.247544 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:03.247544 [error] [MainThread]:         timestamp
[0m00:40:03.255752 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:03.256769 [error] [MainThread]:         timestamp
[0m00:40:03.257327 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:40:03.257327 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:40:03.257327 [error] [MainThread]:     
[0m00:40:03.257327 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:03.257327 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:03.257327 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:03.257327 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:03.263338 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:03.264907 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:03.264907 [error] [MainThread]:         GROUP BY
[0m00:40:03.264907 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:40:03.264907 [error] [MainThread]:     ),
[0m00:40:03.264907 [error] [MainThread]:     temp as (
[0m00:40:03.264907 [error] [MainThread]:         select 
[0m00:40:03.264907 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:40:03.270448 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:40:03.271455 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:40:03.272950 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:40:03.273712 [error] [MainThread]:             ,ndb.s_name.`name`,
[0m00:40:03.273712 [error] [MainThread]:     --------^^^
[0m00:40:03.273712 [error] [MainThread]:             from new_rows a
[0m00:40:03.273712 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:03.273712 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:03.273712 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:40:03.273712 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:03.273712 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:03.273712 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:40:03.273712 [error] [MainThread]:         ),
[0m00:40:03.273712 [error] [MainThread]:     pit AS (
[0m00:40:03.283721 [error] [MainThread]:         SELECT * FROM temp
[0m00:40:03.283721 [error] [MainThread]:     )
[0m00:40:03.283721 [error] [MainThread]:     
[0m00:40:03.283721 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:40:03.283721 [error] [MainThread]:     
[0m00:40:03.286967 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:40:03.286967 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:40:03.287976 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:40:03.289109 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:40:03.289649 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:40:03.289649 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:40:03.292238 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:40:03.292238 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:40:03.292238 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:40:03.292238 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:40:03.292238 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:40:03.292238 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:40:03.301079 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:40:03.304786 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:40:03.305787 [error] [MainThread]:     Syntax error at or near ',': extra input ','(line 59, pos 8)
[0m00:40:03.306788 [error] [MainThread]:     
[0m00:40:03.307620 [error] [MainThread]:     == SQL ==
[0m00:40:03.308550 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:40:03.308550 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:40:03.308550 [error] [MainThread]:       
[0m00:40:03.308550 [error] [MainThread]:       as
[0m00:40:03.308550 [error] [MainThread]:         
[0m00:40:03.308550 [error] [MainThread]:     
[0m00:40:03.308550 [error] [MainThread]:     
[0m00:40:03.308550 [error] [MainThread]:     
[0m00:40:03.308550 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:40:03.313567 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     
[0m00:40:03.313567 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:40:03.313567 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:40:03.313567 [error] [MainThread]:     ),
[0m00:40:03.320347 [error] [MainThread]:     
[0m00:40:03.320347 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:40:03.321355 [error] [MainThread]:         SELECT
[0m00:40:03.321862 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:03.322875 [error] [MainThread]:             b.AS_OF_DATE
[0m00:40:03.323239 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:40:03.323239 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:40:03.323239 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:40:03.323239 [error] [MainThread]:     ),
[0m00:40:03.323239 [error] [MainThread]:     
[0m00:40:03.323239 [error] [MainThread]:     new_rows AS (
[0m00:40:03.323239 [error] [MainThread]:         SELECT
[0m00:40:03.323239 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:03.323239 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:03.323239 [error] [MainThread]:         timestamp
[0m00:40:03.323239 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:03.323239 [error] [MainThread]:         timestamp
[0m00:40:03.323239 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:40:03.323239 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:40:03.323239 [error] [MainThread]:     
[0m00:40:03.323239 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:03.323239 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:03.323239 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:03.323239 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:03.323239 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:03.336997 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:03.336997 [error] [MainThread]:         GROUP BY
[0m00:40:03.339008 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:40:03.340425 [error] [MainThread]:     ),
[0m00:40:03.340425 [error] [MainThread]:     temp as (
[0m00:40:03.341939 [error] [MainThread]:         select 
[0m00:40:03.341939 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:40:03.341939 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:40:03.341939 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:40:03.341939 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:40:03.341939 [error] [MainThread]:             ,ndb.s_name.`name`,
[0m00:40:03.341939 [error] [MainThread]:     --------^^^
[0m00:40:03.341939 [error] [MainThread]:             from new_rows a
[0m00:40:03.341939 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:03.341939 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:03.341939 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:40:03.341939 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:03.341939 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:03.341939 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:40:03.341939 [error] [MainThread]:         ),
[0m00:40:03.341939 [error] [MainThread]:     pit AS (
[0m00:40:03.341939 [error] [MainThread]:         SELECT * FROM temp
[0m00:40:03.341939 [error] [MainThread]:     )
[0m00:40:03.353691 [error] [MainThread]:     
[0m00:40:03.353691 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:40:03.354699 [error] [MainThread]:     
[0m00:40:03.355699 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:40:03.356595 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:40:03.356595 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:40:03.356595 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:40:03.360606 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:40:03.360606 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:40:03.360606 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:40:03.360606 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:40:03.363624 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:40:03.363624 [error] [MainThread]:     	... 16 more
[0m00:40:03.363624 [error] [MainThread]:     
[0m00:40:03.370631 [info ] [MainThread]: 
[0m00:40:03.371352 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:40:03.373250 [debug] [MainThread]: Command `dbt run` failed at 00:40:03.372349 after 1.68 seconds
[0m00:40:03.373250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1CE7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1D1F2E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016F1BFBBB20>]}
[0m00:40:03.373250 [debug] [MainThread]: Flushing usage events
[0m00:40:31.564967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024231AADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242341C2D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242341C2BC0>]}


============================== 00:40:31.580613 | fde58039-15cd-4dec-8e40-4478c0a39ed4 ==============================
[0m00:40:31.580613 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:40:31.580613 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:40:31.688181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242341C2DD0>]}
[0m00:40:31.703812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024234566F20>]}
[0m00:40:31.703812 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:40:31.740459 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:40:31.836952 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:40:31.836952 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:40:31.900490 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:40:32.020075 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:40:32.020075 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:40:32.041890 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:40:32.057436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242349F9D50>]}
[0m00:40:32.104375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024231E09FF0>]}
[0m00:40:32.104375 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:40:32.104375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024231E0A980>]}
[0m00:40:32.104375 [info ] [MainThread]: 
[0m00:40:32.104375 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:40:32.121547 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:40:32.133098 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:40:32.134062 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:40:32.134062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:40:32.225208 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:32.227241 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:32.235711 [debug] [ThreadPool]: On list_schemas: Close
[0m00:40:32.240358 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m00:40:32.240358 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:32.240358 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:40:32.240358 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:40:32.240358 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:40:32.555617 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:32.555617 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:32.558125 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:40:32.558125 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:40:32.558125 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:40:32.575470 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m00:40:32.590077 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:32.590077 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:40:32.590077 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:40:32.590077 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:40:32.804618 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:40:32.804618 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:40:32.818787 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:40:32.821379 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:40:32.821379 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:40:32.834644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002423488FA30>]}
[0m00:40:32.834644 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:32.834644 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:40:32.834644 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:40:32.834644 [info ] [MainThread]: 
[0m00:40:32.834644 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:40:32.834644 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:40:32.834644 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:40:32.845074 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:40:32.862693 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:40:32.873741 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:40:32.845074 => 00:40:32.872700
[0m00:40:32.873741 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:40:32.891503 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:40:32.891503 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:32.891503 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:40:32.891503 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:40:32.891503 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:40:32.940604 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 58, pos 57)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a\n---------------------------------------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:40:32.940604 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:40:32.940604 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:40:32.940604 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 58, pos 57)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
  ---------------------------------------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:40:32.940604 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:40:32.873741 => 00:40:32.940604
[0m00:40:32.940604 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:40:32.940604 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:40:32.950614 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:40:32.959871 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 58, pos 57)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
    ---------------------------------------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:40:32.959871 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fde58039-15cd-4dec-8e40-4478c0a39ed4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002423494F6A0>]}
[0m00:40:32.968377 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.13s]
[0m00:40:32.968377 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:40:32.972168 [debug] [MainThread]: On master: ROLLBACK
[0m00:40:32.973122 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:40:33.023607 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:40:33.024604 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:40:33.024818 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:40:33.025525 [debug] [MainThread]: On master: ROLLBACK
[0m00:40:33.025525 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:40:33.025525 [debug] [MainThread]: On master: Close
[0m00:40:33.032090 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:40:33.034650 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:40:33.034650 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m00:40:33.034650 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:40:33.034650 [info ] [MainThread]: 
[0m00:40:33.034650 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.93 seconds (0.93s).
[0m00:40:33.034650 [debug] [MainThread]: Command end result
[0m00:40:33.049737 [info ] [MainThread]: 
[0m00:40:33.050763 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:40:33.052060 [info ] [MainThread]: 
[0m00:40:33.053095 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:40:33.054066 [error] [MainThread]:   Database Error
[0m00:40:33.055066 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:40:33.056070 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:40:33.057066 [error] [MainThread]:     
[0m00:40:33.057987 [error] [MainThread]:     == SQL ==
[0m00:40:33.059338 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:40:33.059338 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:40:33.060587 [error] [MainThread]:       
[0m00:40:33.060587 [error] [MainThread]:       as
[0m00:40:33.060587 [error] [MainThread]:         
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     
[0m00:40:33.060587 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:40:33.069108 [error] [MainThread]:     
[0m00:40:33.069108 [error] [MainThread]:     
[0m00:40:33.069108 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:40:33.069108 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:40:33.069108 [error] [MainThread]:     
[0m00:40:33.072555 [error] [MainThread]:     
[0m00:40:33.073273 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:40:33.073273 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:40:33.073273 [error] [MainThread]:     ),
[0m00:40:33.073273 [error] [MainThread]:     
[0m00:40:33.073273 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:40:33.073273 [error] [MainThread]:         SELECT
[0m00:40:33.073273 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:33.073273 [error] [MainThread]:             b.AS_OF_DATE
[0m00:40:33.073273 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:40:33.073273 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:40:33.073273 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:40:33.073273 [error] [MainThread]:     ),
[0m00:40:33.073273 [error] [MainThread]:     
[0m00:40:33.073273 [error] [MainThread]:     new_rows AS (
[0m00:40:33.073273 [error] [MainThread]:         SELECT
[0m00:40:33.073273 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:33.073273 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:33.073273 [error] [MainThread]:         timestamp
[0m00:40:33.073273 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:33.073273 [error] [MainThread]:         timestamp
[0m00:40:33.073273 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:40:33.088280 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:40:33.089206 [error] [MainThread]:     
[0m00:40:33.089206 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:33.090436 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:33.090436 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:33.090436 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:33.090436 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:33.090436 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:33.090436 [error] [MainThread]:         GROUP BY
[0m00:40:33.090436 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:40:33.090436 [error] [MainThread]:     ),
[0m00:40:33.090436 [error] [MainThread]:     temp as (
[0m00:40:33.090436 [error] [MainThread]:         select 
[0m00:40:33.090436 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:40:33.090436 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:40:33.090436 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:40:33.090436 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:40:33.090436 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:40:33.090436 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:33.102616 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:33.102616 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:40:33.102616 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:33.102616 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:33.102616 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:40:33.102616 [error] [MainThread]:         ),
[0m00:40:33.105869 [error] [MainThread]:     pit AS (
[0m00:40:33.106643 [error] [MainThread]:         SELECT * FROM temp
[0m00:40:33.106643 [error] [MainThread]:     )
[0m00:40:33.106643 [error] [MainThread]:     
[0m00:40:33.106643 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:40:33.106643 [error] [MainThread]:     
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:40:33.106643 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:40:33.106643 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:40:33.106643 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:40:33.106643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:40:33.106643 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:40:33.106643 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:40:33.106643 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:40:33.121613 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:40:33.122496 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:40:33.122496 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:40:33.123334 [error] [MainThread]:     Syntax error at or near 'a'(line 58, pos 57)
[0m00:40:33.123334 [error] [MainThread]:     
[0m00:40:33.123334 [error] [MainThread]:     == SQL ==
[0m00:40:33.126447 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:40:33.126447 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:40:33.126447 [error] [MainThread]:       
[0m00:40:33.126447 [error] [MainThread]:       as
[0m00:40:33.126447 [error] [MainThread]:         
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.126447 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:40:33.126447 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:40:33.126447 [error] [MainThread]:     
[0m00:40:33.138455 [error] [MainThread]:     
[0m00:40:33.139189 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:40:33.140265 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:40:33.140265 [error] [MainThread]:     ),
[0m00:40:33.140265 [error] [MainThread]:     
[0m00:40:33.140265 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:40:33.140265 [error] [MainThread]:         SELECT
[0m00:40:33.140265 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:33.140265 [error] [MainThread]:             b.AS_OF_DATE
[0m00:40:33.140265 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:40:33.140265 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:40:33.140265 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:40:33.140265 [error] [MainThread]:     ),
[0m00:40:33.140265 [error] [MainThread]:     
[0m00:40:33.140265 [error] [MainThread]:     new_rows AS (
[0m00:40:33.140265 [error] [MainThread]:         SELECT
[0m00:40:33.140265 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:40:33.140265 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:33.140265 [error] [MainThread]:         timestamp
[0m00:40:33.140265 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:40:33.140265 [error] [MainThread]:         timestamp
[0m00:40:33.140265 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:40:33.140265 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:40:33.140265 [error] [MainThread]:     
[0m00:40:33.140265 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:33.140265 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:33.140265 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:33.155303 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:33.155840 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:33.156700 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:40:33.156700 [error] [MainThread]:         GROUP BY
[0m00:40:33.156700 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:40:33.156700 [error] [MainThread]:     ),
[0m00:40:33.156700 [error] [MainThread]:     temp as (
[0m00:40:33.156700 [error] [MainThread]:         select 
[0m00:40:33.156700 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:40:33.156700 [error] [MainThread]:         a.AS_OF_DATE,
[0m00:40:33.156700 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:40:33.156700 [error] [MainThread]:         ndb.s_address.`addr`,ndb.s_name.`name`,from new_rows a
[0m00:40:33.156700 [error] [MainThread]:     ---------------------------------------------------------^^^
[0m00:40:33.156700 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:40:33.156700 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:40:33.156700 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:40:33.156700 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:40:33.156700 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:40:33.156700 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:40:33.156700 [error] [MainThread]:         ),
[0m00:40:33.156700 [error] [MainThread]:     pit AS (
[0m00:40:33.156700 [error] [MainThread]:         SELECT * FROM temp
[0m00:40:33.156700 [error] [MainThread]:     )
[0m00:40:33.156700 [error] [MainThread]:     
[0m00:40:33.156700 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:40:33.156700 [error] [MainThread]:     
[0m00:40:33.156700 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:40:33.171868 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:40:33.172543 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:40:33.172543 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:40:33.173334 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:40:33.173334 [error] [MainThread]:     	... 16 more
[0m00:40:33.173334 [error] [MainThread]:     
[0m00:40:33.173334 [info ] [MainThread]: 
[0m00:40:33.173334 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:40:33.173334 [debug] [MainThread]: Command `dbt run` failed at 00:40:33.173334 after 1.62 seconds
[0m00:40:33.173334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024231AADB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024231E09FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024230BEBB50>]}
[0m00:40:33.173334 [debug] [MainThread]: Flushing usage events
[0m00:41:59.177977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D04CDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D073F2DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D073F2B90>]}


============================== 00:41:59.177977 | 36883a0f-ee39-48df-b807-90effbe95b9d ==============================
[0m00:41:59.177977 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:41:59.193615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:41:59.311561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D073F2E00>]}
[0m00:41:59.311561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D077AADA0>]}
[0m00:41:59.311561 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:41:59.356520 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:41:59.499778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:41:59.499778 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:41:59.555767 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:41:59.666071 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:41:59.666071 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:41:59.700167 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:41:59.729448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D07C41D80>]}
[0m00:41:59.760710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D05038D60>]}
[0m00:41:59.776358 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:41:59.776358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D0503A260>]}
[0m00:41:59.776358 [info ] [MainThread]: 
[0m00:41:59.776358 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:41:59.776358 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:41:59.793743 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:41:59.794839 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:41:59.794839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:41:59.892622 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:41:59.892622 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:41:59.900207 [debug] [ThreadPool]: On list_schemas: Close
[0m00:41:59.912866 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m00:41:59.919214 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:41:59.919214 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:41:59.919214 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:41:59.919214 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:42:00.161774 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:42:00.161774 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:42:00.172427 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:42:00.172427 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:42:00.172427 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:42:00.186435 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m00:42:00.192943 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:42:00.192943 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:42:00.192943 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:42:00.192943 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:42:00.435866 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:42:00.435866 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:42:00.444984 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:42:00.444984 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:42:00.444984 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:42:00.462416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D07ACF940>]}
[0m00:42:00.462416 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:42:00.463965 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:42:00.465009 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:42:00.466016 [info ] [MainThread]: 
[0m00:42:00.471971 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:42:00.472928 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:42:00.474975 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:42:00.475974 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:42:00.502484 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:42:00.502484 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:42:00.475974 => 00:42:00.502484
[0m00:42:00.502484 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:42:00.533324 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:42:00.535328 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:42:00.536336 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:42:00.536336 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:42:00.537327 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:42:00.587274 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'.\'(line 58, pos 27)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`ndb.s_name.`name`from new_rows a\n---------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'.\'(line 58, pos 27)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`ndb.s_name.`name`from new_rows a\n---------------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:42:00.595848 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:42:00.595848 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:42:00.595848 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near '.'(line 58, pos 27)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
  ---------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near '.'(line 58, pos 27)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
  ---------------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:42:00.595848 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:42:00.502484 => 00:42:00.595848
[0m00:42:00.595848 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:42:00.595848 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:42:00.595848 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:42:00.611079 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near '.'(line 58, pos 27)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
    ---------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near '.'(line 58, pos 27)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
    ---------------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:42:00.611079 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '36883a0f-ee39-48df-b807-90effbe95b9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D07B8F6D0>]}
[0m00:42:00.611079 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.14s]
[0m00:42:00.611079 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:42:00.611079 [debug] [MainThread]: On master: ROLLBACK
[0m00:42:00.611079 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:42:00.664924 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:42:00.664924 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:42:00.664924 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:42:00.664924 [debug] [MainThread]: On master: ROLLBACK
[0m00:42:00.664924 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:42:00.664924 [debug] [MainThread]: On master: Close
[0m00:42:00.669940 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:42:00.669940 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:42:00.669940 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m00:42:00.669940 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:42:00.669940 [info ] [MainThread]: 
[0m00:42:00.669940 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.89 seconds (0.89s).
[0m00:42:00.682264 [debug] [MainThread]: Command end result
[0m00:42:00.694030 [info ] [MainThread]: 
[0m00:42:00.695003 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:42:00.695952 [info ] [MainThread]: 
[0m00:42:00.697949 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:42:00.698986 [error] [MainThread]:   Database Error
[0m00:42:00.698986 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:42:00.700424 [error] [MainThread]:     Syntax error at or near '.'(line 58, pos 27)
[0m00:42:00.701933 [error] [MainThread]:     
[0m00:42:00.702940 [error] [MainThread]:     == SQL ==
[0m00:42:00.702940 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:42:00.703942 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:42:00.705482 [error] [MainThread]:       
[0m00:42:00.706492 [error] [MainThread]:       as
[0m00:42:00.707490 [error] [MainThread]:         
[0m00:42:00.707490 [error] [MainThread]:     
[0m00:42:00.708490 [error] [MainThread]:     
[0m00:42:00.709490 [error] [MainThread]:     
[0m00:42:00.710541 [error] [MainThread]:     
[0m00:42:00.711492 [error] [MainThread]:     
[0m00:42:00.712495 [error] [MainThread]:     
[0m00:42:00.713521 [error] [MainThread]:     
[0m00:42:00.714547 [error] [MainThread]:     
[0m00:42:00.714547 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:42:00.716077 [error] [MainThread]:     
[0m00:42:00.716967 [error] [MainThread]:     
[0m00:42:00.716967 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:42:00.718378 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:42:00.718378 [error] [MainThread]:     
[0m00:42:00.718378 [error] [MainThread]:     
[0m00:42:00.720898 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:42:00.720898 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:42:00.720898 [error] [MainThread]:     ),
[0m00:42:00.720898 [error] [MainThread]:     
[0m00:42:00.720898 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:42:00.720898 [error] [MainThread]:         SELECT
[0m00:42:00.720898 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:42:00.720898 [error] [MainThread]:             b.AS_OF_DATE
[0m00:42:00.727431 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:42:00.727431 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:42:00.729064 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:42:00.729064 [error] [MainThread]:     ),
[0m00:42:00.730071 [error] [MainThread]:     
[0m00:42:00.731096 [error] [MainThread]:     new_rows AS (
[0m00:42:00.732074 [error] [MainThread]:         SELECT
[0m00:42:00.732074 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:42:00.732074 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:42:00.732074 [error] [MainThread]:         timestamp
[0m00:42:00.732074 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:42:00.732074 [error] [MainThread]:         timestamp
[0m00:42:00.732074 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:42:00.732074 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:42:00.732074 [error] [MainThread]:     
[0m00:42:00.732074 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:42:00.732074 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:42:00.732074 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:42:00.732074 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:42:00.732074 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:42:00.732074 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:42:00.732074 [error] [MainThread]:         GROUP BY
[0m00:42:00.732074 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:42:00.732074 [error] [MainThread]:     ),
[0m00:42:00.745712 [error] [MainThread]:     temp as (
[0m00:42:00.746723 [error] [MainThread]:         select 
[0m00:42:00.747717 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:42:00.748718 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:42:00.750036 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:42:00.751084 [error] [MainThread]:         ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
[0m00:42:00.752122 [error] [MainThread]:     ---------------------------^^^
[0m00:42:00.752122 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:42:00.752122 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:42:00.752122 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:42:00.752122 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:42:00.752122 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:42:00.752122 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:42:00.752122 [error] [MainThread]:         ),
[0m00:42:00.752122 [error] [MainThread]:     pit AS (
[0m00:42:00.752122 [error] [MainThread]:         SELECT * FROM temp
[0m00:42:00.752122 [error] [MainThread]:     )
[0m00:42:00.762287 [error] [MainThread]:     
[0m00:42:00.763336 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:42:00.764313 [error] [MainThread]:     
[0m00:42:00.765294 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:42:00.766112 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:42:00.766112 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:42:00.766112 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:42:00.766112 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:42:00.766112 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:42:00.766112 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:42:00.773550 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:42:00.773550 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:42:00.773550 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:42:00.773550 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:42:00.773550 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:42:00.779048 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:42:00.780059 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:42:00.781059 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:42:00.782034 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:42:00.782034 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:42:00.782034 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:42:00.782034 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:42:00.782034 [error] [MainThread]:     Syntax error at or near '.'(line 58, pos 27)
[0m00:42:00.782034 [error] [MainThread]:     
[0m00:42:00.782034 [error] [MainThread]:     == SQL ==
[0m00:42:00.782034 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:42:00.782034 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:42:00.782034 [error] [MainThread]:       
[0m00:42:00.782034 [error] [MainThread]:       as
[0m00:42:00.782034 [error] [MainThread]:         
[0m00:42:00.795739 [error] [MainThread]:     
[0m00:42:00.796775 [error] [MainThread]:     
[0m00:42:00.797770 [error] [MainThread]:     
[0m00:42:00.798544 [error] [MainThread]:     
[0m00:42:00.798544 [error] [MainThread]:     
[0m00:42:00.798544 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:42:00.802053 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     
[0m00:42:00.802053 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:42:00.812394 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:42:00.813401 [error] [MainThread]:     ),
[0m00:42:00.813401 [error] [MainThread]:     
[0m00:42:00.815474 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:42:00.815474 [error] [MainThread]:         SELECT
[0m00:42:00.815474 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:42:00.815474 [error] [MainThread]:             b.AS_OF_DATE
[0m00:42:00.815474 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:42:00.815474 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:42:00.815474 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:42:00.815474 [error] [MainThread]:     ),
[0m00:42:00.815474 [error] [MainThread]:     
[0m00:42:00.815474 [error] [MainThread]:     new_rows AS (
[0m00:42:00.815474 [error] [MainThread]:         SELECT
[0m00:42:00.815474 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:42:00.815474 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:42:00.829251 [error] [MainThread]:         timestamp
[0m00:42:00.830260 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:42:00.831258 [error] [MainThread]:         timestamp
[0m00:42:00.831985 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:42:00.831985 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:42:00.831985 [error] [MainThread]:     
[0m00:42:00.836773 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:42:00.837942 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:42:00.839022 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:42:00.840149 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:42:00.840149 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:42:00.840149 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:42:00.840149 [error] [MainThread]:         GROUP BY
[0m00:42:00.840149 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:42:00.845826 [error] [MainThread]:     ),
[0m00:42:00.846837 [error] [MainThread]:     temp as (
[0m00:42:00.847832 [error] [MainThread]:         select 
[0m00:42:00.849359 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:42:00.849359 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:42:00.849359 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:42:00.849359 [error] [MainThread]:         ndb.s_address.`addr`ndb.s_name.`name`from new_rows a
[0m00:42:00.849359 [error] [MainThread]:     ---------------------------^^^
[0m00:42:00.849359 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:42:00.849359 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:42:00.849359 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:42:00.849359 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:42:00.849359 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:42:00.849359 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:42:00.849359 [error] [MainThread]:         ),
[0m00:42:00.862328 [error] [MainThread]:     pit AS (
[0m00:42:00.863334 [error] [MainThread]:         SELECT * FROM temp
[0m00:42:00.863334 [error] [MainThread]:     )
[0m00:42:00.864314 [error] [MainThread]:     
[0m00:42:00.865072 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:42:00.865072 [error] [MainThread]:     
[0m00:42:00.865072 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:42:00.869425 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:42:00.875476 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:42:00.875476 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:42:00.875476 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:42:00.879134 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:42:00.880135 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:42:00.881815 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:42:00.883616 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:42:00.883616 [error] [MainThread]:     	... 16 more
[0m00:42:00.883616 [error] [MainThread]:     
[0m00:42:00.886383 [info ] [MainThread]: 
[0m00:42:00.886383 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:42:00.886383 [debug] [MainThread]: Command `dbt run` failed at 00:42:00.886383 after 1.72 seconds
[0m00:42:00.886383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D04CDDAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D05038D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D03E0BDF0>]}
[0m00:42:00.886383 [debug] [MainThread]: Flushing usage events
[0m00:43:35.214349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9D9B2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC242CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC242B30>]}


============================== 00:43:35.214349 | 059ab55d-1db0-4dcc-b5b9-c67d88c30293 ==============================
[0m00:43:35.214349 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:43:35.214349 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:43:35.336324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '059ab55d-1db0-4dcc-b5b9-c67d88c30293', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC242D40>]}
[0m00:43:35.351954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '059ab55d-1db0-4dcc-b5b9-c67d88c30293', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC5F2F50>]}
[0m00:43:35.351954 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:43:35.369521 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:43:35.486085 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:43:35.486085 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:43:35.517864 [error] [MainThread]: Encountered an error:
Compilation Error
  unexpected '%'
    line 211
      {{% ',' if not loop.last %}}
[0m00:43:35.517864 [debug] [MainThread]: Command `dbt run` failed at 00:43:35.517864 after 0.31 seconds
[0m00:43:35.517864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9D9B2DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC8EB9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9DC8EB9D0>]}
[0m00:43:35.517864 [debug] [MainThread]: Flushing usage events
[0m00:43:52.818019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001621725DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016219972DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016219972BF0>]}


============================== 00:43:52.824595 | 4a66ad11-b489-4cca-8cce-7dcbe57614fb ==============================
[0m00:43:52.824595 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:43:52.824595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:43:52.953290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a66ad11-b489-4cca-8cce-7dcbe57614fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016219972E00>]}
[0m00:43:52.953290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a66ad11-b489-4cca-8cce-7dcbe57614fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016219D22DA0>]}
[0m00:43:52.953290 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:43:52.988466 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:43:53.088265 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:43:53.088265 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:43:53.123850 [error] [MainThread]: Encountered an error:
Compilation Error
  unexpected '%'
    line 211
      {{% ',' if not loop.last -%}}
[0m00:43:53.125857 [debug] [MainThread]: Command `dbt run` failed at 00:43:53.125857 after 0.32 seconds
[0m00:43:53.126861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001621725DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001621A01BA60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001621A01BA90>]}
[0m00:43:53.126861 [debug] [MainThread]: Flushing usage events
[0m00:44:48.704013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FC929DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCB9B2D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCB9B2B30>]}


============================== 00:44:48.704013 | 762ddf86-c86b-47c3-abb5-71c7dbd226f2 ==============================
[0m00:44:48.704013 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:44:48.704013 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:44:48.808918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '762ddf86-c86b-47c3-abb5-71c7dbd226f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCB9B2DA0>]}
[0m00:44:48.824549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '762ddf86-c86b-47c3-abb5-71c7dbd226f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCBD62CB0>]}
[0m00:44:48.824549 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:44:48.851464 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:44:48.961713 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:44:48.962705 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:44:48.975813 [error] [MainThread]: Encountered an error:
Compilation Error
  unexpected '%'
    line 211
      {{% if not loop.last %}}
[0m00:44:48.991472 [debug] [MainThread]: Command `dbt run` failed at 00:44:48.975813 after 0.31 seconds
[0m00:44:48.992176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FC929DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCC05BA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FCC05BA30>]}
[0m00:44:48.992176 [debug] [MainThread]: Flushing usage events
[0m00:45:30.519913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B1FE7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22592D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22592B60>]}


============================== 00:45:30.538266 | 7fafd920-f4a2-4a7f-8462-e8f7de405c0e ==============================
[0m00:45:30.538266 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:45:30.540365 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:45:30.660990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7fafd920-f4a2-4a7f-8462-e8f7de405c0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22592D70>]}
[0m00:45:30.660990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7fafd920-f4a2-4a7f-8462-e8f7de405c0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22942F50>]}
[0m00:45:30.660990 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:45:30.700042 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:45:30.802251 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:45:30.802251 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:45:30.825579 [error] [MainThread]: Encountered an error:
Compilation Error
  unexpected '%'
    line 211
      {{% ',' if not loop.last else '' %}}
[0m00:45:30.825579 [debug] [MainThread]: Command `dbt run` failed at 00:45:30.825579 after 0.32 seconds
[0m00:45:30.825579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B1FE7DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22C3B9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B22C3BA00>]}
[0m00:45:30.825579 [debug] [MainThread]: Flushing usage events
[0m00:46:31.301272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F77FCFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7024D2D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7024D2BC0>]}


============================== 00:46:31.307014 | 7ef8800e-b098-4dde-b788-2ab56e200c6a ==============================
[0m00:46:31.307014 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:46:31.308018 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:46:31.416802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7024D2DD0>]}
[0m00:46:31.432512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F702976F20>]}
[0m00:46:31.432512 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:46:31.451335 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:46:31.548822 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:46:31.548822 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:46:31.617157 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:46:31.726416 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:46:31.726416 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:46:31.757838 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:46:31.773387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F702E11D50>]}
[0m00:46:31.836637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F700139FF0>]}
[0m00:46:31.836637 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:46:31.836637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F70013A980>]}
[0m00:46:31.836637 [info ] [MainThread]: 
[0m00:46:31.836637 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:46:31.836637 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:46:31.854459 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:46:31.855461 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:46:31.855461 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:31.959088 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:31.965594 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:31.966602 [debug] [ThreadPool]: On list_schemas: Close
[0m00:46:31.981610 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:46:31.991706 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:31.991706 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:46:31.991706 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:46:31.991706 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:32.236486 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:32.236486 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:32.244884 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:46:32.244884 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:46:32.244884 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:46:32.261771 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:46:32.261771 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:32.261771 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:46:32.261771 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:46:32.261771 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:46:32.489036 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:32.489036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:32.499107 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:46:32.499107 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:46:32.499107 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:46:32.514276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F702C9FA30>]}
[0m00:46:32.514276 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:32.514276 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:32.518340 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:46:32.518340 [info ] [MainThread]: 
[0m00:46:32.524897 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:46:32.524897 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:46:32.524897 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:46:32.524897 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:46:32.554664 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:46:32.554664 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:46:32.524897 => 00:46:32.554664
[0m00:46:32.554664 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:46:32.568403 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:46:32.582884 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:32.582884 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:46:32.583908 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`
            
        ndb.s_name.`name`
            
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:46:32.585171 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:46:32.635177 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'.\'(line 60, pos 11)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`\n            \n        ndb.s_name.`name`\n-----------^^^\n            \n        from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'.\'(line 60, pos 11)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`\n            \n        ndb.s_name.`name`\n-----------^^^\n            \n        from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:46:32.636188 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:46:32.637203 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`
            
        ndb.s_name.`name`
            
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:46:32.638184 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near '.'(line 60, pos 11)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`
              
          ndb.s_name.`name`
  -----------^^^
              
          from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near '.'(line 60, pos 11)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`
              
          ndb.s_name.`name`
  -----------^^^
              
          from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:46:32.639185 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:46:32.554664 => 00:46:32.638184
[0m00:46:32.639185 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:46:32.640186 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:46:32.640186 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:46:32.651833 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near '.'(line 60, pos 11)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`
                
            ndb.s_name.`name`
    -----------^^^
                
            from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near '.'(line 60, pos 11)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`
                
            ndb.s_name.`name`
    -----------^^^
                
            from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:46:32.651833 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7ef8800e-b098-4dde-b788-2ab56e200c6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F702D5F7F0>]}
[0m00:46:32.651833 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.13s]
[0m00:46:32.651833 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:46:32.651833 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:32.651833 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:46:32.702145 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:46:32.702669 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:32.703190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:32.703704 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:32.704241 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:46:32.704768 [debug] [MainThread]: On master: Close
[0m00:46:32.713006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:46:32.713006 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:46:32.713006 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:46:32.713006 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:46:32.713006 [info ] [MainThread]: 
[0m00:46:32.715774 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.88 seconds (0.88s).
[0m00:46:32.716782 [debug] [MainThread]: Command end result
[0m00:46:32.718160 [info ] [MainThread]: 
[0m00:46:32.718160 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:46:32.732379 [info ] [MainThread]: 
[0m00:46:32.733389 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:46:32.735391 [error] [MainThread]:   Database Error
[0m00:46:32.735391 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:46:32.736387 [error] [MainThread]:     Syntax error at or near '.'(line 60, pos 11)
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.738137 [error] [MainThread]:     == SQL ==
[0m00:46:32.738137 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:46:32.738137 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:46:32.738137 [error] [MainThread]:       
[0m00:46:32.738137 [error] [MainThread]:       as
[0m00:46:32.738137 [error] [MainThread]:         
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.738137 [error] [MainThread]:     
[0m00:46:32.749010 [error] [MainThread]:     
[0m00:46:32.750032 [error] [MainThread]:     
[0m00:46:32.751016 [error] [MainThread]:     
[0m00:46:32.753025 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:46:32.753025 [error] [MainThread]:     
[0m00:46:32.753025 [error] [MainThread]:     
[0m00:46:32.754428 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:46:32.755385 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:46:32.755385 [error] [MainThread]:     
[0m00:46:32.755385 [error] [MainThread]:     
[0m00:46:32.755385 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:46:32.755385 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:46:32.755385 [error] [MainThread]:     ),
[0m00:46:32.755385 [error] [MainThread]:     
[0m00:46:32.755385 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:46:32.755385 [error] [MainThread]:         SELECT
[0m00:46:32.755385 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:32.755385 [error] [MainThread]:             b.AS_OF_DATE
[0m00:46:32.755385 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:46:32.755385 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:46:32.765563 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:46:32.765563 [error] [MainThread]:     ),
[0m00:46:32.766605 [error] [MainThread]:     
[0m00:46:32.767718 [error] [MainThread]:     new_rows AS (
[0m00:46:32.767718 [error] [MainThread]:         SELECT
[0m00:46:32.767718 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:32.767718 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:32.767718 [error] [MainThread]:         timestamp
[0m00:46:32.767718 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:32.767718 [error] [MainThread]:         timestamp
[0m00:46:32.767718 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:46:32.767718 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:46:32.767718 [error] [MainThread]:     
[0m00:46:32.767718 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:32.767718 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:32.767718 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:32.767718 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:32.767718 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:32.767718 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:32.767718 [error] [MainThread]:         GROUP BY
[0m00:46:32.767718 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:46:32.767718 [error] [MainThread]:     ),
[0m00:46:32.767718 [error] [MainThread]:     temp as (
[0m00:46:32.782248 [error] [MainThread]:         select 
[0m00:46:32.782248 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:46:32.783862 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:46:32.784840 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:46:32.786443 [error] [MainThread]:         ndb.s_address.`addr`
[0m00:46:32.787502 [error] [MainThread]:                 
[0m00:46:32.788024 [error] [MainThread]:             ndb.s_name.`name`
[0m00:46:32.789128 [error] [MainThread]:     -----------^^^
[0m00:46:32.789761 [error] [MainThread]:                 
[0m00:46:32.790265 [error] [MainThread]:             from new_rows a
[0m00:46:32.790800 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:32.791319 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:32.791846 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:46:32.792902 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:32.793422 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:32.793941 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:46:32.794453 [error] [MainThread]:         ),
[0m00:46:32.795009 [error] [MainThread]:     pit AS (
[0m00:46:32.795539 [error] [MainThread]:         SELECT * FROM temp
[0m00:46:32.796058 [error] [MainThread]:     )
[0m00:46:32.796577 [error] [MainThread]:     
[0m00:46:32.797093 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:46:32.797609 [error] [MainThread]:     
[0m00:46:32.798127 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:46:32.798127 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:46:32.799643 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:46:32.800161 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:46:32.800685 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:46:32.801512 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:46:32.803158 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:46:32.804275 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:46:32.804780 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:46:32.805821 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:46:32.806339 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:46:32.806856 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:46:32.807373 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:46:32.807892 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:46:32.808935 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:46:32.809987 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:46:32.810507 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:46:32.811027 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:46:32.812056 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:46:32.812056 [error] [MainThread]:     Syntax error at or near '.'(line 60, pos 11)
[0m00:46:32.812056 [error] [MainThread]:     
[0m00:46:32.812056 [error] [MainThread]:     == SQL ==
[0m00:46:32.814487 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:46:32.815652 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:46:32.815652 [error] [MainThread]:       
[0m00:46:32.816660 [error] [MainThread]:       as
[0m00:46:32.817644 [error] [MainThread]:         
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:46:32.818842 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.818842 [error] [MainThread]:     
[0m00:46:32.832421 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:46:32.833495 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:46:32.834507 [error] [MainThread]:     ),
[0m00:46:32.835649 [error] [MainThread]:     
[0m00:46:32.837079 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:46:32.838103 [error] [MainThread]:         SELECT
[0m00:46:32.838103 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:32.838103 [error] [MainThread]:             b.AS_OF_DATE
[0m00:46:32.838103 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:46:32.838103 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:46:32.838103 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:46:32.838103 [error] [MainThread]:     ),
[0m00:46:32.838103 [error] [MainThread]:     
[0m00:46:32.838103 [error] [MainThread]:     new_rows AS (
[0m00:46:32.838103 [error] [MainThread]:         SELECT
[0m00:46:32.838103 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:32.838103 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:32.848956 [error] [MainThread]:         timestamp
[0m00:46:32.849968 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:32.850967 [error] [MainThread]:         timestamp
[0m00:46:32.852975 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:46:32.853458 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:46:32.853458 [error] [MainThread]:     
[0m00:46:32.853458 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:32.853458 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:32.853458 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:32.853458 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:32.853458 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:32.853458 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:32.853458 [error] [MainThread]:         GROUP BY
[0m00:46:32.853458 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:46:32.853458 [error] [MainThread]:     ),
[0m00:46:32.853458 [error] [MainThread]:     temp as (
[0m00:46:32.853458 [error] [MainThread]:         select 
[0m00:46:32.853458 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:46:32.853458 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:46:32.853458 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:46:32.853458 [error] [MainThread]:         ndb.s_address.`addr`
[0m00:46:32.865655 [error] [MainThread]:                 
[0m00:46:32.866663 [error] [MainThread]:             ndb.s_name.`name`
[0m00:46:32.868719 [error] [MainThread]:     -----------^^^
[0m00:46:32.868719 [error] [MainThread]:                 
[0m00:46:32.868719 [error] [MainThread]:             from new_rows a
[0m00:46:32.872647 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:32.873230 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:32.873230 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:46:32.873230 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:32.873230 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:32.873230 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:46:32.873230 [error] [MainThread]:         ),
[0m00:46:32.873230 [error] [MainThread]:     pit AS (
[0m00:46:32.873230 [error] [MainThread]:         SELECT * FROM temp
[0m00:46:32.873230 [error] [MainThread]:     )
[0m00:46:32.873230 [error] [MainThread]:     
[0m00:46:32.873230 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:46:32.882385 [error] [MainThread]:     
[0m00:46:32.883401 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:46:32.884391 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:46:32.885600 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:46:32.885600 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:46:32.885600 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:46:32.885600 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:46:32.889508 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:46:32.889508 [error] [MainThread]:     	... 16 more
[0m00:46:32.889508 [error] [MainThread]:     
[0m00:46:32.899030 [info ] [MainThread]: 
[0m00:46:32.900053 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:46:32.902126 [debug] [MainThread]: Command `dbt run` failed at 00:46:32.902126 after 1.62 seconds
[0m00:46:32.902126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F77FCFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F700139FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F70290BB50>]}
[0m00:46:32.902126 [debug] [MainThread]: Flushing usage events
[0m00:46:52.777192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B501DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7732DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7732B90>]}


============================== 00:46:52.777192 | f7f76e73-6408-43e9-ac0c-b2f7973e1cb6 ==============================
[0m00:46:52.777192 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:46:52.777192 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:46:52.902293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7732E00>]}
[0m00:46:52.918019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7ADADA0>]}
[0m00:46:52.918019 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:46:52.936629 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:46:53.067416 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:46:53.083057 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:46:53.145684 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:46:53.248301 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:46:53.248301 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:46:53.265070 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:46:53.280078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7F69D80>]}
[0m00:46:53.327362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B5378D60>]}
[0m00:46:53.342899 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:46:53.342899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B537A260>]}
[0m00:46:53.342899 [info ] [MainThread]: 
[0m00:46:53.342899 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:46:53.342899 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:46:53.360713 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:46:53.361721 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:46:53.361721 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:53.450804 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:53.450804 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:53.460826 [debug] [ThreadPool]: On list_schemas: Close
[0m00:46:53.467637 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m00:46:53.467637 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:53.482647 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:46:53.483544 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:46:53.483544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:46:53.728614 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:53.728614 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:53.739811 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:46:53.740808 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:46:53.740808 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:46:53.764432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m00:46:53.773593 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:53.773593 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:46:53.773593 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:46:53.773593 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:46:54.011550 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:46:54.011550 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:46:54.025155 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:46:54.025155 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:46:54.025155 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:46:54.038665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7E0B8E0>]}
[0m00:46:54.038665 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:54.038665 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:54.038665 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:46:54.038665 [info ] [MainThread]: 
[0m00:46:54.047178 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:46:54.047178 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:46:54.047178 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:46:54.047178 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:46:54.075008 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:46:54.075008 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:46:54.047178 => 00:46:54.075008
[0m00:46:54.075008 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:46:54.101459 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:46:54.101459 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:54.101459 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:46:54.101459 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
            
        ndb.s_name.`name`,
            
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:46:54.101459 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:46:54.151482 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 62, pos 22)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n            \n        ndb.s_name.`name`,\n            \n        from new_rows a\n----------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 62, pos 22)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n            \n        ndb.s_name.`name`,\n            \n        from new_rows a\n----------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:46:54.151482 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:46:54.152996 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
            
        ndb.s_name.`name`,
            
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:46:54.152996 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 62, pos 22)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
              
          ndb.s_name.`name`,
              
          from new_rows a
  ----------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 62, pos 22)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
              
          ndb.s_name.`name`,
              
          from new_rows a
  ----------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:46:54.152996 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:46:54.075008 => 00:46:54.152996
[0m00:46:54.152996 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:46:54.152996 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:46:54.152996 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:46:54.167038 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 62, pos 22)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
                
            ndb.s_name.`name`,
                
            from new_rows a
    ----------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 62, pos 22)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
                
            ndb.s_name.`name`,
                
            from new_rows a
    ----------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:46:54.167038 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7f76e73-6408-43e9-ac0c-b2f7973e1cb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B7EBF820>]}
[0m00:46:54.167038 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.12s]
[0m00:46:54.167038 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:46:54.167038 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:54.167038 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:46:54.219780 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:46:54.222344 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:46:54.222344 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:46:54.222344 [debug] [MainThread]: On master: ROLLBACK
[0m00:46:54.222344 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:46:54.222344 [debug] [MainThread]: On master: Close
[0m00:46:54.231350 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:46:54.231350 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:46:54.231350 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m00:46:54.233918 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:46:54.234967 [info ] [MainThread]: 
[0m00:46:54.235926 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.89 seconds (0.89s).
[0m00:46:54.237926 [debug] [MainThread]: Command end result
[0m00:46:54.251431 [info ] [MainThread]: 
[0m00:46:54.253478 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:46:54.254481 [info ] [MainThread]: 
[0m00:46:54.255445 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:46:54.257437 [error] [MainThread]:   Database Error
[0m00:46:54.258398 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:46:54.259272 [error] [MainThread]:     Syntax error at or near 'a'(line 62, pos 22)
[0m00:46:54.259272 [error] [MainThread]:     
[0m00:46:54.259272 [error] [MainThread]:     == SQL ==
[0m00:46:54.259272 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:46:54.259272 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:46:54.259272 [error] [MainThread]:       
[0m00:46:54.267275 [error] [MainThread]:       as
[0m00:46:54.268281 [error] [MainThread]:         
[0m00:46:54.269283 [error] [MainThread]:     
[0m00:46:54.271283 [error] [MainThread]:     
[0m00:46:54.271283 [error] [MainThread]:     
[0m00:46:54.272282 [error] [MainThread]:     
[0m00:46:54.273301 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:46:54.275120 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     
[0m00:46:54.275120 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:46:54.284144 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:46:54.285150 [error] [MainThread]:     
[0m00:46:54.286169 [error] [MainThread]:     
[0m00:46:54.287339 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:46:54.287339 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:46:54.287339 [error] [MainThread]:     ),
[0m00:46:54.287339 [error] [MainThread]:     
[0m00:46:54.287339 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:46:54.287339 [error] [MainThread]:         SELECT
[0m00:46:54.287339 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:54.287339 [error] [MainThread]:             b.AS_OF_DATE
[0m00:46:54.287339 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:46:54.287339 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:46:54.287339 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:46:54.300897 [error] [MainThread]:     ),
[0m00:46:54.301916 [error] [MainThread]:     
[0m00:46:54.302904 [error] [MainThread]:     new_rows AS (
[0m00:46:54.303646 [error] [MainThread]:         SELECT
[0m00:46:54.303646 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:54.303646 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:54.303646 [error] [MainThread]:         timestamp
[0m00:46:54.303646 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:54.303646 [error] [MainThread]:         timestamp
[0m00:46:54.303646 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:46:54.303646 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:46:54.314253 [error] [MainThread]:     
[0m00:46:54.314253 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:54.314253 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:54.318402 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:54.319425 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:54.320926 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:54.320926 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:54.320926 [error] [MainThread]:         GROUP BY
[0m00:46:54.320926 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:46:54.320926 [error] [MainThread]:     ),
[0m00:46:54.320926 [error] [MainThread]:     temp as (
[0m00:46:54.320926 [error] [MainThread]:         select 
[0m00:46:54.320926 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:46:54.320926 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:46:54.320926 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:46:54.320926 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:46:54.334077 [error] [MainThread]:                 
[0m00:46:54.335085 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:46:54.337085 [error] [MainThread]:                 
[0m00:46:54.338084 [error] [MainThread]:             from new_rows a
[0m00:46:54.338084 [error] [MainThread]:     ----------------------^^^
[0m00:46:54.338084 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:54.338084 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:54.338084 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:46:54.338084 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:54.338084 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:54.338084 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:46:54.338084 [error] [MainThread]:         ),
[0m00:46:54.338084 [error] [MainThread]:     pit AS (
[0m00:46:54.350672 [error] [MainThread]:         SELECT * FROM temp
[0m00:46:54.351693 [error] [MainThread]:     )
[0m00:46:54.352681 [error] [MainThread]:     
[0m00:46:54.354565 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:46:54.354565 [error] [MainThread]:     
[0m00:46:54.354565 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:46:54.357573 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:46:54.357573 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:46:54.367229 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:46:54.368260 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:46:54.369263 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:46:54.370229 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:46:54.370229 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:46:54.370229 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:46:54.370229 [error] [MainThread]:     Syntax error at or near 'a'(line 62, pos 22)
[0m00:46:54.370229 [error] [MainThread]:     
[0m00:46:54.370229 [error] [MainThread]:     == SQL ==
[0m00:46:54.384079 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:46:54.385084 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:46:54.387430 [error] [MainThread]:       
[0m00:46:54.388016 [error] [MainThread]:       as
[0m00:46:54.388016 [error] [MainThread]:         
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     
[0m00:46:54.388016 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:46:54.400561 [error] [MainThread]:     
[0m00:46:54.401612 [error] [MainThread]:     
[0m00:46:54.402573 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:46:54.403571 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:46:54.404276 [error] [MainThread]:     
[0m00:46:54.404276 [error] [MainThread]:     
[0m00:46:54.404276 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:46:54.404276 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:46:54.404276 [error] [MainThread]:     ),
[0m00:46:54.404276 [error] [MainThread]:     
[0m00:46:54.404276 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:46:54.404276 [error] [MainThread]:         SELECT
[0m00:46:54.404276 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:54.404276 [error] [MainThread]:             b.AS_OF_DATE
[0m00:46:54.404276 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:46:54.417372 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:46:54.417372 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:46:54.418380 [error] [MainThread]:     ),
[0m00:46:54.420046 [error] [MainThread]:     
[0m00:46:54.420046 [error] [MainThread]:     new_rows AS (
[0m00:46:54.420046 [error] [MainThread]:         SELECT
[0m00:46:54.422783 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:46:54.422783 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:54.422783 [error] [MainThread]:         timestamp
[0m00:46:54.422783 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:46:54.422783 [error] [MainThread]:         timestamp
[0m00:46:54.422783 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:46:54.422783 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:46:54.422783 [error] [MainThread]:     
[0m00:46:54.422783 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:54.422783 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:54.422783 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:54.433935 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:54.434946 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:54.435942 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:46:54.436368 [error] [MainThread]:         GROUP BY
[0m00:46:54.436368 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:46:54.436368 [error] [MainThread]:     ),
[0m00:46:54.436368 [error] [MainThread]:     temp as (
[0m00:46:54.436368 [error] [MainThread]:         select 
[0m00:46:54.436368 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:46:54.436368 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:46:54.436368 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:46:54.436368 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:46:54.436368 [error] [MainThread]:                 
[0m00:46:54.436368 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:46:54.436368 [error] [MainThread]:                 
[0m00:46:54.450694 [error] [MainThread]:             from new_rows a
[0m00:46:54.450694 [error] [MainThread]:     ----------------------^^^
[0m00:46:54.452715 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:46:54.453719 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:46:54.454151 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:46:54.455307 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:46:54.457875 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:46:54.457875 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:46:54.457875 [error] [MainThread]:         ),
[0m00:46:54.457875 [error] [MainThread]:     pit AS (
[0m00:46:54.457875 [error] [MainThread]:         SELECT * FROM temp
[0m00:46:54.457875 [error] [MainThread]:     )
[0m00:46:54.457875 [error] [MainThread]:     
[0m00:46:54.457875 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:46:54.457875 [error] [MainThread]:     
[0m00:46:54.457875 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:46:54.457875 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:46:54.457875 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:46:54.469381 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:46:54.470709 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:46:54.471728 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:46:54.472248 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:46:54.472248 [error] [MainThread]:     	... 16 more
[0m00:46:54.472248 [error] [MainThread]:     
[0m00:46:54.472248 [info ] [MainThread]: 
[0m00:46:54.487023 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:46:54.488787 [debug] [MainThread]: Command `dbt run` failed at 00:46:54.488787 after 1.72 seconds
[0m00:46:54.488787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B501DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B5378D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1B415BDF0>]}
[0m00:46:54.488787 [debug] [MainThread]: Flushing usage events
[0m00:47:17.645016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018537D8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853A4A2D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853A4A2B90>]}


============================== 00:47:17.649874 | 6e03d37e-fe53-4e7f-937d-7ad38512e783 ==============================
[0m00:47:17.649874 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:47:17.650907 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:47:17.769633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853A4A2DA0>]}
[0m00:47:17.769633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853A852CB0>]}
[0m00:47:17.769633 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:47:17.785346 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:47:17.915673 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:47:17.916612 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:47:17.967985 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:47:18.081494 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:47:18.081494 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:47:18.127248 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:47:18.127248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853ACE1960>]}
[0m00:47:18.189757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185380DC3D0>]}
[0m00:47:18.189757 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:47:18.189757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185380DDF00>]}
[0m00:47:18.189757 [info ] [MainThread]: 
[0m00:47:18.189757 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:47:18.189757 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:47:18.211414 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:47:18.211414 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:47:18.211414 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:18.307408 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:18.307408 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:18.316439 [debug] [ThreadPool]: On list_schemas: Close
[0m00:47:18.325995 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:47:18.327961 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.327961 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:47:18.327961 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:47:18.327961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:18.531356 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:18.531356 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:18.531356 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:47:18.531356 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:47:18.531356 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:47:18.547534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:47:18.547534 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.547534 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:47:18.547534 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:47:18.547534 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:47:18.785938 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:18.787134 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:18.795661 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:47:18.795661 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:47:18.795661 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:47:18.807686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853ABA3130>]}
[0m00:47:18.807686 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.807686 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:18.807686 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:47:18.812706 [info ] [MainThread]: 
[0m00:47:18.819967 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:47:18.819967 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:47:18.822016 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:47:18.822427 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:47:18.839972 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:47:18.839972 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:47:18.822427 => 00:47:18.839972
[0m00:47:18.839972 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:47:18.871926 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:47:18.871926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:18.871926 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:47:18.871926 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:47:18.886104 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:47:18.947146 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 60, pos 22)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        from new_rows a\n----------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \nSyntax error at or near \'a\'(line 60, pos 22)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */\ncreate or replace view ndb.pit_client2\n  \n  as\n    \n\n\n\n\n\n\n\n\n-- Generated by dbtvault.\n\n\n-- depends_on: ndb.v_stg_s_address\n-- depends_on: ndb.v_stg_s_name\n\n\nWITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST(\'0000000000000000\' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST(\'1900-01-01 00:00:00.000\' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        from new_rows a\n----------------------^^^\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)\n\tat org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)\n\tat org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:47:18.948146 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:47:18.949152 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:47:18.949152 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 60, pos 22)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          from new_rows a
  ----------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  Syntax error at or near 'a'(line 60, pos 22)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
  create or replace view ndb.pit_client2
    
    as
      
  
  
  
  
  
  
  
  
  -- Generated by dbtvault.
  
  
  -- depends_on: ndb.v_stg_s_address
  -- depends_on: ndb.v_stg_s_name
  
  
  WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          from new_rows a
  ----------------------^^^
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
  	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
  	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:47:18.950659 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:47:18.839972 => 00:47:18.950659
[0m00:47:18.950659 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:47:18.952193 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:47:18.952193 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:47:18.958748 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 60, pos 22)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            from new_rows a
    ----------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    Syntax error at or near 'a'(line 60, pos 22)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
    create or replace view ndb.pit_client2
      
      as
        
    
    
    
    
    
    
    
    
    -- Generated by dbtvault.
    
    
    -- depends_on: ndb.v_stg_s_address
    -- depends_on: ndb.v_stg_s_name
    
    
    WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            from new_rows a
    ----------------------^^^
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
    	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
    	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:47:18.958748 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e03d37e-fe53-4e7f-937d-7ad38512e783', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001853ACEAE00>]}
[0m00:47:18.967257 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.14s]
[0m00:47:18.969112 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:47:18.971120 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:18.972164 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:47:19.014666 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:47:19.014666 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:19.014666 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:19.014666 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:19.014666 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:47:19.014666 [debug] [MainThread]: On master: Close
[0m00:47:19.025435 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:47:19.025435 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:47:19.025435 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:47:19.025435 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:47:19.025435 [info ] [MainThread]: 
[0m00:47:19.025435 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.84 seconds (0.84s).
[0m00:47:19.025435 [debug] [MainThread]: Command end result
[0m00:47:19.043195 [info ] [MainThread]: 
[0m00:47:19.043195 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:47:19.043195 [info ] [MainThread]: 
[0m00:47:19.043195 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:47:19.043195 [error] [MainThread]:   Database Error
[0m00:47:19.043195 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:47:19.052396 [error] [MainThread]:     Syntax error at or near 'a'(line 60, pos 22)
[0m00:47:19.053419 [error] [MainThread]:     
[0m00:47:19.054404 [error] [MainThread]:     == SQL ==
[0m00:47:19.055410 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:47:19.056404 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:47:19.056404 [error] [MainThread]:       
[0m00:47:19.057405 [error] [MainThread]:       as
[0m00:47:19.058627 [error] [MainThread]:         
[0m00:47:19.058627 [error] [MainThread]:     
[0m00:47:19.061136 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.061984 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:47:19.061984 [error] [MainThread]:     
[0m00:47:19.069183 [error] [MainThread]:     
[0m00:47:19.070190 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:47:19.071191 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:47:19.071765 [error] [MainThread]:     
[0m00:47:19.072770 [error] [MainThread]:     
[0m00:47:19.073770 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:47:19.075107 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:47:19.075107 [error] [MainThread]:     ),
[0m00:47:19.075107 [error] [MainThread]:     
[0m00:47:19.075107 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:47:19.075107 [error] [MainThread]:         SELECT
[0m00:47:19.079327 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:19.080886 [error] [MainThread]:             b.AS_OF_DATE
[0m00:47:19.080886 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:47:19.080886 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:47:19.080886 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:47:19.080886 [error] [MainThread]:     ),
[0m00:47:19.080886 [error] [MainThread]:     
[0m00:47:19.085937 [error] [MainThread]:     new_rows AS (
[0m00:47:19.086943 [error] [MainThread]:         SELECT
[0m00:47:19.087943 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:19.089277 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:19.089277 [error] [MainThread]:         timestamp
[0m00:47:19.091590 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:19.091590 [error] [MainThread]:         timestamp
[0m00:47:19.091590 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:47:19.091590 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:47:19.091590 [error] [MainThread]:     
[0m00:47:19.091590 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:19.091590 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:19.091590 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:19.091590 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:19.101095 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:19.101095 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:19.102564 [error] [MainThread]:         GROUP BY
[0m00:47:19.103571 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:47:19.104571 [error] [MainThread]:     ),
[0m00:47:19.105674 [error] [MainThread]:     temp as (
[0m00:47:19.105674 [error] [MainThread]:         select 
[0m00:47:19.107683 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:47:19.108569 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:47:19.109170 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:47:19.109170 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:47:19.111155 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:47:19.111155 [error] [MainThread]:             from new_rows a
[0m00:47:19.111155 [error] [MainThread]:     ----------------------^^^
[0m00:47:19.111155 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:19.111155 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:19.111155 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:47:19.111155 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:19.119176 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:19.119176 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:47:19.120182 [error] [MainThread]:         ),
[0m00:47:19.122191 [error] [MainThread]:     pit AS (
[0m00:47:19.122299 [error] [MainThread]:         SELECT * FROM temp
[0m00:47:19.122299 [error] [MainThread]:     )
[0m00:47:19.125515 [error] [MainThread]:     
[0m00:47:19.125515 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:47:19.127508 [error] [MainThread]:     
[0m00:47:19.127508 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:47:19.129416 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:47:19.129416 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:47:19.132242 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:47:19.133248 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:47:19.133248 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:47:19.134273 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:47:19.135676 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:47:19.136684 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:47:19.137689 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:47:19.138690 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:47:19.138690 [error] [MainThread]:     Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[0m00:47:19.138690 [error] [MainThread]:     Syntax error at or near 'a'(line 60, pos 22)
[0m00:47:19.138690 [error] [MainThread]:     
[0m00:47:19.138690 [error] [MainThread]:     == SQL ==
[0m00:47:19.152394 [error] [MainThread]:     /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
[0m00:47:19.153408 [error] [MainThread]:     create or replace view ndb.pit_client2
[0m00:47:19.154403 [error] [MainThread]:       
[0m00:47:19.154806 [error] [MainThread]:       as
[0m00:47:19.154806 [error] [MainThread]:         
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     -- Generated by dbtvault.
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     
[0m00:47:19.157311 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_address
[0m00:47:19.169316 [error] [MainThread]:     -- depends_on: ndb.v_stg_s_name
[0m00:47:19.170082 [error] [MainThread]:     
[0m00:47:19.171079 [error] [MainThread]:     
[0m00:47:19.172081 [error] [MainThread]:     WITH as_of_dates AS (
[0m00:47:19.172185 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:47:19.172185 [error] [MainThread]:     ),
[0m00:47:19.172185 [error] [MainThread]:     
[0m00:47:19.172185 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:47:19.172185 [error] [MainThread]:         SELECT
[0m00:47:19.172185 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:19.172185 [error] [MainThread]:             b.AS_OF_DATE
[0m00:47:19.172185 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:47:19.179193 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:47:19.179193 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:47:19.179193 [error] [MainThread]:     ),
[0m00:47:19.179193 [error] [MainThread]:     
[0m00:47:19.179193 [error] [MainThread]:     new_rows AS (
[0m00:47:19.179193 [error] [MainThread]:         SELECT
[0m00:47:19.179193 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:19.185786 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:19.185786 [error] [MainThread]:         timestamp
[0m00:47:19.187793 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:19.188458 [error] [MainThread]:         timestamp
[0m00:47:19.190226 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:47:19.190226 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:47:19.190226 [error] [MainThread]:     
[0m00:47:19.190226 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:19.190226 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:19.190226 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:19.190226 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:19.190226 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:19.190226 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:19.190226 [error] [MainThread]:         GROUP BY
[0m00:47:19.190226 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:47:19.190226 [error] [MainThread]:     ),
[0m00:47:19.200234 [error] [MainThread]:     temp as (
[0m00:47:19.200234 [error] [MainThread]:         select 
[0m00:47:19.200234 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:47:19.202353 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:47:19.202353 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:47:19.203363 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:47:19.204361 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:47:19.205014 [error] [MainThread]:             from new_rows a
[0m00:47:19.205014 [error] [MainThread]:     ----------------------^^^
[0m00:47:19.207524 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:19.208053 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:19.208053 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:47:19.209572 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:19.209572 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:19.209572 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:47:19.209572 [error] [MainThread]:         ),
[0m00:47:19.209572 [error] [MainThread]:     pit AS (
[0m00:47:19.209572 [error] [MainThread]:         SELECT * FROM temp
[0m00:47:19.209572 [error] [MainThread]:     )
[0m00:47:19.209572 [error] [MainThread]:     
[0m00:47:19.209572 [error] [MainThread]:     SELECT DISTINCT * FROM pit
[0m00:47:19.217581 [error] [MainThread]:     
[0m00:47:19.217581 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
[0m00:47:19.219043 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
[0m00:47:19.220050 [error] [MainThread]:     	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:52)
[0m00:47:19.222048 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:89)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieSpark3_3ExtendedSqlParser.parsePlan(HoodieSpark3_3ExtendedSqlParser.scala:55)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.$anonfun$parsePlan$1(HoodieCommonSqlParser.scala:44)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parse(HoodieCommonSqlParser.scala:92)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.parser.HoodieCommonSqlParser.parsePlan(HoodieCommonSqlParser.scala:41)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:620)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:620)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:47:19.222394 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:47:19.222394 [error] [MainThread]:     	... 16 more
[0m00:47:19.222394 [error] [MainThread]:     
[0m00:47:19.235740 [info ] [MainThread]: 
[0m00:47:19.236856 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:47:19.238645 [debug] [MainThread]: Command `dbt run` failed at 00:47:19.238645 after 1.61 seconds
[0m00:47:19.239663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018537D8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185380DC3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018536EBBBB0>]}
[0m00:47:19.239663 [debug] [MainThread]: Flushing usage events
[0m00:47:40.666577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB770DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB9E22DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB9E22C20>]}


============================== 00:47:40.666577 | 953b7cb8-bf85-4626-83c1-927b81a5abb9 ==============================
[0m00:47:40.666577 [info ] [MainThread]: Running with dbt=1.5.2
[0m00:47:40.666577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m00:47:40.787483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB9E22E30>]}
[0m00:47:40.804047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FBA1D2F50>]}
[0m00:47:40.804047 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m00:47:40.843906 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m00:47:40.953524 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:47:40.953524 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m00:47:41.017064 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m00:47:41.126002 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m00:47:41.126002 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m00:47:41.157679 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m00:47:41.173311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FBA6599F0>]}
[0m00:47:41.245445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB684BA30>]}
[0m00:47:41.246015 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m00:47:41.246914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB684B970>]}
[0m00:47:41.248839 [info ] [MainThread]: 
[0m00:47:41.250399 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m00:47:41.252060 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m00:47:41.263403 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m00:47:41.264059 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:47:41.264643 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:41.352057 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:41.352057 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:41.359061 [debug] [ThreadPool]: On list_schemas: Close
[0m00:47:41.368612 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m00:47:41.370905 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:41.370905 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m00:47:41.375470 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m00:47:41.375470 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:47:41.585678 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:41.585678 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:41.594555 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m00:47:41.594555 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:47:41.594555 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m00:47:41.605567 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m00:47:41.613559 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:41.613559 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m00:47:41.613559 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m00:47:41.613559 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:47:41.819815 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m00:47:41.819815 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m00:47:41.834050 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m00:47:41.838266 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m00:47:41.838266 [debug] [ThreadPool]: On list_None_ndb: Close
[0m00:47:41.849530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FBA526E60>]}
[0m00:47:41.849530 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:41.849530 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:41.849530 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:47:41.849530 [info ] [MainThread]: 
[0m00:47:41.855307 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m00:47:41.855307 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m00:47:41.855307 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m00:47:41.864087 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m00:47:41.890054 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m00:47:41.891431 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 00:47:41.864087 => 00:47:41.891076
[0m00:47:41.892061 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m00:47:41.912081 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m00:47:41.912081 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:41.912081 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m00:47:41.912081 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:47:41.912081 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:47:42.322311 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]\n                  +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))\n                     :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 50, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 51, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 52, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]\n                     :  :           :  :                                :           :              :- CTERelationDef 53, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]\n                     :  :           :  :                                :           :              :- CTERelationDef 54, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 56, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 57, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]\n                     :  :           :  :                                :                          :- CTERelationDef 58, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]\n                     :  :           :  :                                :                          :- CTERelationDef 59, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]\n                  +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))\n                     :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 50, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 51, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 52, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]\n                     :  :           :  :                                :           :              :- CTERelationDef 53, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]\n                     :  :           :  :                                :           :              :- CTERelationDef 54, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 56, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 57, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]\n                     :  :           :  :                                :                          :- CTERelationDef 58, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]\n                     :  :           :  :                                :                          :- CTERelationDef 59, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m00:47:42.322311 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m00:47:42.322311 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m00:47:42.322311 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
                    +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
                       :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 50, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 51, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 52, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                       :  :           :  :                                :           :              :- CTERelationDef 53, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
                       :  :           :  :                                :           :              :- CTERelationDef 54, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 56, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 57, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                       :  :           :  :                                :                          :- CTERelationDef 58, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
                       :  :           :  :                                :                          :- CTERelationDef 59, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
                       :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
                    +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
                       :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 50, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 51, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 52, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                       :  :           :  :                                :           :              :- CTERelationDef 53, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
                       :  :           :  :                                :           :              :- CTERelationDef 54, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 56, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 57, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                       :  :           :  :                                :                          :- CTERelationDef 58, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
                       :  :           :  :                                :                          :- CTERelationDef 59, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
                       :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m00:47:42.333344 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 00:47:41.892061 => 00:47:42.333344
[0m00:47:42.333344 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m00:47:42.335850 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m00:47:42.335850 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m00:47:42.351962 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
                      +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
                         :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 50, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 51, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 52, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                         :  :           :  :                                :           :              :- CTERelationDef 53, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
                         :  :           :  :                                :           :              :- CTERelationDef 54, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 56, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 57, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                         :  :           :  :                                :                          :- CTERelationDef 58, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
                         :  :           :  :                                :                          :- CTERelationDef 59, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
                         :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
                      +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
                         :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 50, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 51, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 52, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
                         :  :           :  :                                :           :              :- CTERelationDef 53, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
                         :  :           :  :                                :           :              :- CTERelationDef 54, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 56, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 57, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
                         :  :           :  :                                :                          :- CTERelationDef 58, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
                         :  :           :  :                                :                          :- CTERelationDef 59, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
                         :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m00:47:42.354471 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '953b7cb8-bf85-4626-83c1-927b81a5abb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FBA66AE90>]}
[0m00:47:42.354471 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.50s]
[0m00:47:42.354471 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m00:47:42.354471 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:42.354471 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:47:42.394590 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:47:42.394590 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:47:42.394590 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:47:42.394590 [debug] [MainThread]: On master: ROLLBACK
[0m00:47:42.394590 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m00:47:42.401100 [debug] [MainThread]: On master: Close
[0m00:47:42.407651 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:47:42.407651 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m00:47:42.407651 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m00:47:42.407651 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m00:47:42.407651 [info ] [MainThread]: 
[0m00:47:42.407651 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m00:47:42.407651 [debug] [MainThread]: Command end result
[0m00:47:42.428041 [info ] [MainThread]: 
[0m00:47:42.428041 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:47:42.428041 [info ] [MainThread]: 
[0m00:47:42.428041 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m00:47:42.428041 [error] [MainThread]:   Database Error
[0m00:47:42.428041 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
[0m00:47:42.428041 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m00:47:42.428041 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:47:42.428041 [error] [MainThread]:     ),
[0m00:47:42.437636 [error] [MainThread]:     
[0m00:47:42.439663 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:47:42.440645 [error] [MainThread]:         SELECT
[0m00:47:42.441647 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:42.442643 [error] [MainThread]:             b.AS_OF_DATE
[0m00:47:42.443646 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:47:42.444390 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:47:42.444390 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:47:42.445902 [error] [MainThread]:     ),
[0m00:47:42.445902 [error] [MainThread]:     
[0m00:47:42.445902 [error] [MainThread]:     new_rows AS (
[0m00:47:42.445902 [error] [MainThread]:         SELECT
[0m00:47:42.445902 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:42.445902 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:42.445902 [error] [MainThread]:         timestamp
[0m00:47:42.449924 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:42.449924 [error] [MainThread]:         timestamp
[0m00:47:42.449924 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:47:42.449924 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:47:42.449924 [error] [MainThread]:     
[0m00:47:42.449924 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:42.449924 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:42.449924 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:42.449924 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:42.449924 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:42.456145 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:42.457179 [error] [MainThread]:         GROUP BY
[0m00:47:42.458175 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:47:42.459217 [error] [MainThread]:     ),
[0m00:47:42.459217 [error] [MainThread]:     temp as (
[0m00:47:42.460074 [error] [MainThread]:         select 
[0m00:47:42.460074 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:47:42.460074 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:47:42.460074 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:47:42.460074 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:47:42.460074 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:47:42.460074 [error] [MainThread]:             'try' as t
[0m00:47:42.460074 [error] [MainThread]:         from new_rows a
[0m00:47:42.460074 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:42.460074 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:42.465586 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:47:42.465586 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:42.465586 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:42.465586 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:47:42.465586 [error] [MainThread]:         ),
[0m00:47:42.465586 [error] [MainThread]:     pit AS (
[0m00:47:42.465586 [error] [MainThread]:         SELECT * FROM temp
[0m00:47:42.465586 [error] [MainThread]:     )
[0m00:47:42.465586 [error] [MainThread]:     
[0m00:47:42.465586 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m00:47:42.465586 [error] [MainThread]:     +- 'Distinct
[0m00:47:42.470875 [error] [MainThread]:        +- 'Project [*]
[0m00:47:42.471891 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m00:47:42.472885 [error] [MainThread]:              +- 'Project [*]
[0m00:47:42.472885 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m00:47:42.473883 [error] [MainThread]:                    +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
[0m00:47:42.475507 [error] [MainThread]:                       +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
[0m00:47:42.475507 [error] [MainThread]:                          :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
[0m00:47:42.475507 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m00:47:42.475507 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m00:47:42.475507 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
[0m00:47:42.478106 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m00:47:42.478106 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m00:47:42.486615 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
[0m00:47:42.486615 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m00:47:42.487512 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
[0m00:47:42.487512 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
[0m00:47:42.489519 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m00:47:42.490521 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 50, false
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 51, false
[0m00:47:42.490827 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m00:47:42.497296 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
[0m00:47:42.498187 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
[0m00:47:42.504187 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m00:47:42.505199 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 52, false
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 53, false
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
[0m00:47:42.507520 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m00:47:42.520814 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
[0m00:47:42.521838 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 54, false
[0m00:47:42.522858 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m00:47:42.523646 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.523646 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m00:47:42.525255 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
[0m00:47:42.525255 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.525255 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m00:47:42.525255 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 56, false
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m00:47:42.527613 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
[0m00:47:42.537501 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m00:47:42.538509 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m00:47:42.539508 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 57, false
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 58, false
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m00:47:42.540175 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 59, false
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
[0m00:47:42.550184 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.554160 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m00:47:42.555177 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.555177 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m00:47:42.556668 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
[0m00:47:42.557238 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m00:47:42.557238 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
[0m00:47:42.557238 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m00:47:42.557238 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m00:47:42.557238 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
[0m00:47:42.557238 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m00:47:42.557238 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m00:47:42.557238 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
[0m00:47:42.562526 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m00:47:42.562526 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m00:47:42.562526 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
[0m00:47:42.562526 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m00:47:42.562526 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m00:47:42.562526 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
[0m00:47:42.562526 [error] [MainThread]:     
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m00:47:42.562526 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m00:47:42.562526 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m00:47:42.570806 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m00:47:42.571833 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m00:47:42.572813 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m00:47:42.573387 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m00:47:42.573387 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m00:47:42.573387 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m00:47:42.573387 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m00:47:42.573387 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m00:47:42.573387 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m00:47:42.573387 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Column 's_address_src.S_ADDRESS_LDTS' does not exist. Did you mean one of the following? [a.S_ADDRESS_LDTS, s_address_src.EFFECTIVE_FROM, s_address_src.ld_dt_tm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_address_src.addr, s_address_src.hsh_ky_cli_cd, a.S_ADDRESS_PK, s_address_src._hoodie_file_name, a.S_NAME_LDTS, s_address_src._hoodie_commit_time, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, a.S_NAME_PK, a.AS_OF_DATE, a.hk_cli_cd]; line 64 pos 47;
[0m00:47:42.573387 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m00:47:42.573387 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m00:47:42.573387 [error] [MainThread]:     ),
[0m00:47:42.573387 [error] [MainThread]:     
[0m00:47:42.573387 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m00:47:42.573387 [error] [MainThread]:         SELECT
[0m00:47:42.573387 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:42.573387 [error] [MainThread]:             b.AS_OF_DATE
[0m00:47:42.573387 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m00:47:42.573387 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m00:47:42.573387 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m00:47:42.573387 [error] [MainThread]:     ),
[0m00:47:42.573387 [error] [MainThread]:     
[0m00:47:42.573387 [error] [MainThread]:     new_rows AS (
[0m00:47:42.573387 [error] [MainThread]:         SELECT
[0m00:47:42.573387 [error] [MainThread]:             a.`hk_cli_cd`,
[0m00:47:42.573387 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:42.573387 [error] [MainThread]:         timestamp
[0m00:47:42.573387 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m00:47:42.573387 [error] [MainThread]:         timestamp
[0m00:47:42.573387 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m00:47:42.573387 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m00:47:42.587507 [error] [MainThread]:     
[0m00:47:42.587507 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:42.588954 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:42.589962 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:42.590076 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:42.590076 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:42.590076 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m00:47:42.590076 [error] [MainThread]:         GROUP BY
[0m00:47:42.590076 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m00:47:42.590076 [error] [MainThread]:     ),
[0m00:47:42.590076 [error] [MainThread]:     temp as (
[0m00:47:42.590076 [error] [MainThread]:         select 
[0m00:47:42.590076 [error] [MainThread]:         a.`hk_cli_cd`,
[0m00:47:42.590076 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m00:47:42.590076 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m00:47:42.590076 [error] [MainThread]:         ndb.s_address.`addr`,
[0m00:47:42.590076 [error] [MainThread]:             ndb.s_name.`name`,
[0m00:47:42.590076 [error] [MainThread]:             'try' as t
[0m00:47:42.590076 [error] [MainThread]:         from new_rows a
[0m00:47:42.590076 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m00:47:42.598073 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m00:47:42.598073 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = `s_address_src`.`S_ADDRESS_LDTS`
[0m00:47:42.598073 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m00:47:42.598073 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m00:47:42.598073 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = `s_name_src`.`S_NAME_LDTS`
[0m00:47:42.598073 [error] [MainThread]:         ),
[0m00:47:42.598073 [error] [MainThread]:     pit AS (
[0m00:47:42.598073 [error] [MainThread]:         SELECT * FROM temp
[0m00:47:42.598073 [error] [MainThread]:     )
[0m00:47:42.598073 [error] [MainThread]:     
[0m00:47:42.598073 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m00:47:42.598073 [error] [MainThread]:     +- 'Distinct
[0m00:47:42.598073 [error] [MainThread]:        +- 'Project [*]
[0m00:47:42.598073 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m00:47:42.598073 [error] [MainThread]:              +- 'Project [*]
[0m00:47:42.604170 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m00:47:42.605190 [error] [MainThread]:                    +- 'Project ['a.hk_cli_cd, 'a.AS_OF_DATE AS start_date#1110, 'lead('a.as_of_date) windowspecdefinition('a.a.hk_cli_cd, 'a.as_of_date ASC NULLS FIRST, unspecifiedframe$()) AS end_date#1111, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1112]
[0m00:47:42.606177 [error] [MainThread]:                       +- 'Join LeftOuter, (('a.hk_cli_cd = 's_name_src.hsh_ky_cli_cd) AND ('s_name_src.EFFECTIVE_FROM = 's_name_src.S_NAME_LDTS))
[0m00:47:42.606980 [error] [MainThread]:                          :- 'Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1199) AND (EFFECTIVE_FROM#1202 = 's_address_src.S_ADDRESS_LDTS))
[0m00:47:42.606980 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m00:47:42.606980 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m00:47:42.606980 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1168, AS_OF_DATE#1116], [hk_cli_cd#1168, AS_OF_DATE#1116, coalesce(max(hsh_ky_cli_cd#1177), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1106, coalesce(max(EFFECTIVE_FROM#1180), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1107, coalesce(max(hsh_ky_cli_cd#1188), cast(0000000000000000 as string)) AS S_NAME_PK#1108, coalesce(max(EFFECTIVE_FROM#1191), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1109]
[0m00:47:42.609338 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1188) AND (EFFECTIVE_FROM#1191 <= AS_OF_DATE#1116))
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1168 = hsh_ky_cli_cd#1177) AND (EFFECTIVE_FROM#1180 <= AS_OF_DATE#1116))
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1168, AS_OF_DATE#1116]
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1168 = hk_cli_cd#1115)
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1163,_hoodie_commit_seqno#1164,_hoodie_record_key#1165,_hoodie_partition_path#1166,_hoodie_file_name#1167,hk_cli_cd#1168,cli_id#1169,ld_dt_tm#1170,rcrd_src_nm#1171] parquet
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1115, AS_OF_DATE#1116]
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1115,AS_OF_DATE#1116])
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1113 as string) AS hk_cli_cd#1115, cast(AS_OF_DATE#1114 as timestamp) AS AS_OF_DATE#1116]
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 50, false
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1127, ts#1124]
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1121,RCRD_SRC_NM#1122,ADDR#1123,TS#1124,LD_DT_TM#1125,EFFECTIVE_FROM#1126,HSH_KY_CLI_CD#1127,RCRD_HSH_ID#1128])
[0m00:47:42.609338 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1129 as string) AS CLI_ID#1121, cast(RCRD_SRC_NM#1130 as string) AS RCRD_SRC_NM#1122, cast(ADDR#1131 as string) AS ADDR#1123, cast(TS#1132 as timestamp) AS TS#1124, cast(LD_DT_TM#1117 as timestamp) AS LD_DT_TM#1125, cast(EFFECTIVE_FROM#1118 as timestamp) AS EFFECTIVE_FROM#1126, cast(HSH_KY_CLI_CD#1119 as string) AS HSH_KY_CLI_CD#1127, cast(RCRD_HSH_ID#1120 as string) AS RCRD_HSH_ID#1128]
[0m00:47:42.620914 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m00:47:42.620914 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 51, false
[0m00:47:42.622921 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1129,rcrd_src_nm#1130,addr#1131,ts#1132])
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1135 as string) AS cli_id#1129, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1130, cast(addr#1139 as string) AS addr#1131, cast(ts#1140 as timestamp) AS ts#1132]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1135, rcrd_src_nm#1136, addr#1139, ts#1140]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1138)
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 55, false
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1137 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1137], Partition Cols: []]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 55, true, [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m00:47:42.623629 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1138, addr#1139, ts#1140], Partition Cols: []]
[0m00:47:42.637481 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 52, false
[0m00:47:42.638501 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, current_timestamp() AS ld_dt_tm#1117, ts#1132 AS EFFECTIVE_FROM#1118]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 51, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 53, false
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, cast(md5(cast(nullif(upper(trim(cast(cli_id#1129 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1119, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1131 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1120]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 52, true, [cli_id#1129, rcrd_src_nm#1130, addr#1131, ts#1132, ld_dt_tm#1117, EFFECTIVE_FROM#1118]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 54, false
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 53, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, hsh_ky_cli_cd#1119, rcrd_hsh_id#1120]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 54, true, [CLI_ID#1129, RCRD_SRC_NM#1130, ADDR#1131, TS#1132, LD_DT_TM#1117, EFFECTIVE_FROM#1118, HSH_KY_CLI_CD#1119, RCRD_HSH_ID#1120]
[0m00:47:42.640050 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1152, ts#1149]
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1146,RCRD_SRC_NM#1147,NAME#1148,TS#1149,LD_DT_TM#1150,EFFECTIVE_FROM#1151,HSH_KY_CLI_CD#1152,RCRD_HSH_ID#1153])
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1154 as string) AS CLI_ID#1146, cast(RCRD_SRC_NM#1155 as string) AS RCRD_SRC_NM#1147, cast(NAME#1156 as string) AS NAME#1148, cast(TS#1157 as timestamp) AS TS#1149, cast(LD_DT_TM#1142 as timestamp) AS LD_DT_TM#1150, cast(EFFECTIVE_FROM#1143 as timestamp) AS EFFECTIVE_FROM#1151, cast(HSH_KY_CLI_CD#1144 as string) AS HSH_KY_CLI_CD#1152, cast(RCRD_HSH_ID#1145 as string) AS RCRD_HSH_ID#1153]
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 56, false
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m00:47:42.650057 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
[0m00:47:42.654224 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m00:47:42.655238 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1154,rcrd_src_nm#1155,name#1156,ts#1157])
[0m00:47:42.656236 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1135 as string) AS cli_id#1154, cast(rcrd_src_nm#1136 as string) AS rcrd_src_nm#1155, cast(name#1160 as string) AS name#1156, cast(ts#1161 as timestamp) AS ts#1157]
[0m00:47:42.657230 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1135, rcrd_src_nm#1136, name#1160, ts#1161]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1135 as int) = cli_id#1159)
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1135,rcrd_src_nm#1136])
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1133 as string) AS cli_id#1135, cast(rcrd_src_nm#1134 as string) AS rcrd_src_nm#1136]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 60, false
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1158 as string), None) AS cli_id#1133, dummy AS rcrd_src_nm#1134]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1158], Partition Cols: []]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 60, true, [cli_id#1133, rcrd_src_nm#1134]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1159, name#1160, ts#1161], Partition Cols: []]
[0m00:47:42.657749 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 57, false
[0m00:47:42.670877 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, current_timestamp() AS ld_dt_tm#1142, ts#1157 AS EFFECTIVE_FROM#1143]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 56, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 58, false
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, cast(md5(cast(nullif(upper(trim(cast(cli_id#1154 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1144, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1156 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1145]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 57, true, [cli_id#1154, rcrd_src_nm#1155, name#1156, ts#1157, ld_dt_tm#1142, EFFECTIVE_FROM#1143]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 59, false
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 58, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, hsh_ky_cli_cd#1144, rcrd_hsh_id#1145]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 59, true, [CLI_ID#1154, RCRD_SRC_NM#1155, NAME#1156, TS#1157, LD_DT_TM#1142, EFFECTIVE_FROM#1143, HSH_KY_CLI_CD#1144, RCRD_HSH_ID#1145]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1127 AS hk_cli_cd#1113, ts#1124 AS AS_OF_DATE#1114]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 50, true, [hsh_ky_cli_cd#1127, ts#1124]
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m00:47:42.673350 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1172,_hoodie_commit_seqno#1173,_hoodie_record_key#1174,_hoodie_partition_path#1175,_hoodie_file_name#1176,hsh_ky_cli_cd#1177,rcrd_hsh_id#1178,addr#1179,EFFECTIVE_FROM#1180,ld_dt_tm#1181,rcrd_src_nm#1182] parquet
[0m00:47:42.673350 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m00:47:42.673350 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m00:47:42.687497 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1183,_hoodie_commit_seqno#1184,_hoodie_record_key#1185,_hoodie_partition_path#1186,_hoodie_file_name#1187,hsh_ky_cli_cd#1188,rcrd_hsh_id#1189,name#1190,EFFECTIVE_FROM#1191,ld_dt_tm#1192,rcrd_src_nm#1193] parquet
[0m00:47:42.687497 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m00:47:42.689506 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m00:47:42.690365 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1194,_hoodie_commit_seqno#1195,_hoodie_record_key#1196,_hoodie_partition_path#1197,_hoodie_file_name#1198,hsh_ky_cli_cd#1199,rcrd_hsh_id#1200,addr#1201,EFFECTIVE_FROM#1202,ld_dt_tm#1203,rcrd_src_nm#1204] parquet
[0m00:47:42.690365 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m00:47:42.690365 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m00:47:42.690365 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1205,_hoodie_commit_seqno#1206,_hoodie_record_key#1207,_hoodie_partition_path#1208,_hoodie_file_name#1209,hsh_ky_cli_cd#1210,rcrd_hsh_id#1211,name#1212,EFFECTIVE_FROM#1213,ld_dt_tm#1214,rcrd_src_nm#1215] parquet
[0m00:47:42.692565 [error] [MainThread]:     
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
[0m00:47:42.692565 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
[0m00:47:42.692565 [error] [MainThread]:     	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
[0m00:47:42.704167 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
[0m00:47:42.705199 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m00:47:42.706417 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m00:47:42.707401 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.707907 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.708427 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.709489 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.710012 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.710532 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.711051 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.711576 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.712089 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.712608 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.713127 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.713127 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.713645 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.714163 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.714675 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.715195 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.715706 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.716225 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.716225 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.716735 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.717252 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.717770 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.718289 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.718808 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.718808 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.718808 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.718808 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.718808 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.720857 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.721881 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m00:47:42.723208 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m00:47:42.737497 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m00:47:42.738504 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m00:47:42.740148 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m00:47:42.740148 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m00:47:42.750158 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m00:47:42.754270 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:47:42.755282 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m00:47:42.756278 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m00:47:42.756975 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m00:47:42.757533 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m00:47:42.757533 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m00:47:42.757533 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m00:47:42.757533 [error] [MainThread]:     	... 16 more
[0m00:47:42.757533 [error] [MainThread]:     
[0m00:47:42.757533 [info ] [MainThread]: 
[0m00:47:42.757533 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:47:42.757533 [debug] [MainThread]: Command `dbt run` failed at 00:47:42.757533 after 2.10 seconds
[0m00:47:42.757533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB770DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB684BA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017FB684BE50>]}
[0m00:47:42.757533 [debug] [MainThread]: Flushing usage events
[0m01:39:35.444807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B912DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BB842DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BB842BF0>]}


============================== 01:39:35.444807 | 4516ae96-d113-40b5-9320-db7ef7881111 ==============================
[0m01:39:35.444807 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:39:35.444807 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:39:35.585213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BB842E00>]}
[0m01:39:35.592757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BBBF2DA0>]}
[0m01:39:35.594383 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:39:35.615097 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:39:35.749628 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:39:35.749628 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m01:39:35.807417 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m01:39:35.924241 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m01:39:35.925258 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m01:39:35.946888 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m01:39:35.964163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BC081930>]}
[0m01:39:36.015086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B9489C60>]}
[0m01:39:36.015086 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:39:36.015086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B9489AB0>]}
[0m01:39:36.015086 [info ] [MainThread]: 
[0m01:39:36.015086 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:39:36.024263 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:39:36.034218 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:39:36.035621 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:39:36.035621 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:39:36.137234 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:39:36.137234 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:39:36.147976 [debug] [ThreadPool]: On list_schemas: Close
[0m01:39:36.158595 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:39:36.164142 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:36.164142 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:39:36.165150 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:39:36.165150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:39:36.397170 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:39:36.401771 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:39:36.406341 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:39:36.406341 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:39:36.412352 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:39:36.416784 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m01:39:36.428430 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:36.428430 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:39:36.428430 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:39:36.428430 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:39:36.656931 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:39:36.657954 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:39:36.665261 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:39:36.665261 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:39:36.665261 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:39:36.678550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BBF46E30>]}
[0m01:39:36.678550 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:36.678550 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:39:36.678550 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:39:36.678550 [info ] [MainThread]: 
[0m01:39:36.689146 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m01:39:36.689146 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m01:39:36.689146 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m01:39:36.692605 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m01:39:36.717666 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m01:39:36.717666 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 01:39:36.692605 => 01:39:36.717666
[0m01:39:36.717666 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m01:39:36.748737 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m01:39:36.748737 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:36.754242 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m01:39:36.755269 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:39:36.755269 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:39:37.129437 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]\n                  +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))\n                     :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 66, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 67, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 68, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]\n                     :  :           :  :                                :           :              :- CTERelationDef 69, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]\n                     :  :           :  :                                :           :              :- CTERelationDef 70, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 72, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 73, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]\n                     :  :           :  :                                :                          :- CTERelationDef 74, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]\n                     :  :           :  :                                :                          :- CTERelationDef 75, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]\n                  +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))\n                     :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 66, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 67, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 68, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]\n                     :  :           :  :                                :           :              :- CTERelationDef 69, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]\n                     :  :           :  :                                :           :              :- CTERelationDef 70, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 72, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 73, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]\n                     :  :           :  :                                :                          :- CTERelationDef 74, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]\n                     :  :           :  :                                :                          :- CTERelationDef 75, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:39:37.134471 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:39:37.135024 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:39:37.136787 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
                    +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
                       :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 66, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 67, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 68, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                       :  :           :  :                                :           :              :- CTERelationDef 69, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
                       :  :           :  :                                :           :              :- CTERelationDef 70, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 72, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 73, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                       :  :           :  :                                :                          :- CTERelationDef 74, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
                       :  :           :  :                                :                          :- CTERelationDef 75, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
                       :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
                    +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
                       :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 66, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 67, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 68, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                       :  :           :  :                                :           :              :- CTERelationDef 69, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
                       :  :           :  :                                :           :              :- CTERelationDef 70, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 72, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 73, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                       :  :           :  :                                :                          :- CTERelationDef 74, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
                       :  :           :  :                                :                          :- CTERelationDef 75, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
                       :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:39:37.144834 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 01:39:36.717666 => 01:39:37.144834
[0m01:39:37.144834 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m01:39:37.147853 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:39:37.147853 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m01:39:37.164194 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
                      +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
                         :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 66, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 67, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 68, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                         :  :           :  :                                :           :              :- CTERelationDef 69, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
                         :  :           :  :                                :           :              :- CTERelationDef 70, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 72, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 73, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                         :  :           :  :                                :                          :- CTERelationDef 74, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
                         :  :           :  :                                :                          :- CTERelationDef 75, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
                         :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
                      +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
                         :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 66, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 67, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 68, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
                         :  :           :  :                                :           :              :- CTERelationDef 69, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
                         :  :           :  :                                :           :              :- CTERelationDef 70, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 72, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 73, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
                         :  :           :  :                                :                          :- CTERelationDef 74, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
                         :  :           :  :                                :                          :- CTERelationDef 75, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
                         :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:39:37.166845 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4516ae96-d113-40b5-9320-db7ef7881111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226BBF63790>]}
[0m01:39:37.166845 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.48s]
[0m01:39:37.166845 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m01:39:37.172449 [debug] [MainThread]: On master: ROLLBACK
[0m01:39:37.172449 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:39:37.220430 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:39:37.220788 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:37.220788 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:39:37.220788 [debug] [MainThread]: On master: ROLLBACK
[0m01:39:37.220788 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:39:37.220788 [debug] [MainThread]: On master: Close
[0m01:39:37.226445 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:39:37.229727 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:39:37.230737 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m01:39:37.230737 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m01:39:37.231736 [info ] [MainThread]: 
[0m01:39:37.232083 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.22 seconds (1.22s).
[0m01:39:37.235035 [debug] [MainThread]: Command end result
[0m01:39:37.249456 [info ] [MainThread]: 
[0m01:39:37.250456 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:39:37.251459 [info ] [MainThread]: 
[0m01:39:37.253457 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m01:39:37.253457 [error] [MainThread]:   Database Error
[0m01:39:37.255018 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
[0m01:39:37.255018 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m01:39:37.255018 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m01:39:37.255018 [error] [MainThread]:     ),
[0m01:39:37.260090 [error] [MainThread]:     
[0m01:39:37.260090 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m01:39:37.260090 [error] [MainThread]:         SELECT
[0m01:39:37.262900 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:39:37.262900 [error] [MainThread]:             b.AS_OF_DATE
[0m01:39:37.264406 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m01:39:37.264932 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m01:39:37.265942 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m01:39:37.266940 [error] [MainThread]:     ),
[0m01:39:37.267521 [error] [MainThread]:     
[0m01:39:37.268527 [error] [MainThread]:     new_rows AS (
[0m01:39:37.269190 [error] [MainThread]:         SELECT
[0m01:39:37.269190 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:39:37.269190 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:39:37.269190 [error] [MainThread]:         timestamp
[0m01:39:37.269190 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:39:37.269190 [error] [MainThread]:         timestamp
[0m01:39:37.274198 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m01:39:37.275234 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m01:39:37.275234 [error] [MainThread]:     
[0m01:39:37.275234 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:39:37.275234 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:39:37.275234 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:39:37.275234 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:39:37.279452 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:39:37.280465 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:39:37.280465 [error] [MainThread]:         GROUP BY
[0m01:39:37.281466 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m01:39:37.282460 [error] [MainThread]:     ),
[0m01:39:37.282460 [error] [MainThread]:     temp as (
[0m01:39:37.285012 [error] [MainThread]:         select 
[0m01:39:37.285012 [error] [MainThread]:         a.`hk_cli_cd`,
[0m01:39:37.285012 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m01:39:37.285012 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m01:39:37.287498 [error] [MainThread]:         ndb.s_address.`addr`,
[0m01:39:37.287498 [error] [MainThread]:             ndb.s_name.`name`,
[0m01:39:37.287498 [error] [MainThread]:             'try' as t
[0m01:39:37.287498 [error] [MainThread]:         from new_rows a
[0m01:39:37.287498 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:39:37.287498 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:39:37.287498 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
[0m01:39:37.287498 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:39:37.287498 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:39:37.294014 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
[0m01:39:37.294014 [error] [MainThread]:         ),
[0m01:39:37.295031 [error] [MainThread]:     pit AS (
[0m01:39:37.295031 [error] [MainThread]:         SELECT * FROM temp
[0m01:39:37.296425 [error] [MainThread]:     )
[0m01:39:37.296425 [error] [MainThread]:     
[0m01:39:37.297435 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m01:39:37.299025 [error] [MainThread]:     +- 'Distinct
[0m01:39:37.299025 [error] [MainThread]:        +- 'Project [*]
[0m01:39:37.299025 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m01:39:37.299025 [error] [MainThread]:              +- 'Project [*]
[0m01:39:37.299025 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m01:39:37.299025 [error] [MainThread]:                    +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
[0m01:39:37.304053 [error] [MainThread]:                       +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
[0m01:39:37.305067 [error] [MainThread]:                          :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
[0m01:39:37.305067 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m01:39:37.305067 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m01:39:37.305067 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
[0m01:39:37.305067 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
[0m01:39:37.305067 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
[0m01:39:37.305067 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m01:39:37.305067 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m01:39:37.305067 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
[0m01:39:37.312816 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
[0m01:39:37.312816 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m01:39:37.314900 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m01:39:37.316097 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
[0m01:39:37.316878 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m01:39:37.316878 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m01:39:37.316878 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
[0m01:39:37.316878 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m01:39:37.316878 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
[0m01:39:37.320898 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
[0m01:39:37.320898 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m01:39:37.320898 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 66, false
[0m01:39:37.320898 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m01:39:37.324415 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m01:39:37.324964 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m01:39:37.324964 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
[0m01:39:37.324964 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m01:39:37.324964 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
[0m01:39:37.329523 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
[0m01:39:37.330529 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m01:39:37.331531 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 67, false
[0m01:39:37.332885 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m01:39:37.333829 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
[0m01:39:37.334874 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m01:39:37.334874 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:39:37.337173 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
[0m01:39:37.343188 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
[0m01:39:37.344203 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m01:39:37.344203 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
[0m01:39:37.345218 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m01:39:37.346136 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m01:39:37.347143 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
[0m01:39:37.348154 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m01:39:37.349067 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m01:39:37.349067 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:39:37.349067 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
[0m01:39:37.349067 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.349067 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m01:39:37.354078 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.355107 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m01:39:37.355107 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m01:39:37.356714 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
[0m01:39:37.356714 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 68, false
[0m01:39:37.356714 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m01:39:37.356714 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
[0m01:39:37.360232 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m01:39:37.360232 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
[0m01:39:37.360232 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 69, false
[0m01:39:37.360232 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m01:39:37.363248 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
[0m01:39:37.364340 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m01:39:37.364844 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
[0m01:39:37.366165 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 70, false
[0m01:39:37.366165 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.367827 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
[0m01:39:37.374384 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m01:39:37.374938 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
[0m01:39:37.374938 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
[0m01:39:37.376838 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m01:39:37.376838 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 72, false
[0m01:39:37.376838 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m01:39:37.376838 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
[0m01:39:37.376838 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m01:39:37.379432 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
[0m01:39:37.380442 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
[0m01:39:37.380442 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m01:39:37.381442 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
[0m01:39:37.382441 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
[0m01:39:37.383442 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m01:39:37.383442 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.384946 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m01:39:37.394320 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.394837 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m01:39:37.394837 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m01:39:37.396136 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
[0m01:39:37.397142 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 73, false
[0m01:39:37.397142 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m01:39:37.398146 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
[0m01:39:37.399143 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m01:39:37.400141 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
[0m01:39:37.400141 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 74, false
[0m01:39:37.401813 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m01:39:37.401813 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
[0m01:39:37.404320 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 75, false
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m01:39:37.404831 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
[0m01:39:37.412771 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m01:39:37.412771 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:39:37.414278 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
[0m01:39:37.415289 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m01:39:37.415874 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:39:37.417500 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
[0m01:39:37.417500 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m01:39:37.419100 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:39:37.419100 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
[0m01:39:37.419100 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m01:39:37.419100 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:39:37.419100 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
[0m01:39:37.419100 [error] [MainThread]:     
[0m01:39:37.419100 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:39:37.419100 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:39:37.419100 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:39:37.424106 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:39:37.425117 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:39:37.425117 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:39:37.425117 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:39:37.425117 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:39:37.425117 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:39:37.431288 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:39:37.431288 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:39:37.432411 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:39:37.432411 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Column 'a.a.hk_cli_cd' does not exist. Did you mean one of the following? [a.hk_cli_cd, a.S_NAME_PK, a.AS_OF_DATE, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_ADDRESS_LDTS, s_name_src.hsh_ky_cli_cd, s_name_src.name, s_address_src.addr, s_name_src.ld_dt_tm, s_address_src.hsh_ky_cli_cd, s_address_src.ld_dt_tm, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_address_src.rcrd_src_nm, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_commit_time, s_name_src._hoodie_record_key, s_address_src.EFFECTIVE_FROM, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_name_src._hoodie_partition_path, s_address_src._hoodie_partition_path]; line 57 pos 42;
[0m01:39:37.435001 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m01:39:37.435001 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m01:39:37.435001 [error] [MainThread]:     ),
[0m01:39:37.436627 [error] [MainThread]:     
[0m01:39:37.436627 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m01:39:37.436627 [error] [MainThread]:         SELECT
[0m01:39:37.436627 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:39:37.436627 [error] [MainThread]:             b.AS_OF_DATE
[0m01:39:37.436627 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m01:39:37.436627 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m01:39:37.436627 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m01:39:37.436627 [error] [MainThread]:     ),
[0m01:39:37.436627 [error] [MainThread]:     
[0m01:39:37.436627 [error] [MainThread]:     new_rows AS (
[0m01:39:37.436627 [error] [MainThread]:         SELECT
[0m01:39:37.436627 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:39:37.436627 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:39:37.436627 [error] [MainThread]:         timestamp
[0m01:39:37.444143 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:39:37.444143 [error] [MainThread]:         timestamp
[0m01:39:37.445158 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m01:39:37.445158 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m01:39:37.446073 [error] [MainThread]:     
[0m01:39:37.447078 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:39:37.447078 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:39:37.448416 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:39:37.449630 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:39:37.449630 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:39:37.449630 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:39:37.449630 [error] [MainThread]:         GROUP BY
[0m01:39:37.449630 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m01:39:37.449630 [error] [MainThread]:     ),
[0m01:39:37.449630 [error] [MainThread]:     temp as (
[0m01:39:37.454190 [error] [MainThread]:         select 
[0m01:39:37.454190 [error] [MainThread]:         a.`hk_cli_cd`,
[0m01:39:37.455243 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m01:39:37.455243 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m01:39:37.455243 [error] [MainThread]:         ndb.s_address.`addr`,
[0m01:39:37.456869 [error] [MainThread]:             ndb.s_name.`name`,
[0m01:39:37.456869 [error] [MainThread]:             'try' as t
[0m01:39:37.456869 [error] [MainThread]:         from new_rows a
[0m01:39:37.456869 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:39:37.456869 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:39:37.456869 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
[0m01:39:37.460380 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:39:37.460380 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:39:37.460380 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
[0m01:39:37.460380 [error] [MainThread]:         ),
[0m01:39:37.460380 [error] [MainThread]:     pit AS (
[0m01:39:37.462788 [error] [MainThread]:         SELECT * FROM temp
[0m01:39:37.462788 [error] [MainThread]:     )
[0m01:39:37.464889 [error] [MainThread]:     
[0m01:39:37.465690 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m01:39:37.465690 [error] [MainThread]:     +- 'Distinct
[0m01:39:37.465690 [error] [MainThread]:        +- 'Project [*]
[0m01:39:37.465690 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m01:39:37.465690 [error] [MainThread]:              +- 'Project [*]
[0m01:39:37.465690 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m01:39:37.465690 [error] [MainThread]:                    +- 'Project [hk_cli_cd#1325, AS_OF_DATE#1273 AS start_date#1267, lead(as_of_date#1273, 1, null) windowspecdefinition('a.a.hk_cli_cd, as_of_date#1273 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1268, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1269]
[0m01:39:37.465690 [error] [MainThread]:                       +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1367) AND (EFFECTIVE_FROM#1370 = S_NAME_LDTS#1266))
[0m01:39:37.465690 [error] [MainThread]:                          :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1356) AND (EFFECTIVE_FROM#1359 = S_ADDRESS_LDTS#1264))
[0m01:39:37.465690 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m01:39:37.465690 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m01:39:37.465690 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1325, AS_OF_DATE#1273], [hk_cli_cd#1325, AS_OF_DATE#1273, coalesce(max(hsh_ky_cli_cd#1334), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1263, coalesce(max(EFFECTIVE_FROM#1337), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1264, coalesce(max(hsh_ky_cli_cd#1345), cast(0000000000000000 as string)) AS S_NAME_PK#1265, coalesce(max(EFFECTIVE_FROM#1348), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1266]
[0m01:39:37.465690 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1345) AND (EFFECTIVE_FROM#1348 <= AS_OF_DATE#1273))
[0m01:39:37.465690 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1325 = hsh_ky_cli_cd#1334) AND (EFFECTIVE_FROM#1337 <= AS_OF_DATE#1273))
[0m01:39:37.465690 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m01:39:37.474195 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m01:39:37.474195 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1325, AS_OF_DATE#1273]
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1325 = hk_cli_cd#1272)
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1320,_hoodie_commit_seqno#1321,_hoodie_record_key#1322,_hoodie_partition_path#1323,_hoodie_file_name#1324,hk_cli_cd#1325,cli_id#1326,ld_dt_tm#1327,rcrd_src_nm#1328] parquet
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1272, AS_OF_DATE#1273]
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1272,AS_OF_DATE#1273])
[0m01:39:37.475208 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1270 as string) AS hk_cli_cd#1272, cast(AS_OF_DATE#1271 as timestamp) AS AS_OF_DATE#1273]
[0m01:39:37.479680 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m01:39:37.480687 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 66, false
[0m01:39:37.482110 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m01:39:37.482110 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m01:39:37.484118 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m01:39:37.484118 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1284, ts#1281]
[0m01:39:37.485144 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m01:39:37.485144 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1278,RCRD_SRC_NM#1279,ADDR#1280,TS#1281,LD_DT_TM#1282,EFFECTIVE_FROM#1283,HSH_KY_CLI_CD#1284,RCRD_HSH_ID#1285])
[0m01:39:37.485144 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1286 as string) AS CLI_ID#1278, cast(RCRD_SRC_NM#1287 as string) AS RCRD_SRC_NM#1279, cast(ADDR#1288 as string) AS ADDR#1280, cast(TS#1289 as timestamp) AS TS#1281, cast(LD_DT_TM#1274 as timestamp) AS LD_DT_TM#1282, cast(EFFECTIVE_FROM#1275 as timestamp) AS EFFECTIVE_FROM#1283, cast(HSH_KY_CLI_CD#1276 as string) AS HSH_KY_CLI_CD#1284, cast(RCRD_HSH_ID#1277 as string) AS RCRD_HSH_ID#1285]
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 67, false
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1286,rcrd_src_nm#1287,addr#1288,ts#1289])
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1292 as string) AS cli_id#1286, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1287, cast(addr#1296 as string) AS addr#1288, cast(ts#1297 as timestamp) AS ts#1289]
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1292, rcrd_src_nm#1293, addr#1296, ts#1297]
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1295)
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
[0m01:39:37.486798 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m01:39:37.494311 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 71, false
[0m01:39:37.494834 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m01:39:37.494834 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m01:39:37.496087 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1294 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
[0m01:39:37.496087 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m01:39:37.497095 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m01:39:37.498454 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1294], Partition Cols: []]
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 71, true, [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1295, addr#1296, ts#1297], Partition Cols: []]
[0m01:39:37.498985 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 68, false
[0m01:39:37.503991 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m01:39:37.503991 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, current_timestamp() AS ld_dt_tm#1274, ts#1289 AS EFFECTIVE_FROM#1275]
[0m01:39:37.505001 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m01:39:37.505001 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 67, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289]
[0m01:39:37.505001 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 69, false
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, cast(md5(cast(nullif(upper(trim(cast(cli_id#1286 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1276, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1288 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1277]
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 68, true, [cli_id#1286, rcrd_src_nm#1287, addr#1288, ts#1289, ld_dt_tm#1274, EFFECTIVE_FROM#1275]
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 70, false
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 69, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, hsh_ky_cli_cd#1276, rcrd_hsh_id#1277]
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m01:39:37.506555 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 70, true, [CLI_ID#1286, RCRD_SRC_NM#1287, ADDR#1288, TS#1289, LD_DT_TM#1274, EFFECTIVE_FROM#1275, HSH_KY_CLI_CD#1276, RCRD_HSH_ID#1277]
[0m01:39:37.512756 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1309, ts#1306]
[0m01:39:37.512756 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m01:39:37.514840 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1303,RCRD_SRC_NM#1304,NAME#1305,TS#1306,LD_DT_TM#1307,EFFECTIVE_FROM#1308,HSH_KY_CLI_CD#1309,RCRD_HSH_ID#1310])
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1311 as string) AS CLI_ID#1303, cast(RCRD_SRC_NM#1312 as string) AS RCRD_SRC_NM#1304, cast(NAME#1313 as string) AS NAME#1305, cast(TS#1314 as timestamp) AS TS#1306, cast(LD_DT_TM#1299 as timestamp) AS LD_DT_TM#1307, cast(EFFECTIVE_FROM#1300 as timestamp) AS EFFECTIVE_FROM#1308, cast(HSH_KY_CLI_CD#1301 as string) AS HSH_KY_CLI_CD#1309, cast(RCRD_HSH_ID#1302 as string) AS RCRD_HSH_ID#1310]
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 72, false
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1311,rcrd_src_nm#1312,name#1313,ts#1314])
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1292 as string) AS cli_id#1311, cast(rcrd_src_nm#1293 as string) AS rcrd_src_nm#1312, cast(name#1317 as string) AS name#1313, cast(ts#1318 as timestamp) AS ts#1314]
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1292, rcrd_src_nm#1293, name#1317, ts#1318]
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1292 as int) = cli_id#1316)
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1292,rcrd_src_nm#1293])
[0m01:39:37.515845 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1290 as string) AS cli_id#1292, cast(rcrd_src_nm#1291 as string) AS rcrd_src_nm#1293]
[0m01:39:37.524364 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m01:39:37.524914 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 76, false
[0m01:39:37.524914 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m01:39:37.526280 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m01:39:37.526280 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1315 as string), None) AS cli_id#1290, dummy AS rcrd_src_nm#1291]
[0m01:39:37.526280 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m01:39:37.526280 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m01:39:37.526280 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:39:37.529448 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1315], Partition Cols: []]
[0m01:39:37.529448 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.530459 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m01:39:37.531456 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 76, true, [cli_id#1290, rcrd_src_nm#1291]
[0m01:39:37.531986 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m01:39:37.532663 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m01:39:37.532663 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1316, name#1317, ts#1318], Partition Cols: []]
[0m01:39:37.534171 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 73, false
[0m01:39:37.534171 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, current_timestamp() AS ld_dt_tm#1299, ts#1314 AS EFFECTIVE_FROM#1300]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 72, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 74, false
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, cast(md5(cast(nullif(upper(trim(cast(cli_id#1311 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1301, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1313 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1302]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 73, true, [cli_id#1311, rcrd_src_nm#1312, name#1313, ts#1314, ld_dt_tm#1299, EFFECTIVE_FROM#1300]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 75, false
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 74, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, hsh_ky_cli_cd#1301, rcrd_hsh_id#1302]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 75, true, [CLI_ID#1311, RCRD_SRC_NM#1312, NAME#1313, TS#1314, LD_DT_TM#1299, EFFECTIVE_FROM#1300, HSH_KY_CLI_CD#1301, RCRD_HSH_ID#1302]
[0m01:39:37.535183 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m01:39:37.544441 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1284 AS hk_cli_cd#1270, ts#1281 AS AS_OF_DATE#1271]
[0m01:39:37.544953 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m01:39:37.544953 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 66, true, [hsh_ky_cli_cd#1284, ts#1281]
[0m01:39:37.544953 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m01:39:37.546507 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:39:37.547525 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1329,_hoodie_commit_seqno#1330,_hoodie_record_key#1331,_hoodie_partition_path#1332,_hoodie_file_name#1333,hsh_ky_cli_cd#1334,rcrd_hsh_id#1335,addr#1336,EFFECTIVE_FROM#1337,ld_dt_tm#1338,rcrd_src_nm#1339] parquet
[0m01:39:37.547978 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m01:39:37.548977 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:39:37.548977 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1340,_hoodie_commit_seqno#1341,_hoodie_record_key#1342,_hoodie_partition_path#1343,_hoodie_file_name#1344,hsh_ky_cli_cd#1345,rcrd_hsh_id#1346,name#1347,EFFECTIVE_FROM#1348,ld_dt_tm#1349,rcrd_src_nm#1350] parquet
[0m01:39:37.548977 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m01:39:37.548977 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:39:37.548977 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1351,_hoodie_commit_seqno#1352,_hoodie_record_key#1353,_hoodie_partition_path#1354,_hoodie_file_name#1355,hsh_ky_cli_cd#1356,rcrd_hsh_id#1357,addr#1358,EFFECTIVE_FROM#1359,ld_dt_tm#1360,rcrd_src_nm#1361] parquet
[0m01:39:37.548977 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m01:39:37.548977 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:39:37.548977 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1362,_hoodie_commit_seqno#1363,_hoodie_record_key#1364,_hoodie_partition_path#1365,_hoodie_file_name#1366,hsh_ky_cli_cd#1367,rcrd_hsh_id#1368,name#1369,EFFECTIVE_FROM#1370,ld_dt_tm#1371,rcrd_src_nm#1372] parquet
[0m01:39:37.554007 [error] [MainThread]:     
[0m01:39:37.554007 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.555020 [error] [MainThread]:     	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[0m01:39:37.555020 [error] [MainThread]:     	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[0m01:39:37.555020 [error] [MainThread]:     	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.555020 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.555020 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.560385 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.560385 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.560385 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.560385 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.562764 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.562764 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.562764 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.564828 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.565245 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.566842 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.566842 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.574872 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.574872 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.574872 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.574872 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.579419 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.579419 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.580814 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.581823 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.582167 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.582167 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.584174 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.585194 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.585194 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.585194 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.586816 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.586816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.594847 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:39:37.594847 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:39:37.596111 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:39:37.596111 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:39:37.597135 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:39:37.598122 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:39:37.598919 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.598919 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:39:37.598919 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:39:37.598919 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:39:37.598919 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:39:37.598919 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:39:37.604941 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:39:37.604941 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:39:37.606829 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:39:37.612999 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:39:37.612999 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:39:37.614076 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:39:37.615531 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:39:37.625050 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:39:37.625050 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:39:37.625050 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:39:37.625050 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:39:37.629587 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:39:37.630593 [error] [MainThread]:     	... 16 more
[0m01:39:37.631601 [error] [MainThread]:     
[0m01:39:37.633136 [info ] [MainThread]: 
[0m01:39:37.635228 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:39:37.635228 [debug] [MainThread]: Command `dbt run` failed at 01:39:37.635228 after 2.21 seconds
[0m01:39:37.637283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B912DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B9489C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B826B910>]}
[0m01:39:37.637283 [debug] [MainThread]: Flushing usage events
[0m01:41:32.605035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C182D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C182B90>]}


============================== 01:41:32.616732 | e4638bbe-0e2a-495d-8603-678101306d37 ==============================
[0m01:41:32.616732 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:41:32.616732 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:41:32.735228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C182DA0>]}
[0m01:41:32.739820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C526CB0>]}
[0m01:41:32.744325 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:41:32.773670 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:41:32.896877 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:41:32.896877 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m01:41:32.960182 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m01:41:33.074858 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m01:41:33.074858 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m01:41:33.104375 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m01:41:33.115006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C9C18D0>]}
[0m01:41:33.169183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009DB43D0>]}
[0m01:41:33.169183 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:41:33.169183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009DB5F00>]}
[0m01:41:33.174999 [info ] [MainThread]: 
[0m01:41:33.177116 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:41:33.180945 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:41:33.194277 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:41:33.195371 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:41:33.195371 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:41:33.285239 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:41:33.288856 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:41:33.296944 [debug] [ThreadPool]: On list_schemas: Close
[0m01:41:33.305060 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:41:33.317359 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:33.317864 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:41:33.318440 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:41:33.318945 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:41:33.536472 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:41:33.545013 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:41:33.545673 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:41:33.545673 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:41:33.545673 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:41:33.560785 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m01:41:33.564805 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:33.564805 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:41:33.564805 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:41:33.564805 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:41:33.849796 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:41:33.849796 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:41:33.860056 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:41:33.860056 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:41:33.860056 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:41:33.870635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C885570>]}
[0m01:41:33.874146 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:33.874146 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:41:33.875155 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:41:33.876169 [info ] [MainThread]: 
[0m01:41:33.876617 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m01:41:33.876617 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m01:41:33.876617 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m01:41:33.882834 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m01:41:33.907885 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m01:41:33.909894 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 01:41:33.882834 => 01:41:33.908884
[0m01:41:33.910636 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m01:41:33.934863 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m01:41:33.936379 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:33.936379 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m01:41:33.938045 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:41:33.938045 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:41:34.247758 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42000', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]\n                  +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))\n                     :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 82, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 83, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 84, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]\n                     :  :           :  :                                :           :              :- CTERelationDef 85, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]\n                     :  :           :  :                                :           :              :- CTERelationDef 86, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 88, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 89, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]\n                     :  :           :  :                                :                          :- CTERelationDef 90, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]\n                     :  :           :  :                                :                          :- CTERelationDef 91, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;\n'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (\n    SELECT * FROM ndb.as_of_date\n),\n\nnew_rows_as_of_dates AS (\n    SELECT\n        a.`hk_cli_cd`,\n        b.AS_OF_DATE\n    FROM ndb.h_cli AS a\n    LEFT JOIN as_of_dates AS b\n    ON a.`hk_cli_cd` = b.`hk_cli_cd`\n),\n\nnew_rows AS (\n    SELECT\n        a.`hk_cli_cd`,\n        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS \n    timestamp\n)) AS `S_NAME_LDTS`\n    FROM new_rows_as_of_dates AS a\n\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE\n    GROUP BY\n        a.`hk_cli_cd`, a.AS_OF_DATE\n),\ntemp as (\n    select \n    a.`hk_cli_cd`,\n    a.AS_OF_DATE as start_date,\n    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,\n    ndb.s_address.`addr`,\n        ndb.s_name.`name`,\n        'try' as t\n    from new_rows a\n    LEFT JOIN ndb.s_address AS `s_address_src`\n        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`\n        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`\n    LEFT JOIN ndb.s_name AS `s_name_src`\n        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`\n        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`\n    ),\npit AS (\n    SELECT * FROM temp\n)\n\nSELECT DISTINCT * FROM pit, false, true, PersistedView, false\n+- 'Distinct\n   +- 'Project [*]\n      +- 'SubqueryAlias pit\n         +- 'Project [*]\n            +- 'SubqueryAlias temp\n               +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]\n                  +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))\n                     :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))\n                     :  :- SubqueryAlias a\n                     :  :  +- SubqueryAlias new_rows\n                     :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]\n                     :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))\n                     :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))\n                     :  :           :  :- SubqueryAlias a\n                     :  :           :  :  +- SubqueryAlias new_rows_as_of_dates\n                     :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]\n                     :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)\n                     :  :           :  :           :- SubqueryAlias a\n                     :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli\n                     :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet\n                     :  :           :  :           +- SubqueryAlias b\n                     :  :           :  :              +- SubqueryAlias as_of_dates\n                     :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]\n                     :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date\n                     :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])\n                     :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]\n                     :  :           :  :                             +- WithCTE\n                     :  :           :  :                                :- CTERelationDef 82, false\n                     :  :           :  :                                :  +- SubqueryAlias as_of_date\n                     :  :           :  :                                :     +- Distinct\n                     :  :           :  :                                :        +- Union false, false\n                     :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]\n                     :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address\n                     :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])\n                     :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]\n                     :  :           :  :                                :           :           +- WithCTE\n                     :  :           :  :                                :           :              :- CTERelationDef 83, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address\n                     :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])\n                     :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]\n                     :  :           :  :                                :           :              :                 +- Distinct\n                     :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]\n                     :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)\n                     :  :           :  :                                :           :              :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])\n                     :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]\n                     :  :           :  :                                :           :              :                          :           +- WithCTE\n                     :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false\n                     :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :     +- Distinct\n                     :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]\n                     :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address\n                     :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2\n                     :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]\n                     :  :           :  :                                :           :              :- CTERelationDef 84, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias source_data\n                     :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]\n                     :  :           :  :                                :           :              :- CTERelationDef 85, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]\n                     :  :           :  :                                :           :              :- CTERelationDef 86, false\n                     :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]\n                     :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]\n                     :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]\n                     :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name\n                     :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])\n                     :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]\n                     :  :           :  :                                :                       +- WithCTE\n                     :  :           :  :                                :                          :- CTERelationDef 88, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]\n                     :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name\n                     :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])\n                     :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]\n                     :  :           :  :                                :                          :                 +- Distinct\n                     :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]\n                     :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)\n                     :  :           :  :                                :                          :                          :- SubqueryAlias v_h\n                     :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli\n                     :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])\n                     :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]\n                     :  :           :  :                                :                          :                          :           +- WithCTE\n                     :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false\n                     :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :     +- Distinct\n                     :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)\n                     :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub\n                     :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]\n                     :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli\n                     :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]\n                     :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name\n                     :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2\n                     :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]\n                     :  :           :  :                                :                          :- CTERelationDef 89, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]\n                     :  :           :  :                                :                          :        +- SubqueryAlias source_data\n                     :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]\n                     :  :           :  :                                :                          :- CTERelationDef 90, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]\n                     :  :           :  :                                :                          :        +- SubqueryAlias derived_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]\n                     :  :           :  :                                :                          :- CTERelationDef 91, false\n                     :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns\n                     :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]\n                     :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                :                             +- SubqueryAlias columns_to_select\n                     :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]\n                     :  :           :  :                                +- Distinct\n                     :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]\n                     :  :           :  :                                      +- SubqueryAlias as_of_date\n                     :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]\n                     :  :           :  +- SubqueryAlias s_address_src\n                     :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet\n                     :  :           +- SubqueryAlias s_name_src\n                     :  :              +- SubqueryAlias spark_catalog.ndb.s_name\n                     :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet\n                     :  +- SubqueryAlias s_address_src\n                     :     +- SubqueryAlias spark_catalog.ndb.s_address\n                     :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet\n                     +- SubqueryAlias s_name_src\n                        +- SubqueryAlias spark_catalog.ndb.s_name\n                           +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m01:41:34.249779 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m01:41:34.249779 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    ndb.s_address.`addr`,
        ndb.s_name.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:41:34.254818 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
                    +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
                       :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 82, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 83, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 84, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                       :  :           :  :                                :           :              :- CTERelationDef 85, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
                       :  :           :  :                                :           :              :- CTERelationDef 86, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 88, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 89, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                       :  :           :  :                                :                          :- CTERelationDef 90, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
                       :  :           :  :                                :                          :- CTERelationDef 91, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
                       :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
  	at java.base/java.security.AccessController.doPrivileged(Native Method)
  	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  	at java.base/java.lang.Thread.run(Unknown Source)
  Caused by: org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
  'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
      SELECT * FROM ndb.as_of_date
  ),
  
  new_rows_as_of_dates AS (
      SELECT
          a.`hk_cli_cd`,
          b.AS_OF_DATE
      FROM ndb.h_cli AS a
      LEFT JOIN as_of_dates AS b
      ON a.`hk_cli_cd` = b.`hk_cli_cd`
  ),
  
  new_rows AS (
      SELECT
          a.`hk_cli_cd`,
          a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
      timestamp
  )) AS `S_NAME_LDTS`
      FROM new_rows_as_of_dates AS a
  
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
      GROUP BY
          a.`hk_cli_cd`, a.AS_OF_DATE
  ),
  temp as (
      select 
      a.`hk_cli_cd`,
      a.AS_OF_DATE as start_date,
      lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
      ndb.s_address.`addr`,
          ndb.s_name.`name`,
          'try' as t
      from new_rows a
      LEFT JOIN ndb.s_address AS `s_address_src`
          ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
          AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
      LEFT JOIN ndb.s_name AS `s_name_src`
          ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
          AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
      ),
  pit AS (
      SELECT * FROM temp
  )
  
  SELECT DISTINCT * FROM pit, false, true, PersistedView, false
  +- 'Distinct
     +- 'Project [*]
        +- 'SubqueryAlias pit
           +- 'Project [*]
              +- 'SubqueryAlias temp
                 +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
                    +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
                       :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
                       :  :- SubqueryAlias a
                       :  :  +- SubqueryAlias new_rows
                       :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
                       :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
                       :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
                       :  :           :  :- SubqueryAlias a
                       :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                       :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
                       :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
                       :  :           :  :           :- SubqueryAlias a
                       :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                       :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
                       :  :           :  :           +- SubqueryAlias b
                       :  :           :  :              +- SubqueryAlias as_of_dates
                       :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
                       :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                       :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
                       :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
                       :  :           :  :                             +- WithCTE
                       :  :           :  :                                :- CTERelationDef 82, false
                       :  :           :  :                                :  +- SubqueryAlias as_of_date
                       :  :           :  :                                :     +- Distinct
                       :  :           :  :                                :        +- Union false, false
                       :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
                       :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                       :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
                       :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
                       :  :           :  :                                :           :           +- WithCTE
                       :  :           :  :                                :           :              :- CTERelationDef 83, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                       :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                       :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
                       :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
                       :  :           :  :                                :           :              :                 +- Distinct
                       :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
                       :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
                       :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                       :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                       :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                       :  :           :  :                                :           :              :                          :           +- WithCTE
                       :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
                       :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :     +- Distinct
                       :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
                       :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                       :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                       :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
                       :  :           :  :                                :           :              :- CTERelationDef 84, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
                       :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                       :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                       :  :           :  :                                :           :              :- CTERelationDef 85, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
                       :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
                       :  :           :  :                                :           :              :- CTERelationDef 86, false
                       :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
                       :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                       :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                       :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
                       :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                       :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
                       :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
                       :  :           :  :                                :                       +- WithCTE
                       :  :           :  :                                :                          :- CTERelationDef 88, false
                       :  :           :  :                                :                          :  +- SubqueryAlias source_data
                       :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                       :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                       :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
                       :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
                       :  :           :  :                                :                          :                 +- Distinct
                       :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
                       :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
                       :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                       :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                       :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                       :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                       :  :           :  :                                :                          :                          :           +- WithCTE
                       :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
                       :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :     +- Distinct
                       :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                       :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                       :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
                       :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                       :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
                       :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                       :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                       :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
                       :  :           :  :                                :                          :- CTERelationDef 89, false
                       :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
                       :  :           :  :                                :                          :        +- SubqueryAlias source_data
                       :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                       :  :           :  :                                :                          :- CTERelationDef 90, false
                       :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
                       :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
                       :  :           :  :                                :                          :- CTERelationDef 91, false
                       :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                       :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
                       :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                       :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                       :  :           :  :                                +- Distinct
                       :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
                       :  :           :  :                                      +- SubqueryAlias as_of_date
                       :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
                       :  :           :  +- SubqueryAlias s_address_src
                       :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
                       :  :           +- SubqueryAlias s_name_src
                       :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                       :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
                       :  +- SubqueryAlias s_address_src
                       :     +- SubqueryAlias spark_catalog.ndb.s_address
                       :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
                       +- SubqueryAlias s_name_src
                          +- SubqueryAlias spark_catalog.ndb.s_name
                             +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
  	... 16 more
  
[0m01:41:34.257098 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 01:41:33.910636 => 01:41:34.257098
[0m01:41:34.257098 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m01:41:34.257098 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:41:34.257098 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m01:41:34.275202 [debug] [Thread-1 (]: Runtime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
                      +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
                         :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 82, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 83, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 84, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                         :  :           :  :                                :           :              :- CTERelationDef 85, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
                         :  :           :  :                                :           :              :- CTERelationDef 86, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 88, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 89, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                         :  :           :  :                                :                          :- CTERelationDef 90, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
                         :  :           :  :                                :                          :- CTERelationDef 91, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
                         :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
    	at java.base/java.security.AccessController.doPrivileged(Native Method)
    	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    	at java.base/java.lang.Thread.run(Unknown Source)
    Caused by: org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
    'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
        SELECT * FROM ndb.as_of_date
    ),
    
    new_rows_as_of_dates AS (
        SELECT
            a.`hk_cli_cd`,
            b.AS_OF_DATE
        FROM ndb.h_cli AS a
        LEFT JOIN as_of_dates AS b
        ON a.`hk_cli_cd` = b.`hk_cli_cd`
    ),
    
    new_rows AS (
        SELECT
            a.`hk_cli_cd`,
            a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
        timestamp
    )) AS `S_NAME_LDTS`
        FROM new_rows_as_of_dates AS a
    
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
        GROUP BY
            a.`hk_cli_cd`, a.AS_OF_DATE
    ),
    temp as (
        select 
        a.`hk_cli_cd`,
        a.AS_OF_DATE as start_date,
        lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
        ndb.s_address.`addr`,
            ndb.s_name.`name`,
            'try' as t
        from new_rows a
        LEFT JOIN ndb.s_address AS `s_address_src`
            ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
            AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
        LEFT JOIN ndb.s_name AS `s_name_src`
            ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
            AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
        ),
    pit AS (
        SELECT * FROM temp
    )
    
    SELECT DISTINCT * FROM pit, false, true, PersistedView, false
    +- 'Distinct
       +- 'Project [*]
          +- 'SubqueryAlias pit
             +- 'Project [*]
                +- 'SubqueryAlias temp
                   +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
                      +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
                         :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
                         :  :- SubqueryAlias a
                         :  :  +- SubqueryAlias new_rows
                         :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
                         :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
                         :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
                         :  :           :  :- SubqueryAlias a
                         :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
                         :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
                         :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
                         :  :           :  :           :- SubqueryAlias a
                         :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
                         :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
                         :  :           :  :           +- SubqueryAlias b
                         :  :           :  :              +- SubqueryAlias as_of_dates
                         :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
                         :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
                         :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
                         :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
                         :  :           :  :                             +- WithCTE
                         :  :           :  :                                :- CTERelationDef 82, false
                         :  :           :  :                                :  +- SubqueryAlias as_of_date
                         :  :           :  :                                :     +- Distinct
                         :  :           :  :                                :        +- Union false, false
                         :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
                         :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
                         :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
                         :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
                         :  :           :  :                                :           :           +- WithCTE
                         :  :           :  :                                :           :              :- CTERelationDef 83, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                         :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
                         :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
                         :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
                         :  :           :  :                                :           :              :                 +- Distinct
                         :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
                         :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
                         :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
                         :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                         :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                         :  :           :  :                                :           :              :                          :           +- WithCTE
                         :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
                         :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :     +- Distinct
                         :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
                         :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
                         :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
                         :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
                         :  :           :  :                                :           :              :- CTERelationDef 84, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
                         :  :           :  :                                :           :              :        +- SubqueryAlias source_data
                         :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
                         :  :           :  :                                :           :              :- CTERelationDef 85, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
                         :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
                         :  :           :  :                                :           :              :- CTERelationDef 86, false
                         :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
                         :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
                         :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
                         :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
                         :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
                         :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
                         :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
                         :  :           :  :                                :                       +- WithCTE
                         :  :           :  :                                :                          :- CTERelationDef 88, false
                         :  :           :  :                                :                          :  +- SubqueryAlias source_data
                         :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                         :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
                         :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
                         :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
                         :  :           :  :                                :                          :                 +- Distinct
                         :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
                         :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
                         :  :           :  :                                :                          :                          :- SubqueryAlias v_h
                         :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
                         :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
                         :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
                         :  :           :  :                                :                          :                          :           +- WithCTE
                         :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
                         :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :     +- Distinct
                         :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
                         :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
                         :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
                         :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
                         :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
                         :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
                         :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
                         :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
                         :  :           :  :                                :                          :- CTERelationDef 89, false
                         :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
                         :  :           :  :                                :                          :        +- SubqueryAlias source_data
                         :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
                         :  :           :  :                                :                          :- CTERelationDef 90, false
                         :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
                         :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
                         :  :           :  :                                :                          :- CTERelationDef 91, false
                         :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
                         :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
                         :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                :                             +- SubqueryAlias columns_to_select
                         :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
                         :  :           :  :                                +- Distinct
                         :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
                         :  :           :  :                                      +- SubqueryAlias as_of_date
                         :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
                         :  :           :  +- SubqueryAlias s_address_src
                         :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
                         :  :           +- SubqueryAlias s_name_src
                         :  :              +- SubqueryAlias spark_catalog.ndb.s_name
                         :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
                         :  +- SubqueryAlias s_address_src
                         :     +- SubqueryAlias spark_catalog.ndb.s_address
                         :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
                         +- SubqueryAlias s_name_src
                            +- SubqueryAlias spark_catalog.ndb.s_name
                               +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
    	... 16 more
    
[0m01:41:34.279230 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e4638bbe-0e2a-495d-8603-678101306d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800C8A3790>]}
[0m01:41:34.279230 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model ndb.pit_client2 ........................... [[31mERROR[0m in 0.40s]
[0m01:41:34.285250 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m01:41:34.286884 [debug] [MainThread]: On master: ROLLBACK
[0m01:41:34.286884 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:41:34.327329 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:41:34.327329 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:34.329847 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:41:34.329847 [debug] [MainThread]: On master: ROLLBACK
[0m01:41:34.329847 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:41:34.330931 [debug] [MainThread]: On master: Close
[0m01:41:34.334984 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:41:34.337999 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:41:34.337999 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m01:41:34.337999 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m01:41:34.337999 [info ] [MainThread]: 
[0m01:41:34.343342 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m01:41:34.346661 [debug] [MainThread]: Command end result
[0m01:41:34.365252 [info ] [MainThread]: 
[0m01:41:34.365252 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:41:34.367156 [info ] [MainThread]: 
[0m01:41:34.367156 [error] [MainThread]: [33mRuntime Error in model pit_client2 (models\pit_test\pit_test\pit_client2.sql)[0m
[0m01:41:34.367156 [error] [MainThread]:   Database Error
[0m01:41:34.367156 [error] [MainThread]:     org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
[0m01:41:34.371643 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m01:41:34.372651 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m01:41:34.372651 [error] [MainThread]:     ),
[0m01:41:34.374157 [error] [MainThread]:     
[0m01:41:34.375167 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m01:41:34.375167 [error] [MainThread]:         SELECT
[0m01:41:34.376385 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:41:34.377971 [error] [MainThread]:             b.AS_OF_DATE
[0m01:41:34.377971 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m01:41:34.377971 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m01:41:34.377971 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m01:41:34.377971 [error] [MainThread]:     ),
[0m01:41:34.377971 [error] [MainThread]:     
[0m01:41:34.383978 [error] [MainThread]:     new_rows AS (
[0m01:41:34.383978 [error] [MainThread]:         SELECT
[0m01:41:34.384991 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:41:34.384991 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:41:34.386851 [error] [MainThread]:         timestamp
[0m01:41:34.388059 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:41:34.389065 [error] [MainThread]:         timestamp
[0m01:41:34.389065 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m01:41:34.390068 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m01:41:34.391065 [error] [MainThread]:     
[0m01:41:34.392064 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:41:34.392064 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:41:34.393548 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:41:34.395078 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:41:34.395078 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:41:34.395078 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:41:34.395078 [error] [MainThread]:         GROUP BY
[0m01:41:34.395078 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m01:41:34.395078 [error] [MainThread]:     ),
[0m01:41:34.395078 [error] [MainThread]:     temp as (
[0m01:41:34.395078 [error] [MainThread]:         select 
[0m01:41:34.395078 [error] [MainThread]:         a.`hk_cli_cd`,
[0m01:41:34.395078 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m01:41:34.395078 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m01:41:34.395078 [error] [MainThread]:         ndb.s_address.`addr`,
[0m01:41:34.395078 [error] [MainThread]:             ndb.s_name.`name`,
[0m01:41:34.395078 [error] [MainThread]:             'try' as t
[0m01:41:34.395078 [error] [MainThread]:         from new_rows a
[0m01:41:34.395078 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:41:34.404113 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:41:34.404673 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
[0m01:41:34.405179 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:41:34.405179 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:41:34.406187 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
[0m01:41:34.406837 [error] [MainThread]:         ),
[0m01:41:34.407376 [error] [MainThread]:     pit AS (
[0m01:41:34.407376 [error] [MainThread]:         SELECT * FROM temp
[0m01:41:34.407376 [error] [MainThread]:     )
[0m01:41:34.407376 [error] [MainThread]:     
[0m01:41:34.407376 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m01:41:34.407376 [error] [MainThread]:     +- 'Distinct
[0m01:41:34.407376 [error] [MainThread]:        +- 'Project [*]
[0m01:41:34.407376 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m01:41:34.407376 [error] [MainThread]:              +- 'Project [*]
[0m01:41:34.407376 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m01:41:34.407376 [error] [MainThread]:                    +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
[0m01:41:34.407376 [error] [MainThread]:                       +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
[0m01:41:34.407376 [error] [MainThread]:                          :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
[0m01:41:34.407376 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m01:41:34.414442 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m01:41:34.414947 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
[0m01:41:34.414947 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m01:41:34.414947 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
[0m01:41:34.421331 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m01:41:34.421331 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
[0m01:41:34.422342 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
[0m01:41:34.423339 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m01:41:34.423764 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 82, false
[0m01:41:34.424270 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 83, false
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
[0m01:41:34.425290 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
[0m01:41:34.431029 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m01:41:34.434051 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:41:34.434051 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
[0m01:41:34.435060 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
[0m01:41:34.435060 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m01:41:34.435060 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
[0m01:41:34.436644 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m01:41:34.436644 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m01:41:34.438009 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
[0m01:41:34.438009 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m01:41:34.439015 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m01:41:34.440572 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:41:34.441401 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
[0m01:41:34.441401 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.441401 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m01:41:34.441401 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.444411 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 84, false
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 85, false
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 86, false
[0m01:41:34.444933 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m01:41:34.453993 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.455252 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m01:41:34.456263 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
[0m01:41:34.457554 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m01:41:34.458479 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 88, false
[0m01:41:34.464056 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m01:41:34.464056 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
[0m01:41:34.465062 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m01:41:34.471405 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
[0m01:41:34.471405 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m01:41:34.472413 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m01:41:34.473930 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
[0m01:41:34.474964 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m01:41:34.474964 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m01:41:34.474964 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 89, false
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m01:41:34.476990 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
[0m01:41:34.484000 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 90, false
[0m01:41:34.485014 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m01:41:34.485014 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
[0m01:41:34.485014 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m01:41:34.485014 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
[0m01:41:34.485014 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 91, false
[0m01:41:34.488048 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m01:41:34.489054 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.490058 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m01:41:34.491068 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
[0m01:41:34.492055 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.492055 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m01:41:34.493930 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.494972 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m01:41:34.494972 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
[0m01:41:34.497080 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m01:41:34.497080 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
[0m01:41:34.497080 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m01:41:34.497080 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:41:34.497080 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
[0m01:41:34.497080 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m01:41:34.497080 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:41:34.497080 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
[0m01:41:34.497080 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m01:41:34.497080 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:41:34.497080 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
[0m01:41:34.497080 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m01:41:34.497080 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:41:34.504694 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
[0m01:41:34.505200 [error] [MainThread]:     
[0m01:41:34.505200 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
[0m01:41:34.506208 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
[0m01:41:34.507205 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)
[0m01:41:34.508206 [error] [MainThread]:     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[0m01:41:34.509209 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
[0m01:41:34.510205 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
[0m01:41:34.510502 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
[0m01:41:34.510502 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)
[0m01:41:34.510502 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)
[0m01:41:34.510502 [error] [MainThread]:     	at java.base/java.security.AccessController.doPrivileged(Native Method)
[0m01:41:34.514008 [error] [MainThread]:     	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
[0m01:41:34.514008 [error] [MainThread]:     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
[0m01:41:34.514008 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)
[0m01:41:34.515024 [error] [MainThread]:     	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[0m01:41:34.515024 [error] [MainThread]:     	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[0m01:41:34.515024 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[0m01:41:34.516573 [error] [MainThread]:     	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[0m01:41:34.516573 [error] [MainThread]:     	at java.base/java.lang.Thread.run(Unknown Source)
[0m01:41:34.516573 [error] [MainThread]:     Caused by: org.apache.spark.sql.AnalysisException: Column 'ndb.s_address.addr' does not exist. Did you mean one of the following? [s_address_src.addr, s_address_src.ld_dt_tm, s_name_src.name, a.S_ADDRESS_LDTS, a.S_ADDRESS_PK, a.S_NAME_LDTS, a.S_NAME_PK, a.hk_cli_cd, a.AS_OF_DATE, s_name_src.ld_dt_tm, s_address_src.rcrd_src_nm, s_address_src.rcrd_hsh_id, s_name_src.rcrd_hsh_id, s_name_src.rcrd_src_nm, s_address_src.hsh_ky_cli_cd, s_address_src.EFFECTIVE_FROM, s_name_src.hsh_ky_cli_cd, s_name_src.EFFECTIVE_FROM, s_name_src._hoodie_file_name, s_name_src._hoodie_record_key, s_address_src._hoodie_file_name, s_address_src._hoodie_record_key, s_name_src._hoodie_commit_time, s_name_src._hoodie_commit_seqno, s_address_src._hoodie_commit_time, s_address_src._hoodie_commit_seqno, s_address_src._hoodie_partition_path, s_name_src._hoodie_partition_path]; line 58 pos 4;
[0m01:41:34.516573 [error] [MainThread]:     'CreateViewCommand `ndb`.`pit_client2`, WITH as_of_dates AS (
[0m01:41:34.516573 [error] [MainThread]:         SELECT * FROM ndb.as_of_date
[0m01:41:34.516573 [error] [MainThread]:     ),
[0m01:41:34.516573 [error] [MainThread]:     
[0m01:41:34.516573 [error] [MainThread]:     new_rows_as_of_dates AS (
[0m01:41:34.516573 [error] [MainThread]:         SELECT
[0m01:41:34.521366 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:41:34.521366 [error] [MainThread]:             b.AS_OF_DATE
[0m01:41:34.522377 [error] [MainThread]:         FROM ndb.h_cli AS a
[0m01:41:34.523862 [error] [MainThread]:         LEFT JOIN as_of_dates AS b
[0m01:41:34.524887 [error] [MainThread]:         ON a.`hk_cli_cd` = b.`hk_cli_cd`
[0m01:41:34.524887 [error] [MainThread]:     ),
[0m01:41:34.524887 [error] [MainThread]:     
[0m01:41:34.524887 [error] [MainThread]:     new_rows AS (
[0m01:41:34.524887 [error] [MainThread]:         SELECT
[0m01:41:34.524887 [error] [MainThread]:             a.`hk_cli_cd`,
[0m01:41:34.524887 [error] [MainThread]:             a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:41:34.524887 [error] [MainThread]:         timestamp
[0m01:41:34.524887 [error] [MainThread]:     )) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
[0m01:41:34.524887 [error] [MainThread]:         timestamp
[0m01:41:34.524887 [error] [MainThread]:     )) AS `S_NAME_LDTS`
[0m01:41:34.531071 [error] [MainThread]:         FROM new_rows_as_of_dates AS a
[0m01:41:34.531071 [error] [MainThread]:     
[0m01:41:34.531071 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:41:34.531071 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:41:34.531071 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:41:34.531071 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:41:34.531071 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:41:34.534105 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
[0m01:41:34.534105 [error] [MainThread]:         GROUP BY
[0m01:41:34.535117 [error] [MainThread]:             a.`hk_cli_cd`, a.AS_OF_DATE
[0m01:41:34.535117 [error] [MainThread]:     ),
[0m01:41:34.535117 [error] [MainThread]:     temp as (
[0m01:41:34.536631 [error] [MainThread]:         select 
[0m01:41:34.536631 [error] [MainThread]:         a.`hk_cli_cd`,
[0m01:41:34.536631 [error] [MainThread]:         a.AS_OF_DATE as start_date,
[0m01:41:34.538081 [error] [MainThread]:         lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
[0m01:41:34.539088 [error] [MainThread]:         ndb.s_address.`addr`,
[0m01:41:34.540088 [error] [MainThread]:             ndb.s_name.`name`,
[0m01:41:34.540884 [error] [MainThread]:             'try' as t
[0m01:41:34.540884 [error] [MainThread]:         from new_rows a
[0m01:41:34.540884 [error] [MainThread]:         LEFT JOIN ndb.s_address AS `s_address_src`
[0m01:41:34.540884 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
[0m01:41:34.540884 [error] [MainThread]:             AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
[0m01:41:34.540884 [error] [MainThread]:         LEFT JOIN ndb.s_name AS `s_name_src`
[0m01:41:34.544392 [error] [MainThread]:             ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
[0m01:41:34.544906 [error] [MainThread]:             AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
[0m01:41:34.544906 [error] [MainThread]:         ),
[0m01:41:34.544906 [error] [MainThread]:     pit AS (
[0m01:41:34.544906 [error] [MainThread]:         SELECT * FROM temp
[0m01:41:34.544906 [error] [MainThread]:     )
[0m01:41:34.544906 [error] [MainThread]:     
[0m01:41:34.544906 [error] [MainThread]:     SELECT DISTINCT * FROM pit, false, true, PersistedView, false
[0m01:41:34.544906 [error] [MainThread]:     +- 'Distinct
[0m01:41:34.544906 [error] [MainThread]:        +- 'Project [*]
[0m01:41:34.544906 [error] [MainThread]:           +- 'SubqueryAlias pit
[0m01:41:34.544906 [error] [MainThread]:              +- 'Project [*]
[0m01:41:34.544906 [error] [MainThread]:                 +- 'SubqueryAlias temp
[0m01:41:34.544906 [error] [MainThread]:                    +- 'Project [hk_cli_cd#1483, AS_OF_DATE#1431 AS start_date#1425, lead(as_of_date#1431, 1, null) windowspecdefinition(hk_cli_cd#1483, as_of_date#1431 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS end_date#1426, 'ndb.s_address.addr, 'ndb.s_name.name, try AS t#1427]
[0m01:41:34.544906 [error] [MainThread]:                       +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1525) AND (EFFECTIVE_FROM#1528 = S_NAME_LDTS#1424))
[0m01:41:34.544906 [error] [MainThread]:                          :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1514) AND (EFFECTIVE_FROM#1517 = S_ADDRESS_LDTS#1422))
[0m01:41:34.544906 [error] [MainThread]:                          :  :- SubqueryAlias a
[0m01:41:34.544906 [error] [MainThread]:                          :  :  +- SubqueryAlias new_rows
[0m01:41:34.544906 [error] [MainThread]:                          :  :     +- Aggregate [hk_cli_cd#1483, AS_OF_DATE#1431], [hk_cli_cd#1483, AS_OF_DATE#1431, coalesce(max(hsh_ky_cli_cd#1492), cast(0000000000000000 as string)) AS S_ADDRESS_PK#1421, coalesce(max(EFFECTIVE_FROM#1495), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_ADDRESS_LDTS#1422, coalesce(max(hsh_ky_cli_cd#1503), cast(0000000000000000 as string)) AS S_NAME_PK#1423, coalesce(max(EFFECTIVE_FROM#1506), cast(1900-01-01 00:00:00.000 as timestamp)) AS S_NAME_LDTS#1424]
[0m01:41:34.544906 [error] [MainThread]:                          :  :        +- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1503) AND (EFFECTIVE_FROM#1506 <= AS_OF_DATE#1431))
[0m01:41:34.544906 [error] [MainThread]:                          :  :           :- Join LeftOuter, ((hk_cli_cd#1483 = hsh_ky_cli_cd#1492) AND (EFFECTIVE_FROM#1495 <= AS_OF_DATE#1431))
[0m01:41:34.554087 [error] [MainThread]:                          :  :           :  :- SubqueryAlias a
[0m01:41:34.554971 [error] [MainThread]:                          :  :           :  :  +- SubqueryAlias new_rows_as_of_dates
[0m01:41:34.554971 [error] [MainThread]:                          :  :           :  :     +- Project [hk_cli_cd#1483, AS_OF_DATE#1431]
[0m01:41:34.555978 [error] [MainThread]:                          :  :           :  :        +- Join LeftOuter, (hk_cli_cd#1483 = hk_cli_cd#1430)
[0m01:41:34.556983 [error] [MainThread]:                          :  :           :  :           :- SubqueryAlias a
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :           :  +- SubqueryAlias spark_catalog.ndb.h_cli
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :           :     +- Relation ndb.h_cli[_hoodie_commit_time#1478,_hoodie_commit_seqno#1479,_hoodie_record_key#1480,_hoodie_partition_path#1481,_hoodie_file_name#1482,hk_cli_cd#1483,cli_id#1484,ld_dt_tm#1485,rcrd_src_nm#1486] parquet
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :           +- SubqueryAlias b
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :              +- SubqueryAlias as_of_dates
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                 +- Project [hk_cli_cd#1430, AS_OF_DATE#1431]
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                    +- SubqueryAlias spark_catalog.ndb.as_of_date
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                       +- View (`ndb`.`as_of_date`, [hk_cli_cd#1430,AS_OF_DATE#1431])
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                          +- Project [cast(hk_cli_cd#1428 as string) AS hk_cli_cd#1430, cast(AS_OF_DATE#1429 as timestamp) AS AS_OF_DATE#1431]
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                             +- WithCTE
[0m01:41:34.557543 [error] [MainThread]:                          :  :           :  :                                :- CTERelationDef 82, false
[0m01:41:34.564049 [error] [MainThread]:                          :  :           :  :                                :  +- SubqueryAlias as_of_date
[0m01:41:34.564049 [error] [MainThread]:                          :  :           :  :                                :     +- Distinct
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :        +- Union false, false
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :- Project [hsh_ky_cli_cd#1442, ts#1439]
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :  +- SubqueryAlias spark_catalog.ndb.v_stg_s_address
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :     +- View (`ndb`.`v_stg_s_address`, [CLI_ID#1436,RCRD_SRC_NM#1437,ADDR#1438,TS#1439,LD_DT_TM#1440,EFFECTIVE_FROM#1441,HSH_KY_CLI_CD#1442,RCRD_HSH_ID#1443])
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :        +- Project [cast(CLI_ID#1444 as string) AS CLI_ID#1436, cast(RCRD_SRC_NM#1445 as string) AS RCRD_SRC_NM#1437, cast(ADDR#1446 as string) AS ADDR#1438, cast(TS#1447 as timestamp) AS TS#1439, cast(LD_DT_TM#1432 as timestamp) AS LD_DT_TM#1440, cast(EFFECTIVE_FROM#1433 as timestamp) AS EFFECTIVE_FROM#1441, cast(HSH_KY_CLI_CD#1434 as string) AS HSH_KY_CLI_CD#1442, cast(RCRD_HSH_ID#1435 as string) AS RCRD_HSH_ID#1443]
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :           +- WithCTE
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 83, false
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias source_data
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias spark_catalog.ndb.raw_s_address
[0m01:41:34.565062 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- View (`ndb`.`raw_s_address`, [cli_id#1444,rcrd_src_nm#1445,addr#1446,ts#1447])
[0m01:41:34.571397 [error] [MainThread]:                          :  :           :  :                                :           :              :              +- Project [cast(cli_id#1450 as string) AS cli_id#1444, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1445, cast(addr#1454 as string) AS addr#1446, cast(ts#1455 as timestamp) AS ts#1447]
[0m01:41:34.572404 [error] [MainThread]:                          :  :           :  :                                :           :              :                 +- Distinct
[0m01:41:34.573409 [error] [MainThread]:                          :  :           :  :                                :           :              :                    +- Project [cli_id#1450, rcrd_src_nm#1451, addr#1454, ts#1455]
[0m01:41:34.575043 [error] [MainThread]:                          :  :           :  :                                :           :              :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1453)
[0m01:41:34.575043 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :- SubqueryAlias v_h
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :           +- WithCTE
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :- CTERelationDef 87, false
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :  +- SubqueryAlias cli
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :     +- Distinct
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :        +- Project [trim(cast(cli_id#1452 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :           +- Filter (1 = 1)
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :              +- SubqueryAlias cli
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:41:34.577045 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1452], Partition Cols: []]
[0m01:41:34.584061 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.584061 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                 +- SubqueryAlias cli
[0m01:41:34.585074 [error] [MainThread]:                          :  :           :  :                                :           :              :                          :                    +- CTERelationRef 87, true, [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.585074 [error] [MainThread]:                          :  :           :  :                                :           :              :                          +- SubqueryAlias v_s_address
[0m01:41:34.585074 [error] [MainThread]:                          :  :           :  :                                :           :              :                             +- SubqueryAlias spark_catalog.ndb.address2
[0m01:41:34.585074 [error] [MainThread]:                          :  :           :  :                                :           :              :                                +- HiveTableRelation [`ndb`.`address2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1453, addr#1454, ts#1455], Partition Cols: []]
[0m01:41:34.588031 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 84, false
[0m01:41:34.588031 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias derived_columns
[0m01:41:34.589038 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, current_timestamp() AS ld_dt_tm#1432, ts#1447 AS EFFECTIVE_FROM#1433]
[0m01:41:34.590114 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias source_data
[0m01:41:34.590114 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 83, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447]
[0m01:41:34.590114 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 85, false
[0m01:41:34.590114 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias hashed_columns
[0m01:41:34.594232 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, cast(md5(cast(nullif(upper(trim(cast(cli_id#1444 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1434, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(addr#1446 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1435]
[0m01:41:34.594232 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias derived_columns
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 84, true, [cli_id#1444, rcrd_src_nm#1445, addr#1446, ts#1447, ld_dt_tm#1432, EFFECTIVE_FROM#1433]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :- CTERelationDef 86, false
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :  +- SubqueryAlias columns_to_select
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :     +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :        +- SubqueryAlias hashed_columns
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              :           +- CTERelationRef 85, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, hsh_ky_cli_cd#1434, rcrd_hsh_id#1435]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :              +- Project [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :                 +- SubqueryAlias columns_to_select
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           :                    +- CTERelationRef 86, true, [CLI_ID#1444, RCRD_SRC_NM#1445, ADDR#1446, TS#1447, LD_DT_TM#1432, EFFECTIVE_FROM#1433, HSH_KY_CLI_CD#1434, RCRD_HSH_ID#1435]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :           +- Project [hsh_ky_cli_cd#1467, ts#1464]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :              +- SubqueryAlias spark_catalog.ndb.v_stg_s_name
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :                 +- View (`ndb`.`v_stg_s_name`, [CLI_ID#1461,RCRD_SRC_NM#1462,NAME#1463,TS#1464,LD_DT_TM#1465,EFFECTIVE_FROM#1466,HSH_KY_CLI_CD#1467,RCRD_HSH_ID#1468])
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :                    +- Project [cast(CLI_ID#1469 as string) AS CLI_ID#1461, cast(RCRD_SRC_NM#1470 as string) AS RCRD_SRC_NM#1462, cast(NAME#1471 as string) AS NAME#1463, cast(TS#1472 as timestamp) AS TS#1464, cast(LD_DT_TM#1457 as timestamp) AS LD_DT_TM#1465, cast(EFFECTIVE_FROM#1458 as timestamp) AS EFFECTIVE_FROM#1466, cast(HSH_KY_CLI_CD#1459 as string) AS HSH_KY_CLI_CD#1467, cast(RCRD_HSH_ID#1460 as string) AS RCRD_HSH_ID#1468]
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :                       +- WithCTE
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 88, false
[0m01:41:34.595253 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias source_data
[0m01:41:34.604048 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
[0m01:41:34.604930 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias spark_catalog.ndb.raw_s_name
[0m01:41:34.604930 [error] [MainThread]:                          :  :           :  :                                :                          :           +- View (`ndb`.`raw_s_name`, [cli_id#1469,rcrd_src_nm#1470,name#1471,ts#1472])
[0m01:41:34.607008 [error] [MainThread]:                          :  :           :  :                                :                          :              +- Project [cast(cli_id#1450 as string) AS cli_id#1469, cast(rcrd_src_nm#1451 as string) AS rcrd_src_nm#1470, cast(name#1475 as string) AS name#1471, cast(ts#1476 as timestamp) AS ts#1472]
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                 +- Distinct
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                    +- Project [cli_id#1450, rcrd_src_nm#1451, name#1475, ts#1476]
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                       +- Join LeftOuter, (cast(cli_id#1450 as int) = cli_id#1474)
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :- SubqueryAlias v_h
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :  +- SubqueryAlias spark_catalog.ndb.raw_h_cli
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :     +- View (`ndb`.`raw_h_cli`, [cli_id#1450,rcrd_src_nm#1451])
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :        +- Project [cast(cli_id#1448 as string) AS cli_id#1450, cast(rcrd_src_nm#1449 as string) AS rcrd_src_nm#1451]
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :           +- WithCTE
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :- CTERelationDef 92, false
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :  +- SubqueryAlias cli
[0m01:41:34.607359 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :     +- Distinct
[0m01:41:34.614365 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :        +- Project [trim(cast(cli_id#1473 as string), None) AS cli_id#1448, dummy AS rcrd_src_nm#1449]
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :           +- Filter (1 = 1)
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :              +- SubqueryAlias cli
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                 +- SubqueryAlias spark_catalog.ndb.cli_hub
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              :                    +- HiveTableRelation [`ndb`.`cli_hub`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1473], Partition Cols: []]
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :              +- Project [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                 +- SubqueryAlias cli
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          :                    +- CTERelationRef 92, true, [cli_id#1448, rcrd_src_nm#1449]
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                          +- SubqueryAlias v_s_name
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                             +- SubqueryAlias spark_catalog.ndb.name2
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :                                +- HiveTableRelation [`ndb`.`name2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [cli_id#1474, name#1475, ts#1476], Partition Cols: []]
[0m01:41:34.614874 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 89, false
[0m01:41:34.621396 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias derived_columns
[0m01:41:34.622405 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, current_timestamp() AS ld_dt_tm#1457, ts#1472 AS EFFECTIVE_FROM#1458]
[0m01:41:34.623416 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias source_data
[0m01:41:34.624157 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 88, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472]
[0m01:41:34.625194 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 90, false
[0m01:41:34.625194 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias hashed_columns
[0m01:41:34.625194 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, cast(md5(cast(nullif(upper(trim(cast(cli_id#1469 as string), None)), ) as binary)) as string) AS hsh_ky_cli_cd#1459, cast(md5(cast(nullif(concat_ws(||, ifnull(nullif(upper(trim(cast(name#1471 as string), None)), ), ^^)), ^^) as binary)) as string) AS rcrd_hsh_id#1460]
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias derived_columns
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 89, true, [cli_id#1469, rcrd_src_nm#1470, name#1471, ts#1472, ld_dt_tm#1457, EFFECTIVE_FROM#1458]
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :- CTERelationDef 91, false
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :  +- SubqueryAlias columns_to_select
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :     +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :        +- SubqueryAlias hashed_columns
[0m01:41:34.627223 [error] [MainThread]:                          :  :           :  :                                :                          :           +- CTERelationRef 90, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, hsh_ky_cli_cd#1459, rcrd_hsh_id#1460]
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                :                          +- Project [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                :                             +- SubqueryAlias columns_to_select
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                :                                +- CTERelationRef 91, true, [CLI_ID#1469, RCRD_SRC_NM#1470, NAME#1471, TS#1472, LD_DT_TM#1457, EFFECTIVE_FROM#1458, HSH_KY_CLI_CD#1459, RCRD_HSH_ID#1460]
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                +- Distinct
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                   +- Project [hsh_ky_cli_cd#1442 AS hk_cli_cd#1428, ts#1439 AS AS_OF_DATE#1429]
[0m01:41:34.631238 [error] [MainThread]:                          :  :           :  :                                      +- SubqueryAlias as_of_date
[0m01:41:34.634260 [error] [MainThread]:                          :  :           :  :                                         +- CTERelationRef 82, true, [hsh_ky_cli_cd#1442, ts#1439]
[0m01:41:34.635273 [error] [MainThread]:                          :  :           :  +- SubqueryAlias s_address_src
[0m01:41:34.635273 [error] [MainThread]:                          :  :           :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:41:34.635273 [error] [MainThread]:                          :  :           :        +- Relation ndb.s_address[_hoodie_commit_time#1487,_hoodie_commit_seqno#1488,_hoodie_record_key#1489,_hoodie_partition_path#1490,_hoodie_file_name#1491,hsh_ky_cli_cd#1492,rcrd_hsh_id#1493,addr#1494,EFFECTIVE_FROM#1495,ld_dt_tm#1496,rcrd_src_nm#1497] parquet
[0m01:41:34.635273 [error] [MainThread]:                          :  :           +- SubqueryAlias s_name_src
[0m01:41:34.635273 [error] [MainThread]:                          :  :              +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:41:34.635273 [error] [MainThread]:                          :  :                 +- Relation ndb.s_name[_hoodie_commit_time#1498,_hoodie_commit_seqno#1499,_hoodie_record_key#1500,_hoodie_partition_path#1501,_hoodie_file_name#1502,hsh_ky_cli_cd#1503,rcrd_hsh_id#1504,name#1505,EFFECTIVE_FROM#1506,ld_dt_tm#1507,rcrd_src_nm#1508] parquet
[0m01:41:34.638040 [error] [MainThread]:                          :  +- SubqueryAlias s_address_src
[0m01:41:34.639048 [error] [MainThread]:                          :     +- SubqueryAlias spark_catalog.ndb.s_address
[0m01:41:34.639048 [error] [MainThread]:                          :        +- Relation ndb.s_address[_hoodie_commit_time#1509,_hoodie_commit_seqno#1510,_hoodie_record_key#1511,_hoodie_partition_path#1512,_hoodie_file_name#1513,hsh_ky_cli_cd#1514,rcrd_hsh_id#1515,addr#1516,EFFECTIVE_FROM#1517,ld_dt_tm#1518,rcrd_src_nm#1519] parquet
[0m01:41:34.640526 [error] [MainThread]:                          +- SubqueryAlias s_name_src
[0m01:41:34.640526 [error] [MainThread]:                             +- SubqueryAlias spark_catalog.ndb.s_name
[0m01:41:34.640526 [error] [MainThread]:                                +- Relation ndb.s_name[_hoodie_commit_time#1520,_hoodie_commit_seqno#1521,_hoodie_record_key#1522,_hoodie_partition_path#1523,_hoodie_file_name#1524,hsh_ky_cli_cd#1525,rcrd_hsh_id#1526,name#1527,EFFECTIVE_FROM#1528,ld_dt_tm#1529,rcrd_src_nm#1530] parquet
[0m01:41:34.640526 [error] [MainThread]:     
[0m01:41:34.640526 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
[0m01:41:34.644033 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:200)
[0m01:41:34.644033 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:193)
[0m01:41:34.645046 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:41:34.645046 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:193)
[0m01:41:34.645046 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:193)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:193)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:102)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.646635 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:41:34.646635 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:41:34.654147 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:41:34.654707 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:41:34.655216 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:41:34.656222 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:41:34.657224 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:41:34.657475 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:41:34.657475 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:41:34.663984 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:41:34.663984 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:41:34.665000 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:41:34.665000 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:41:34.666816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.666816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.666816 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.666816 [error] [MainThread]:     	at scala.collection.Iterator.foreach(Iterator.scala:943)
[0m01:41:34.666816 [error] [MainThread]:     	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[0m01:41:34.666816 [error] [MainThread]:     	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[0m01:41:34.666816 [error] [MainThread]:     	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[0m01:41:34.671406 [error] [MainThread]:     	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[0m01:41:34.672415 [error] [MainThread]:     	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[0m01:41:34.672415 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.674892 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)
[0m01:41:34.674892 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)
[0m01:41:34.674892 [error] [MainThread]:     	at scala.collection.immutable.List.foreach(List.scala:431)
[0m01:41:34.674892 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)
[0m01:41:34.674892 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:102)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:97)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:214)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
[0m01:41:34.679412 [error] [MainThread]:     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[0m01:41:34.684944 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
[0m01:41:34.684944 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
[0m01:41:34.684944 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
[0m01:41:34.686840 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:41:34.686840 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
[0m01:41:34.688065 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
[0m01:41:34.690073 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
[0m01:41:34.691251 [error] [MainThread]:     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
[0m01:41:34.691776 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
[0m01:41:34.691776 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
[0m01:41:34.694804 [error] [MainThread]:     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)
[0m01:41:34.698713 [error] [MainThread]:     	... 16 more
[0m01:41:34.698713 [error] [MainThread]:     
[0m01:41:34.698713 [info ] [MainThread]: 
[0m01:41:34.698713 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:41:34.698713 [debug] [MainThread]: Command `dbt run` failed at 01:41:34.698713 after 2.11 seconds
[0m01:41:34.704225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009BF7EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028009DB43D0>]}
[0m01:41:34.704764 [debug] [MainThread]: Flushing usage events
[0m01:43:26.185242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EA10DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EC822DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EC822C20>]}


============================== 01:43:26.185242 | 21a8caac-c675-459e-92bd-f1087e7e3833 ==============================
[0m01:43:26.185242 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:43:26.194990 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:43:26.315129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EC822E30>]}
[0m01:43:26.328856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ECBC2F50>]}
[0m01:43:26.328856 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:43:26.357271 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:43:26.481742 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:43:26.481742 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m01:43:26.544497 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m01:43:26.655128 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m01:43:26.667241 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m01:43:26.695189 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m01:43:26.714922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ED061900>]}
[0m01:43:26.766317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198E9247A30>]}
[0m01:43:26.766317 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:43:26.766317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198E9247970>]}
[0m01:43:26.766317 [info ] [MainThread]: 
[0m01:43:26.774921 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:43:26.776761 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:43:26.787466 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:43:26.787466 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:43:26.787466 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:43:26.877039 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:43:26.877039 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:43:26.885209 [debug] [ThreadPool]: On list_schemas: Close
[0m01:43:26.896701 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_ndb'
[0m01:43:26.897023 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:43:26.897023 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:43:26.897023 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:43:26.904039 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:43:27.124860 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:43:27.124860 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:43:27.128380 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:43:27.134399 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:43:27.134959 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:43:27.137408 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_ndb, now list_None_spark_catalog.ndb)
[0m01:43:27.146462 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:43:27.146462 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:43:27.146462 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:43:27.146462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:43:27.345135 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:43:27.347217 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:43:27.358081 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:43:27.358081 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:43:27.358081 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:43:27.372741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ECF26E60>]}
[0m01:43:27.374817 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:43:27.374817 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:43:27.374817 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:43:27.374817 [info ] [MainThread]: 
[0m01:43:27.382725 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m01:43:27.384245 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m01:43:27.385258 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m01:43:27.386239 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m01:43:27.417707 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m01:43:27.418997 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 01:43:27.387247 => 01:43:27.418997
[0m01:43:27.418997 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m01:43:27.445004 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m01:43:27.446844 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:43:27.448858 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m01:43:27.449860 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `s_address_src`.`addr`,
        `s_name_src`.`name`,
        'try' as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:43:27.449860 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:43:27.816946 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:43:27.816946 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:43:27.824983 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 01:43:27.418997 => 01:43:27.824983
[0m01:43:27.824983 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m01:43:27.829503 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:43:27.829503 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m01:43:27.837043 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21a8caac-c675-459e-92bd-f1087e7e3833', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198ECF437F0>]}
[0m01:43:27.837043 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.pit_client2 ............................... [[32mOK[0m in 0.45s]
[0m01:43:27.837043 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m01:43:27.844111 [debug] [MainThread]: On master: ROLLBACK
[0m01:43:27.844111 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:43:27.887035 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:43:27.887035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:43:27.887035 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:43:27.887035 [debug] [MainThread]: On master: ROLLBACK
[0m01:43:27.887035 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:43:27.887035 [debug] [MainThread]: On master: Close
[0m01:43:27.896503 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:43:27.897509 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:43:27.898510 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m01:43:27.899510 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m01:43:27.899813 [info ] [MainThread]: 
[0m01:43:27.900320 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m01:43:27.900320 [debug] [MainThread]: Command end result
[0m01:43:27.916668 [info ] [MainThread]: 
[0m01:43:27.917679 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:43:27.918689 [info ] [MainThread]: 
[0m01:43:27.919670 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:43:27.920955 [debug] [MainThread]: Command `dbt run` succeeded at 01:43:27.920955 after 1.75 seconds
[0m01:43:27.920955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EA10DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198EACF9240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198E9247F10>]}
[0m01:43:27.920955 [debug] [MainThread]: Flushing usage events
[0m01:48:21.177598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002082423DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020826952DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020826952BF0>]}


============================== 01:48:21.185128 | 4b8d18b6-5076-4c9d-a995-9f5a1dea978f ==============================
[0m01:48:21.185128 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:48:21.186135 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:48:21.325206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020826952E00>]}
[0m01:48:21.335293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020826CF6DA0>]}
[0m01:48:21.335293 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:48:21.354950 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:48:21.484046 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:48:21.485092 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m01:48:21.549403 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m01:48:21.688870 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m01:48:21.690917 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m01:48:21.732657 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m01:48:21.752243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020827191870>]}
[0m01:48:21.805300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020824599C60>]}
[0m01:48:21.805300 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:48:21.805300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020824599AB0>]}
[0m01:48:21.805300 [info ] [MainThread]: 
[0m01:48:21.805300 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:48:21.805300 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:48:21.823276 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:48:21.824234 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:48:21.824234 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:48:21.918021 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:48:21.919018 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:48:21.924960 [debug] [ThreadPool]: On list_schemas: Close
[0m01:48:21.936849 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m01:48:21.936849 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:48:21.936849 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:48:21.944380 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:48:21.944380 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:48:22.165151 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:48:22.165151 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:48:22.165151 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:48:22.165151 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:48:22.174449 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:48:22.177735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m01:48:22.189164 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:48:22.189164 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m01:48:22.189164 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m01:48:22.189164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:48:22.390593 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:48:22.390593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:48:22.398980 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m01:48:22.398980 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:48:22.398980 [debug] [ThreadPool]: On list_None_ndb: Close
[0m01:48:22.410321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208270543D0>]}
[0m01:48:22.414894 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:48:22.414894 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:48:22.414894 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:48:22.414894 [info ] [MainThread]: 
[0m01:48:22.423192 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m01:48:22.424223 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m01:48:22.426118 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m01:48:22.426118 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m01:48:22.457448 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m01:48:22.457448 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 01:48:22.426118 => 01:48:22.457448
[0m01:48:22.457448 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m01:48:22.489729 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m01:48:22.490658 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:48:22.491655 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m01:48:22.491655 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `s_address_src`.`addr`,
        `s_name_src`.`name`,
        CURRENT_TIMESTAMP as t
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m01:48:22.492740 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:48:23.398366 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:48:23.398366 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m01:48:23.404911 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 01:48:22.457448 => 01:48:23.404911
[0m01:48:23.413379 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m01:48:23.414476 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:48:23.414982 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m01:48:23.424929 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b8d18b6-5076-4c9d-a995-9f5a1dea978f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208270A8100>]}
[0m01:48:23.424929 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.pit_client2 ............................... [[32mOK[0m in 1.00s]
[0m01:48:23.424929 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m01:48:23.424929 [debug] [MainThread]: On master: ROLLBACK
[0m01:48:23.424929 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:48:23.497020 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:48:23.497020 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:48:23.497020 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:48:23.501707 [debug] [MainThread]: On master: ROLLBACK
[0m01:48:23.501707 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:48:23.501707 [debug] [MainThread]: On master: Close
[0m01:48:23.508792 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:48:23.509941 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:48:23.509941 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m01:48:23.509941 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m01:48:23.510948 [info ] [MainThread]: 
[0m01:48:23.511948 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.71 seconds (1.71s).
[0m01:48:23.512946 [debug] [MainThread]: Command end result
[0m01:48:23.530849 [info ] [MainThread]: 
[0m01:48:23.532874 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:48:23.532874 [info ] [MainThread]: 
[0m01:48:23.534380 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:48:23.534857 [debug] [MainThread]: Command `dbt run` succeeded at 01:48:23.534857 after 2.37 seconds
[0m01:48:23.535863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002082423DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020824E29210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002082459AB60>]}
[0m01:48:23.535863 [debug] [MainThread]: Flushing usage events
[0m12:26:50.941520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBC98DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBF0A2BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBF0A2A10>]}


============================== 12:26:50.941520 | ba1c6e37-8e03-49ad-83d6-c5b32b2c5a56 ==============================
[0m12:26:50.941520 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:26:50.941520 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\dbtvault-historical-pit\\poc_demo\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:26:50.941520 [info ] [MainThread]: dbt version: 1.5.2
[0m12:26:50.955079 [info ] [MainThread]: python version: 3.10.11
[0m12:26:50.956587 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m12:26:50.957979 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m12:26:50.957979 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m12:26:50.961501 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\demo_dbt\dbtvault-historical-pit\poc_demo\dbt_project.yml
[0m12:26:50.961501 [info ] [MainThread]: Configuration:
[0m12:26:51.228509 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:26:51.237207 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:26:51.254922 [info ] [MainThread]: Required dependencies:
[0m12:26:51.254922 [debug] [MainThread]: Executing "git --help"
[0m12:26:51.316840 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:26:51.320075 [debug] [MainThread]: STDERR: "b''"
[0m12:26:51.321011 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:26:51.321011 [info ] [MainThread]: Connection:
[0m12:26:51.321011 [info ] [MainThread]:   host: localhost
[0m12:26:51.325245 [info ] [MainThread]:   port: 10000
[0m12:26:51.325245 [info ] [MainThread]:   cluster: None
[0m12:26:51.325245 [info ] [MainThread]:   endpoint: None
[0m12:26:51.325245 [info ] [MainThread]:   schema: ndb
[0m12:26:51.325245 [info ] [MainThread]:   organization: 0
[0m12:26:51.325245 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m12:26:51.325245 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:26:51.325245 [debug] [MainThread]: Using spark connection "debug"
[0m12:26:51.325245 [debug] [MainThread]: On debug: select 1 as id
[0m12:26:51.325245 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:26:51.539134 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:26:51.539134 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m12:26:51.539134 [debug] [MainThread]: On debug: Close
[0m12:26:51.594331 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:26:51.595025 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:26:51.599216 [debug] [MainThread]: Command `dbt debug` succeeded at 12:26:51.599216 after 0.70 seconds
[0m12:26:51.599216 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:26:51.599216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBC98DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBBB17A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDBBB16470>]}
[0m12:26:51.599216 [debug] [MainThread]: Flushing usage events
[0m12:44:50.905241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE6CDDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE93F2CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE93F2B00>]}


============================== 12:44:50.905241 | d4a1a802-3f4e-4b0b-bc58-9357dff3503b ==============================
[0m12:44:50.905241 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:44:50.905241 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\dbtvault-historical-pit\\poc_demo\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:44:51.034590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE93F2D10>]}
[0m12:44:51.034590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE97A2F50>]}
[0m12:44:51.034590 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m12:44:51.085545 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:44:52.817874 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:44:52.819967 [debug] [MainThread]: Partial parsing: updated file: dbtvault://macros\tables\snowflake\pit.sql
[0m12:44:52.884230 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m12:44:53.027959 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m12:44:53.027959 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m12:44:53.060065 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m12:44:53.082072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9C2D780>]}
[0m12:44:53.131676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE5E1B970>]}
[0m12:44:53.131676 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m12:44:53.131676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE5E1B8B0>]}
[0m12:44:53.131676 [info ] [MainThread]: 
[0m12:44:53.131676 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:44:53.131676 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:44:53.155353 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:44:53.155353 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:44:53.155353 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:53.262025 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:44:53.263593 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:44:53.269694 [debug] [ThreadPool]: On list_schemas: Close
[0m12:44:53.280672 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m12:44:53.287552 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:53.287552 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m12:44:53.287552 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m12:44:53.287552 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:53.718905 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:44:53.726022 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:44:53.736272 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m12:44:53.736272 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:44:53.736272 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m12:44:53.747752 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m12:44:53.747752 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:53.747752 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m12:44:53.747752 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m12:44:53.747752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:44:54.144468 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:44:54.145474 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:44:54.156016 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m12:44:54.156016 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:44:54.156016 [debug] [ThreadPool]: On list_None_ndb: Close
[0m12:44:54.173532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9AF6EF0>]}
[0m12:44:54.173532 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:54.173532 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:44:54.173532 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:44:54.182198 [info ] [MainThread]: 
[0m12:44:54.190281 [debug] [Thread-1 (]: Began running node model.poc_demo.h_cli
[0m12:44:54.190281 [info ] [Thread-1 (]: 1 of 3 START sql incremental model ndb.h_cli ................................... [RUN]
[0m12:44:54.194336 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.h_cli'
[0m12:44:54.195347 [debug] [Thread-1 (]: Began compiling node model.poc_demo.h_cli
[0m12:44:54.284696 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.h_cli"
[0m12:44:54.287220 [debug] [Thread-1 (]: Timing info for model.poc_demo.h_cli (compile): 12:44:54.195347 => 12:44:54.286707
[0m12:44:54.288225 [debug] [Thread-1 (]: Began executing node model.poc_demo.h_cli
[0m12:44:54.358871 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:44:54.359949 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.h_cli"
[0m12:44:54.361003 [debug] [Thread-1 (]: On model.poc_demo.h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.h_cli"} */

  
    create temporary view h_cli__dbt_tmp as
      










-- Generated by dbtvault.

WITH row_rank_1 AS (
    SELECT * FROM (
    SELECT rr.`hk_cli_cd`, rr.`cli_id`, rr.`ld_dt_tm`, rr.`rcrd_src_nm`,
           ROW_NUMBER() OVER(
               PARTITION BY rr.`hk_cli_cd`
               ORDER BY rr.`ld_dt_tm`
           ) AS row_number
    FROM ndb.v_stg_h_cli AS rr
    WHERE rr.`hk_cli_cd` IS NOT NULL
     ) WHERE row_number = 1
),

records_to_insert AS (
    SELECT a.`hk_cli_cd`, a.`cli_id`, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM row_rank_1 AS a
    LEFT JOIN ndb.h_cli AS d
    ON a.`hk_cli_cd` = d.`hk_cli_cd`
    WHERE d.`hk_cli_cd` IS NULL
)

SELECT * FROM records_to_insert
  
[0m12:44:54.361526 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:44:54.636929 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:44:54.636929 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:44:54.661272 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.h_cli"
[0m12:44:54.667202 [debug] [Thread-1 (]: On model.poc_demo.h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.h_cli"} */

      describe extended ndb.h_cli
  
[0m12:44:54.739589 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:44:54.739589 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:44:54.749511 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.h_cli"
[0m12:44:54.757009 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.h_cli"
[0m12:44:54.757009 [debug] [Thread-1 (]: On model.poc_demo.h_cli: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.h_cli"} */

    insert into table ndb.h_cli
    select `hk_cli_cd`, `cli_id`, `ld_dt_tm`, `rcrd_src_nm` from h_cli__dbt_tmp


[0m12:44:59.765896 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m12:45:01.556918 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:01.556918 [debug] [Thread-1 (]: SQL status: OK in 7.0 seconds
[0m12:45:01.577387 [debug] [Thread-1 (]: Timing info for model.poc_demo.h_cli (execute): 12:44:54.289224 => 12:45:01.577387
[0m12:45:01.582387 [debug] [Thread-1 (]: On model.poc_demo.h_cli: ROLLBACK
[0m12:45:01.582387 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:45:01.582387 [debug] [Thread-1 (]: On model.poc_demo.h_cli: Close
[0m12:45:01.764606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9B4A050>]}
[0m12:45:01.764606 [info ] [Thread-1 (]: 1 of 3 OK created sql incremental model ndb.h_cli .............................. [[32mOK[0m in 7.57s]
[0m12:45:01.764606 [debug] [Thread-1 (]: Finished running node model.poc_demo.h_cli
[0m12:45:01.764606 [debug] [Thread-1 (]: Began running node model.poc_demo.s_address
[0m12:45:01.772620 [info ] [Thread-1 (]: 2 of 3 START sql incremental model ndb.s_address ............................... [RUN]
[0m12:45:01.772620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.h_cli, now model.poc_demo.s_address)
[0m12:45:01.775538 [debug] [Thread-1 (]: Began compiling node model.poc_demo.s_address
[0m12:45:01.823203 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.s_address"
[0m12:45:01.823203 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_address (compile): 12:45:01.775538 => 12:45:01.823203
[0m12:45:01.823203 [debug] [Thread-1 (]: Began executing node model.poc_demo.s_address
[0m12:45:01.823203 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:01.823203 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_address"
[0m12:45:01.833832 [debug] [Thread-1 (]: On model.poc_demo.s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_address"} */

  
    create temporary view s_address__dbt_tmp as
      














-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`addr`, a.EFFECTIVE_FROM, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM ndb.v_stg_s_address AS a
    WHERE a.`hsh_ky_cli_cd` IS NOT NULL
),

latest_records AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`ld_dt_tm`
    FROM (
        SELECT current_records.`hsh_ky_cli_cd`, current_records.`rcrd_hsh_id`, current_records.`ld_dt_tm`,
            RANK() OVER (
                PARTITION BY current_records.`hsh_ky_cli_cd`
                ORDER BY current_records.`ld_dt_tm` DESC
            ) AS rank
        FROM ndb.s_address AS current_records
            JOIN (
                SELECT DISTINCT source_data.`hsh_ky_cli_cd`
                FROM source_data
            ) AS source_records
                ON current_records.`hsh_ky_cli_cd` = source_records.`hsh_ky_cli_cd`
    ) AS a
    WHERE a.rank = 1
),

records_to_insert AS (
    SELECT DISTINCT stage.`hsh_ky_cli_cd`, stage.`rcrd_hsh_id`, stage.`addr`, stage.EFFECTIVE_FROM, stage.`ld_dt_tm` AS `ld_dt_tm`, stage.`rcrd_src_nm` AS `rcrd_src_nm`
    FROM source_data AS stage
        LEFT JOIN latest_records
            ON latest_records.`hsh_ky_cli_cd` = stage.`hsh_ky_cli_cd`
            WHERE latest_records.`rcrd_hsh_id` != stage.`rcrd_hsh_id`
                OR latest_records.`rcrd_hsh_id` IS NULL
)

SELECT * FROM records_to_insert
  
[0m12:45:01.833832 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:45:02.117633 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:02.117633 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:45:02.124668 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_address"
[0m12:45:02.124668 [debug] [Thread-1 (]: On model.poc_demo.s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_address"} */

      describe extended ndb.s_address
  
[0m12:45:02.180898 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:02.180898 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:45:02.189458 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.s_address"
[0m12:45:02.189458 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_address"
[0m12:45:02.195476 [debug] [Thread-1 (]: On model.poc_demo.s_address: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_address"} */

    insert into table ndb.s_address
    select `hsh_ky_cli_cd`, `rcrd_hsh_id`, `addr`, `EFFECTIVE_FROM`, `ld_dt_tm`, `rcrd_src_nm` from s_address__dbt_tmp


[0m12:45:05.944389 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:05.944389 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m12:45:05.947902 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_address (execute): 12:45:01.823203 => 12:45:05.947902
[0m12:45:05.947902 [debug] [Thread-1 (]: On model.poc_demo.s_address: ROLLBACK
[0m12:45:05.949919 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:45:05.949919 [debug] [Thread-1 (]: On model.poc_demo.s_address: Close
[0m12:45:06.118051 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9AE6E60>]}
[0m12:45:06.118051 [info ] [Thread-1 (]: 2 of 3 OK created sql incremental model ndb.s_address .......................... [[32mOK[0m in 4.35s]
[0m12:45:06.122566 [debug] [Thread-1 (]: Finished running node model.poc_demo.s_address
[0m12:45:06.122566 [debug] [Thread-1 (]: Began running node model.poc_demo.s_name
[0m12:45:06.122566 [info ] [Thread-1 (]: 3 of 3 START sql incremental model ndb.s_name .................................. [RUN]
[0m12:45:06.122566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.poc_demo.s_address, now model.poc_demo.s_name)
[0m12:45:06.122566 [debug] [Thread-1 (]: Began compiling node model.poc_demo.s_name
[0m12:45:06.143132 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.s_name"
[0m12:45:06.150923 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_name (compile): 12:45:06.122566 => 12:45:06.143132
[0m12:45:06.152297 [debug] [Thread-1 (]: Began executing node model.poc_demo.s_name
[0m12:45:06.162603 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:06.162603 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_name"
[0m12:45:06.162603 [debug] [Thread-1 (]: On model.poc_demo.s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_name"} */

  
    create temporary view s_name__dbt_tmp as
      














-- Generated by dbtvault.

WITH source_data AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`name`, a.EFFECTIVE_FROM, a.`ld_dt_tm`, a.`rcrd_src_nm`
    FROM ndb.v_stg_s_name AS a
    WHERE a.`hsh_ky_cli_cd` IS NOT NULL
),

latest_records AS (
    SELECT a.`hsh_ky_cli_cd`, a.`rcrd_hsh_id`, a.`ld_dt_tm`
    FROM (
        SELECT current_records.`hsh_ky_cli_cd`, current_records.`rcrd_hsh_id`, current_records.`ld_dt_tm`,
            RANK() OVER (
                PARTITION BY current_records.`hsh_ky_cli_cd`
                ORDER BY current_records.`ld_dt_tm` DESC
            ) AS rank
        FROM ndb.s_name AS current_records
            JOIN (
                SELECT DISTINCT source_data.`hsh_ky_cli_cd`
                FROM source_data
            ) AS source_records
                ON current_records.`hsh_ky_cli_cd` = source_records.`hsh_ky_cli_cd`
    ) AS a
    WHERE a.rank = 1
),

records_to_insert AS (
    SELECT DISTINCT stage.`hsh_ky_cli_cd`, stage.`rcrd_hsh_id`, stage.`name`, stage.EFFECTIVE_FROM, stage.`ld_dt_tm` AS `ld_dt_tm`, stage.`rcrd_src_nm` AS `rcrd_src_nm`
    FROM source_data AS stage
        LEFT JOIN latest_records
            ON latest_records.`hsh_ky_cli_cd` = stage.`hsh_ky_cli_cd`
            WHERE latest_records.`rcrd_hsh_id` != stage.`rcrd_hsh_id`
                OR latest_records.`rcrd_hsh_id` IS NULL
)

SELECT * FROM records_to_insert
  
[0m12:45:06.162603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:45:06.446428 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:06.446428 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:45:06.446428 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_name"
[0m12:45:06.446428 [debug] [Thread-1 (]: On model.poc_demo.s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_name"} */

      describe extended ndb.s_name
  
[0m12:45:06.509254 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:06.512722 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m12:45:06.519824 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.s_name"
[0m12:45:06.523330 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.s_name"
[0m12:45:06.523330 [debug] [Thread-1 (]: On model.poc_demo.s_name: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.s_name"} */

    insert into table ndb.s_name
    select `hsh_ky_cli_cd`, `rcrd_hsh_id`, `name`, `EFFECTIVE_FROM`, `ld_dt_tm`, `rcrd_src_nm` from s_name__dbt_tmp


[0m12:45:10.050393 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:45:10.050393 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m12:45:10.050393 [debug] [Thread-1 (]: Timing info for model.poc_demo.s_name (execute): 12:45:06.152945 => 12:45:10.050393
[0m12:45:10.050393 [debug] [Thread-1 (]: On model.poc_demo.s_name: ROLLBACK
[0m12:45:10.050393 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:45:10.050393 [debug] [Thread-1 (]: On model.poc_demo.s_name: Close
[0m12:45:10.240959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4a1a802-3f4e-4b0b-bc58-9357dff3503b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9BC5CC0>]}
[0m12:45:10.244965 [info ] [Thread-1 (]: 3 of 3 OK created sql incremental model ndb.s_name ............................. [[32mOK[0m in 4.12s]
[0m12:45:10.244965 [debug] [Thread-1 (]: Finished running node model.poc_demo.s_name
[0m12:45:10.248984 [debug] [MainThread]: On master: ROLLBACK
[0m12:45:10.248984 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:45:10.300112 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:45:10.301115 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:10.301115 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:45:10.302128 [debug] [MainThread]: On master: ROLLBACK
[0m12:45:10.303140 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:45:10.303140 [debug] [MainThread]: On master: Close
[0m12:45:10.317964 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:45:10.319031 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m12:45:10.319971 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m12:45:10.320971 [debug] [MainThread]: Connection 'model.poc_demo.s_name' was properly closed.
[0m12:45:10.320971 [info ] [MainThread]: 
[0m12:45:10.342332 [info ] [MainThread]: Finished running 3 incremental models in 0 hours 0 minutes and 17.19 seconds (17.19s).
[0m12:45:10.345728 [debug] [MainThread]: Command end result
[0m12:45:10.364252 [info ] [MainThread]: 
[0m12:45:10.365508 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:45:10.367521 [info ] [MainThread]: 
[0m12:45:10.367836 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m12:45:10.369845 [debug] [MainThread]: Command `dbt run` succeeded at 12:45:10.369845 after 19.48 seconds
[0m12:45:10.369845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE6CDDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE9B483D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023AE78C8A90>]}
[0m12:45:10.370846 [debug] [MainThread]: Flushing usage events
[0m12:47:55.671273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E412DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6842DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6842B90>]}


============================== 12:47:55.676234 | ff041cf9-d282-46b0-98f0-c1d1882fd2f8 ==============================
[0m12:47:55.676234 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:47:55.678240 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\demo_dbt\\dbtvault-historical-pit\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:47:55.806807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6842E00>]}
[0m12:47:55.814924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6BF2DA0>]}
[0m12:47:55.814924 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m12:47:55.834384 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:47:55.964650 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:47:55.964650 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:47:55.979764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6F240D0>]}
[0m12:47:55.980811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6EE02E0>]}
[0m12:47:55.980811 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m12:47:55.996481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6EE0340>]}
[0m12:47:55.997659 [info ] [MainThread]: 
[0m12:47:55.997659 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:47:55.997659 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:47:55.997659 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:47:56.011690 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:47:56.012598 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:56.104513 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:47:56.106023 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:47:56.112661 [debug] [ThreadPool]: On list_schemas: Close
[0m12:47:56.120632 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m12:47:56.120632 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:56.120632 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m12:47:56.120632 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m12:47:56.120632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:56.372112 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:47:56.372112 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:47:56.380537 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m12:47:56.380537 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:47:56.380537 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m12:47:56.389241 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_ndb)
[0m12:47:56.398781 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:56.398781 [debug] [ThreadPool]: Using spark connection "list_None_ndb"
[0m12:47:56.398781 [debug] [ThreadPool]: On list_None_ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "connection_name": "list_None_ndb"} */
show table extended in ndb like '*'
  
[0m12:47:56.398781 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:47:56.680267 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:47:56.681274 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m12:47:56.685295 [debug] [ThreadPool]: On list_None_ndb: ROLLBACK
[0m12:47:56.685295 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:47:56.685295 [debug] [ThreadPool]: On list_None_ndb: Close
[0m12:47:56.702109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6F57D30>]}
[0m12:47:56.703107 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:56.703653 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:47:56.704158 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:47:56.705162 [info ] [MainThread]: 
[0m12:47:56.708532 [debug] [Thread-1 (]: Began running node model.poc_demo.pit_client2
[0m12:47:56.709534 [info ] [Thread-1 (]: 1 of 1 START sql view model ndb.pit_client2 .................................... [RUN]
[0m12:47:56.711536 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo.pit_client2'
[0m12:47:56.712539 [debug] [Thread-1 (]: Began compiling node model.poc_demo.pit_client2
[0m12:47:56.847186 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo.pit_client2"
[0m12:47:56.847186 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (compile): 12:47:56.712539 => 12:47:56.847186
[0m12:47:56.847186 [debug] [Thread-1 (]: Began executing node model.poc_demo.pit_client2
[0m12:47:56.863666 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo.pit_client2"
[0m12:47:56.863666 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:56.863666 [debug] [Thread-1 (]: Using spark connection "model.poc_demo.pit_client2"
[0m12:47:56.863666 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo", "target_name": "dev", "node_id": "model.poc_demo.pit_client2"} */
create or replace view ndb.pit_client2
  
  as
    








-- Generated by dbtvault.


-- depends_on: ndb.v_stg_s_address
-- depends_on: ndb.v_stg_s_name


WITH as_of_dates AS (
    SELECT * FROM ndb.as_of_date
),

new_rows_as_of_dates AS (
    SELECT
        a.`hk_cli_cd`,
        b.AS_OF_DATE
    FROM ndb.h_cli AS a
    LEFT JOIN as_of_dates AS b
    ON a.`hk_cli_cd` = b.`hk_cli_cd`
),

new_rows AS (
    SELECT
        a.`hk_cli_cd`,
        a.AS_OF_DATE,COALESCE(MAX(`s_address_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_ADDRESS_PK`,COALESCE(MAX(`s_address_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_ADDRESS_LDTS`,COALESCE(MAX(`s_name_src`.`hsh_ky_cli_cd`), CAST('0000000000000000' AS STRING)) AS `S_NAME_PK`,COALESCE(MAX(`s_name_src`.`EFFECTIVE_FROM`), CAST('1900-01-01 00:00:00.000' AS 
    timestamp
)) AS `S_NAME_LDTS`
    FROM new_rows_as_of_dates AS a

    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` <= a.AS_OF_DATE
    GROUP BY
        a.`hk_cli_cd`, a.AS_OF_DATE
),
temp as (
    select 
    a.`hk_cli_cd`,
    a.AS_OF_DATE as start_date,
    lead(a.as_of_date) over (partition by a.`hk_cli_cd` order by a.as_of_date asc) as end_date,
    `s_address_src`.`addr`,
        `s_name_src`.`name`,
        CURRENT_TIMESTAMP as LOAD_DATETIME
    from new_rows a
    LEFT JOIN ndb.s_address AS `s_address_src`
        ON a.`hk_cli_cd` = `s_address_src`.`hsh_ky_cli_cd`
        AND `s_address_src`.`EFFECTIVE_FROM` = a.`S_ADDRESS_LDTS`
    LEFT JOIN ndb.s_name AS `s_name_src`
        ON a.`hk_cli_cd` = `s_name_src`.`hsh_ky_cli_cd`
        AND `s_name_src`.`EFFECTIVE_FROM` = a.`S_NAME_LDTS`
    ),
pit AS (
    SELECT * FROM temp
)

SELECT DISTINCT * FROM pit

[0m12:47:56.863666 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:47:57.902973 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:47:57.905023 [debug] [Thread-1 (]: SQL status: OK in 1.0 seconds
[0m12:47:57.919012 [debug] [Thread-1 (]: Timing info for model.poc_demo.pit_client2 (execute): 12:47:56.847186 => 12:47:57.918164
[0m12:47:57.919012 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: ROLLBACK
[0m12:47:57.919012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:47:57.920518 [debug] [Thread-1 (]: On model.poc_demo.pit_client2: Close
[0m12:47:57.927935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff041cf9-d282-46b0-98f0-c1d1882fd2f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6F316F0>]}
[0m12:47:57.932452 [info ] [Thread-1 (]: 1 of 1 OK created sql view model ndb.pit_client2 ............................... [[32mOK[0m in 1.22s]
[0m12:47:57.932452 [debug] [Thread-1 (]: Finished running node model.poc_demo.pit_client2
[0m12:47:57.936460 [debug] [MainThread]: On master: ROLLBACK
[0m12:47:57.936460 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:47:57.985843 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:47:57.985843 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:47:57.987383 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:47:57.987383 [debug] [MainThread]: On master: ROLLBACK
[0m12:47:57.988388 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:47:57.988620 [debug] [MainThread]: On master: Close
[0m12:47:57.990528 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:47:57.996807 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m12:47:57.997818 [debug] [MainThread]: Connection 'list_None_ndb' was properly closed.
[0m12:47:57.997818 [debug] [MainThread]: Connection 'model.poc_demo.pit_client2' was properly closed.
[0m12:47:57.999325 [info ] [MainThread]: 
[0m12:47:58.000335 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.00 seconds (2.00s).
[0m12:47:58.001864 [debug] [MainThread]: Command end result
[0m12:47:58.021006 [info ] [MainThread]: 
[0m12:47:58.024010 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:47:58.025013 [info ] [MainThread]: 
[0m12:47:58.026011 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m12:47:58.027396 [debug] [MainThread]: Command `dbt run` succeeded at 12:47:58.027396 after 2.37 seconds
[0m12:47:58.027396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E412DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E6F93A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227E4471540>]}
[0m12:47:58.027396 [debug] [MainThread]: Flushing usage events
[0m19:12:11.498476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001364ADFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001364D512C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001364D5129E0>]}


============================== 19:12:11.498476 | d367983c-8bc2-4273-8323-a11a7492057f ==============================
[0m19:12:11.498476 [info ] [MainThread]: Running with dbt=1.5.2
[0m19:12:11.514431 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m19:12:11.515101 [info ] [MainThread]: dbt version: 1.5.2
[0m19:12:11.515101 [info ] [MainThread]: python version: 3.10.11
[0m19:12:11.515101 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m19:12:11.515101 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:12:11.515101 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m19:12:11.521000 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m19:12:11.522010 [info ] [MainThread]: Configuration:
[0m19:12:11.745815 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:12:11.771957 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:12:11.771957 [info ] [MainThread]: Required dependencies:
[0m19:12:11.771957 [debug] [MainThread]: Executing "git --help"
[0m19:12:11.809691 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:12:11.809691 [debug] [MainThread]: STDERR: "b''"
[0m19:12:11.809691 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:12:11.809691 [info ] [MainThread]: Connection:
[0m19:12:11.809691 [info ] [MainThread]:   host: localhost
[0m19:12:11.809691 [info ] [MainThread]:   port: 10000
[0m19:12:11.809691 [info ] [MainThread]:   cluster: None
[0m19:12:11.818756 [info ] [MainThread]:   endpoint: None
[0m19:12:11.819766 [info ] [MainThread]:   schema: kdb
[0m19:12:11.820766 [info ] [MainThread]:   organization: 0
[0m19:12:11.820766 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m19:12:11.822013 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m19:12:11.823021 [debug] [MainThread]: Using spark connection "debug"
[0m19:12:11.823021 [debug] [MainThread]: On debug: select 1 as id
[0m19:12:11.824019 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:12:12.285698 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m19:12:12.285698 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m19:12:12.291722 [debug] [MainThread]: On debug: Close
[0m19:12:12.306227 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m19:12:12.313317 [info ] [MainThread]: [32mAll checks passed![0m
[0m19:12:12.316342 [debug] [MainThread]: Command `dbt debug` succeeded at 19:12:12.316342 after 0.84 seconds
[0m19:12:12.316342 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:12:12.316342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001364ADFDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013649F87A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013649F85F00>]}
[0m19:12:12.316342 [debug] [MainThread]: Flushing usage events
[0m01:14:37.626739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032B29DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032D9B2C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032D9B2A10>]}


============================== 01:14:37.631746 | c7347494-6962-4459-beb2-9dc62df27b2a ==============================
[0m01:14:37.631746 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:14:37.631746 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:14:37.631746 [info ] [MainThread]: dbt version: 1.5.2
[0m01:14:37.640246 [info ] [MainThread]: python version: 3.10.11
[0m01:14:37.640246 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:14:37.640246 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:14:37.640246 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:14:37.640246 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:14:37.640246 [info ] [MainThread]: Configuration:
[0m01:14:37.916157 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:14:37.948609 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:14:37.948609 [info ] [MainThread]: Required dependencies:
[0m01:14:37.948609 [debug] [MainThread]: Executing "git --help"
[0m01:14:38.005559 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:14:38.013421 [debug] [MainThread]: STDERR: "b''"
[0m01:14:38.013421 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:14:38.013421 [info ] [MainThread]: Connection:
[0m01:14:38.013421 [info ] [MainThread]:   host: localhost
[0m01:14:38.013421 [info ] [MainThread]:   port: 10000
[0m01:14:38.013421 [info ] [MainThread]:   cluster: None
[0m01:14:38.013421 [info ] [MainThread]:   endpoint: None
[0m01:14:38.013421 [info ] [MainThread]:   schema: kdb
[0m01:14:38.023066 [info ] [MainThread]:   organization: 0
[0m01:14:38.024689 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:14:38.024689 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:14:38.024689 [debug] [MainThread]: Using spark connection "debug"
[0m01:14:38.029704 [debug] [MainThread]: On debug: select 1 as id
[0m01:14:38.029704 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:14:43.579515 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:14:43.579515 [debug] [MainThread]: SQL status: OK in 6.0 seconds
[0m01:14:43.579515 [debug] [MainThread]: On debug: Close
[0m01:14:43.615020 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:14:43.621529 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:14:43.623717 [debug] [MainThread]: Command `dbt debug` succeeded at 01:14:43.623717 after 6.04 seconds
[0m01:14:43.625769 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:14:43.625769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032B29DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032A427D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032A427D30>]}
[0m01:14:43.625769 [debug] [MainThread]: Flushing usage events
[0m01:17:00.296020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF434FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF45C12E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF45C12B90>]}


============================== 01:17:00.301476 | 233efa28-dd34-4cd7-99e6-bab9b9974b50 ==============================
[0m01:17:00.301476 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:17:00.301476 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:17:00.444864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF45C12EC0>]}
[0m01:17:00.460971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF45C12E60>]}
[0m01:17:00.460971 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:17:00.541587 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:17:00.573963 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:17:00.573963 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m01:17:00.581857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF42671750>]}
[0m01:17:06.001118 [debug] [MainThread]: 1699: static parser successfully parsed example\delta_lake.sql
[0m01:17:06.016769 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_cow.sql
[0m01:17:06.016769 [debug] [MainThread]: 1699: static parser successfully parsed example\hudi_mor.sql
[0m01:17:06.026694 [debug] [MainThread]: 1699: static parser successfully parsed example\iceberg_partition.sql
[0m01:17:06.028224 [debug] [MainThread]: 1699: static parser successfully parsed example\my_first_dbt_model.sql
[0m01:17:06.028224 [debug] [MainThread]: 1699: static parser successfully parsed example\my_second_dbt_model.sql
[0m01:17:06.033477 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\as_of_date.sql
[0m01:17:06.033477 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\final_table.sql
[0m01:17:06.033477 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\h_cli.sql
[0m01:17:06.137401 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\h_cli.sql
[0m01:17:06.137401 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client.sql
[0m01:17:06.209517 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client.sql
[0m01:17:06.217538 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\pit_client2.sql
[0m01:17:06.256889 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\pit_client2.sql
[0m01:17:06.259160 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_h_cli.sql
[0m01:17:06.263199 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_s_address.sql
[0m01:17:06.266463 [debug] [MainThread]: 1699: static parser successfully parsed pit_test\pit_test\raw_s_name.sql
[0m01:17:06.274201 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\s_address.sql
[0m01:17:06.333581 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\s_address.sql
[0m01:17:06.338597 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\s_name.sql
[0m01:17:06.354628 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\s_name.sql
[0m01:17:06.362629 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_h_cli.sql
[0m01:17:06.483524 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_h_cli.sql
[0m01:17:06.483524 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_s_address.sql
[0m01:17:06.515543 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_s_address.sql
[0m01:17:06.519072 [debug] [MainThread]: 1603: static parser failed on pit_test\pit_test\v_stg_s_name.sql
[0m01:17:06.539697 [debug] [MainThread]: 1602: parser fallback to jinja rendering on pit_test\pit_test\v_stg_s_name.sql
[0m01:17:06.701362 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.poc_demo.example
[0m01:17:06.709325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF4635C5B0>]}
[0m01:17:06.729568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF4635E5C0>]}
[0m01:17:06.733693 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:17:06.733693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF4635E560>]}
[0m01:17:06.733693 [info ] [MainThread]: 
[0m01:17:06.733693 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:17:06.743328 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:17:06.758637 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:17:06.758637 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:17:06.758637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:17:07.145254 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:17:07.145254 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:17:07.284278 [debug] [ThreadPool]: On list_schemas: Close
[0m01:17:07.310617 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_kdb'
[0m01:17:07.322047 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:07.323563 [debug] [ThreadPool]: Using spark connection "list_None_kdb"
[0m01:17:07.323563 [debug] [ThreadPool]: On list_None_kdb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_kdb"} */
show table extended in kdb like '*'
  
[0m01:17:07.323563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:17:07.620478 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:17:07.620478 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:17:07.636614 [debug] [ThreadPool]: On list_None_kdb: ROLLBACK
[0m01:17:07.636614 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:17:07.636614 [debug] [ThreadPool]: On list_None_kdb: Close
[0m01:17:07.659890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_kdb, now list_None_spark_catalog.ndb)
[0m01:17:07.665456 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:07.665979 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:17:07.667615 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:17:07.667615 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:17:09.134312 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:17:09.134312 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m01:17:09.153757 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:17:09.156770 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:17:09.156770 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:17:09.179581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF462D6380>]}
[0m01:17:09.179581 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:09.182608 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:17:09.184143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:17:09.184143 [info ] [MainThread]: 
[0m01:17:09.192263 [debug] [Thread-1 (]: Began running node model.poc_demo2.hudi_cow
[0m01:17:09.192263 [info ] [Thread-1 (]: 1 of 1 START sql incremental model kdb.hudi_cow ................................ [RUN]
[0m01:17:09.192263 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo2.hudi_cow'
[0m01:17:09.200842 [debug] [Thread-1 (]: Began compiling node model.poc_demo2.hudi_cow
[0m01:17:09.209045 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo2.hudi_cow"
[0m01:17:09.209045 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_cow (compile): 01:17:09.201851 => 01:17:09.209045
[0m01:17:09.209045 [debug] [Thread-1 (]: Began executing node model.poc_demo2.hudi_cow
[0m01:17:09.336562 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo2.hudi_cow"
[0m01:17:09.346896 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:09.346896 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_cow"
[0m01:17:09.346896 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_cow"} */

  
    
        create table kdb.hudi_cow
      
      
    using hudi
      options (type "cow" , preCombineField "watermark" , primaryKey "application_id" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:17:09.346896 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:17:14.434349 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m01:17:19.438067 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m01:17:21.549621 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:17:21.549621 [debug] [Thread-1 (]: SQL status: OK in 12.0 seconds
[0m01:17:21.587579 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_cow (execute): 01:17:09.209045 => 01:17:21.587579
[0m01:17:21.587579 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: ROLLBACK
[0m01:17:21.587579 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:17:21.594592 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: Close
[0m01:17:21.610821 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '233efa28-dd34-4cd7-99e6-bab9b9974b50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF46356620>]}
[0m01:17:21.610821 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model kdb.hudi_cow ........................... [[32mOK[0m in 12.42s]
[0m01:17:21.614956 [debug] [Thread-1 (]: Finished running node model.poc_demo2.hudi_cow
[0m01:17:21.617975 [debug] [MainThread]: On master: ROLLBACK
[0m01:17:21.617975 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:17:21.699291 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:17:21.699291 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:21.699291 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:17:21.705305 [debug] [MainThread]: On master: ROLLBACK
[0m01:17:21.705305 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:17:21.707354 [debug] [MainThread]: On master: Close
[0m01:17:21.715770 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:17:21.715770 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:17:21.723847 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m01:17:21.723847 [debug] [MainThread]: Connection 'model.poc_demo2.hudi_cow' was properly closed.
[0m01:17:21.723847 [info ] [MainThread]: 
[0m01:17:21.723847 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 14.99 seconds (14.99s).
[0m01:17:21.723847 [debug] [MainThread]: Command end result
[0m01:17:21.750289 [info ] [MainThread]: 
[0m01:17:21.750289 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:17:21.750289 [info ] [MainThread]: 
[0m01:17:21.756788 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:17:21.756788 [debug] [MainThread]: Command `dbt run` succeeded at 01:17:21.756788 after 21.48 seconds
[0m01:17:21.760299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF434FDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF462262F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF46227F40>]}
[0m01:17:21.760299 [debug] [MainThread]: Flushing usage events
[0m01:17:59.243629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017558D8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B4A2E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B4A2B90>]}


============================== 01:17:59.243629 | f4561d62-13b5-4da0-b38c-47a292c58cfa ==============================
[0m01:17:59.243629 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:17:59.243629 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:17:59.437397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B4A2EC0>]}
[0m01:17:59.444905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B4A2E60>]}
[0m01:17:59.444905 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:17:59.497157 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:17:59.649074 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:17:59.649074 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:17:59.649074 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.poc_demo.example
[0m01:17:59.665555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755BB880D0>]}
[0m01:17:59.694598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755BB401F0>]}
[0m01:17:59.695688 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:17:59.698514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755BB40250>]}
[0m01:17:59.701345 [info ] [MainThread]: 
[0m01:17:59.703356 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:17:59.703356 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:17:59.727395 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:17:59.735409 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:17:59.737464 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:17:59.908629 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:17:59.914570 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:17:59.924040 [debug] [ThreadPool]: On list_schemas: Close
[0m01:17:59.945240 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_kdb'
[0m01:17:59.954872 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:17:59.954872 [debug] [ThreadPool]: Using spark connection "list_None_kdb"
[0m01:17:59.954872 [debug] [ThreadPool]: On list_None_kdb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_kdb"} */
show table extended in kdb like '*'
  
[0m01:17:59.954872 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:18:00.209737 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:18:00.210964 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:18:00.229057 [debug] [ThreadPool]: On list_None_kdb: ROLLBACK
[0m01:18:00.229142 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:18:00.229142 [debug] [ThreadPool]: On list_None_kdb: Close
[0m01:18:00.246434 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_kdb, now list_None_spark_catalog.ndb)
[0m01:18:00.256045 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:18:00.256045 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:18:00.260564 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:18:00.260564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:18:01.050860 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:18:01.053889 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m01:18:01.066827 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:18:01.066827 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:18:01.066827 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:18:01.088521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755BB8FA90>]}
[0m01:18:01.088521 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:18:01.088521 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:18:01.088521 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:18:01.088521 [info ] [MainThread]: 
[0m01:18:01.094542 [debug] [Thread-1 (]: Began running node model.poc_demo2.hudi_mor
[0m01:18:01.094542 [info ] [Thread-1 (]: 1 of 1 START sql incremental model kdb.hudi_mor ................................ [RUN]
[0m01:18:01.102556 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo2.hudi_mor'
[0m01:18:01.102556 [debug] [Thread-1 (]: Began compiling node model.poc_demo2.hudi_mor
[0m01:18:01.113076 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo2.hudi_mor"
[0m01:18:01.118601 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_mor (compile): 01:18:01.102556 => 01:18:01.118601
[0m01:18:01.118601 [debug] [Thread-1 (]: Began executing node model.poc_demo2.hudi_mor
[0m01:18:01.218233 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo2.hudi_mor"
[0m01:18:01.218233 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:18:01.221239 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_mor"
[0m01:18:01.221239 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_mor"} */

  
    
        create table kdb.hudi_mor
      
      
    using hudi
      options (primaryKey "application_id" , type "mor" , preCombineField "watermark" 
        )
      partitioned by (category)
      
      
      
      as
      

select *
from ndb.ntable
  
[0m01:18:01.221239 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:18:05.680573 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:18:05.680573 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m01:18:05.730995 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_mor (execute): 01:18:01.121208 => 01:18:05.730995
[0m01:18:05.741522 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: ROLLBACK
[0m01:18:05.741522 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:18:05.741522 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: Close
[0m01:18:05.765294 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4561d62-13b5-4da0-b38c-47a292c58cfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B853280>]}
[0m01:18:05.765834 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model kdb.hudi_mor ........................... [[32mOK[0m in 4.66s]
[0m01:18:05.768790 [debug] [Thread-1 (]: Finished running node model.poc_demo2.hudi_mor
[0m01:18:05.773987 [debug] [MainThread]: On master: ROLLBACK
[0m01:18:05.773987 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:18:05.860974 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:18:05.860974 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:18:05.860974 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:18:05.867000 [debug] [MainThread]: On master: ROLLBACK
[0m01:18:05.869095 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:18:05.869095 [debug] [MainThread]: On master: Close
[0m01:18:05.886600 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:18:05.889453 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:18:05.889453 [debug] [MainThread]: Connection 'list_None_spark_catalog.ndb' was properly closed.
[0m01:18:05.889453 [debug] [MainThread]: Connection 'model.poc_demo2.hudi_mor' was properly closed.
[0m01:18:05.889453 [info ] [MainThread]: 
[0m01:18:05.894983 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.19 seconds (6.19s).
[0m01:18:05.894983 [debug] [MainThread]: Command end result
[0m01:18:05.940169 [info ] [MainThread]: 
[0m01:18:05.945686 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:18:05.947714 [info ] [MainThread]: 
[0m01:18:05.947714 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:18:05.947714 [debug] [MainThread]: Command `dbt run` succeeded at 01:18:05.947714 after 6.74 seconds
[0m01:18:05.955227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017558D8DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001755B4A2E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017557F07730>]}
[0m01:18:05.956418 [debug] [MainThread]: Flushing usage events
[0m01:19:02.134011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E67A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A182DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A182B60>]}


============================== 01:19:02.134011 | 8c818206-8f4c-47c3-ae80-83d497386938 ==============================
[0m01:19:02.134011 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:19:02.134011 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:19:02.289539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A182E90>]}
[0m01:19:02.297547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A182E30>]}
[0m01:19:02.297547 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:19:02.332032 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m01:19:02.466785 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:19:02.466785 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:19:02.466785 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.poc_demo.example
[0m01:19:02.482761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A8680D0>]}
[0m01:19:02.507005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A8201C0>]}
[0m01:19:02.507005 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m01:19:02.507005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A820220>]}
[0m01:19:02.516713 [info ] [MainThread]: 
[0m01:19:02.516713 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:19:02.516713 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:19:02.537118 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:19:02.537118 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:19:02.537118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:19:02.695076 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:19:02.695076 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:19:02.706109 [debug] [ThreadPool]: On list_schemas: Close
[0m01:19:02.716882 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m01:19:02.728427 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:19:02.728427 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m01:19:02.728427 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m01:19:02.728427 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:19:03.354796 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:19:03.354796 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m01:19:03.369581 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m01:19:03.371092 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:19:03.371092 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m01:19:03.381383 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_kdb)
[0m01:19:03.392136 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:19:03.392136 [debug] [ThreadPool]: Using spark connection "list_None_kdb"
[0m01:19:03.392136 [debug] [ThreadPool]: On list_None_kdb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_kdb"} */
show table extended in kdb like '*'
  
[0m01:19:03.392136 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:19:03.575518 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:19:03.575518 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m01:19:03.592176 [debug] [ThreadPool]: On list_None_kdb: ROLLBACK
[0m01:19:03.592176 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:19:03.592176 [debug] [ThreadPool]: On list_None_kdb: Close
[0m01:19:03.607553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A81FBB0>]}
[0m01:19:03.607553 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:19:03.607553 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:19:03.607553 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:19:03.611416 [info ] [MainThread]: 
[0m01:19:03.615932 [debug] [Thread-1 (]: Began running node model.poc_demo2.hudi_mor
[0m01:19:03.615932 [info ] [Thread-1 (]: 1 of 1 START sql incremental model kdb.hudi_mor ................................ [RUN]
[0m01:19:03.621030 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo2.hudi_mor'
[0m01:19:03.621030 [debug] [Thread-1 (]: Began compiling node model.poc_demo2.hudi_mor
[0m01:19:03.629007 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo2.hudi_mor"
[0m01:19:03.634042 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_mor (compile): 01:19:03.621030 => 01:19:03.634042
[0m01:19:03.634042 [debug] [Thread-1 (]: Began executing node model.poc_demo2.hudi_mor
[0m01:19:03.709244 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:19:03.712753 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_mor"
[0m01:19:03.712753 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_mor"} */

  
    create temporary view hudi_mor__dbt_tmp as
      

select *
from ndb.ntable
  
[0m01:19:03.712753 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m01:19:03.862091 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:19:03.862091 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:19:03.909645 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_mor"
[0m01:19:03.909645 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_mor"} */

      describe extended kdb.hudi_mor
  
[0m01:19:04.095760 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:19:04.097634 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m01:19:04.126911 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo2.hudi_mor"
[0m01:19:04.130098 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_mor"
[0m01:19:04.131580 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_mor"} */

    -- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into kdb.hudi_mor as DBT_INTERNAL_DEST
      using hudi_mor__dbt_tmp as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.application_id = DBT_INTERNAL_DEST.application_id
          

      when matched then update set
         * 

--      when not matched then insert *
      when not matched then insert
         * 

[0m01:19:09.140613 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m01:19:09.949886 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:19:09.949886 [debug] [Thread-1 (]: SQL status: OK in 6.0 seconds
[0m01:19:09.972610 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_mor (execute): 01:19:03.634042 => 01:19:09.972610
[0m01:19:09.972610 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: ROLLBACK
[0m01:19:09.976130 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:19:09.976130 [debug] [Thread-1 (]: On model.poc_demo2.hudi_mor: Close
[0m01:19:10.053720 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8c818206-8f4c-47c3-ae80-83d497386938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A900AF0>]}
[0m01:19:10.053720 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model kdb.hudi_mor ........................... [[32mOK[0m in 6.44s]
[0m01:19:10.061908 [debug] [Thread-1 (]: Finished running node model.poc_demo2.hudi_mor
[0m01:19:10.063429 [debug] [MainThread]: On master: ROLLBACK
[0m01:19:10.063429 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:19:10.157543 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:19:10.157543 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:19:10.163597 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:19:10.163597 [debug] [MainThread]: On master: ROLLBACK
[0m01:19:10.165627 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:19:10.165627 [debug] [MainThread]: On master: Close
[0m01:19:10.174142 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:19:10.174142 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m01:19:10.177885 [debug] [MainThread]: Connection 'list_None_kdb' was properly closed.
[0m01:19:10.177885 [debug] [MainThread]: Connection 'model.poc_demo2.hudi_mor' was properly closed.
[0m01:19:10.177885 [info ] [MainThread]: 
[0m01:19:10.177885 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 7.66 seconds (7.66s).
[0m01:19:10.182397 [debug] [MainThread]: Command end result
[0m01:19:10.202120 [info ] [MainThread]: 
[0m01:19:10.202120 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:19:10.202120 [info ] [MainThread]: 
[0m01:19:10.202120 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:19:10.202120 [debug] [MainThread]: Command `dbt run` succeeded at 01:19:10.202120 after 8.09 seconds
[0m01:19:10.209132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E67A6DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E6A182E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E67C4A830>]}
[0m01:19:10.209132 [debug] [MainThread]: Flushing usage events
[0m01:39:18.423270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0AC680E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0AC64FD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0AC690090>]}


============================== 01:39:18.428230 | a93b3a23-27e1-4129-a8de-c4ea6d339c98 ==============================
[0m01:39:18.428230 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:39:18.428230 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:39:18.428230 [info ] [MainThread]: dbt version: 1.6.9
[0m01:39:18.432142 [info ] [MainThread]: python version: 3.11.4
[0m01:39:18.432142 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:39:18.432142 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:39:18.512969 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:39:18.512969 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:39:18.512969 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:39:18.581733 [info ] [MainThread]: Configuration:
[0m01:39:18.598129 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:39:18.598129 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:39:18.598129 [info ] [MainThread]: Required dependencies:
[0m01:39:18.598129 [debug] [MainThread]: Executing "git --help"
[0m01:39:18.662348 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:39:18.662348 [debug] [MainThread]: STDERR: "b''"
[0m01:39:18.662348 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:39:18.662348 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:39:18.662348 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:39:18.662348 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:39:18.662348 [debug] [MainThread]: Command `dbt debug` failed at 01:39:18.662348 after 0.27 seconds
[0m01:39:18.662348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0AC43BD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0AC46F750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0A57C6A10>]}
[0m01:39:18.662348 [debug] [MainThread]: Flushing usage events
[0m01:40:23.811632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8812DB9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A880E0EFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A880DD6F90>]}


============================== 01:40:23.816849 | 10b3b823-1870-4c1a-9318-ca78d7e54745 ==============================
[0m01:40:23.816849 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:40:23.816849 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt debug', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:40:23.816849 [info ] [MainThread]: dbt version: 1.6.9
[0m01:40:23.816849 [info ] [MainThread]: python version: 3.11.4
[0m01:40:23.816849 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:40:23.816849 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:40:23.907294 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:40:23.915236 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:40:23.915236 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:40:23.955569 [info ] [MainThread]: Configuration:
[0m01:40:23.955569 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:40:23.955569 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:40:23.964203 [info ] [MainThread]: Required dependencies:
[0m01:40:23.964203 [debug] [MainThread]: Executing "git --help"
[0m01:40:24.026124 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:40:24.026124 [debug] [MainThread]: STDERR: "b''"
[0m01:40:24.026124 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:40:24.026124 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:40:24.026124 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:40:24.032611 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:40:24.032611 [debug] [MainThread]: Command `dbt debug` failed at 01:40:24.032611 after 0.24 seconds
[0m01:40:24.032611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A880F0D050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8FFEFA390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8810C7990>]}
[0m01:40:24.032611 [debug] [MainThread]: Flushing usage events
[0m01:41:29.463164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F9926DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F9B982BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F9B982950>]}


============================== 01:41:29.463164 | d328cf45-c7b2-468b-ae14-56fee0bfbd2b ==============================
[0m01:41:29.463164 [info ] [MainThread]: Running with dbt=1.5.2
[0m01:41:29.463164 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m01:41:29.463164 [info ] [MainThread]: dbt version: 1.5.2
[0m01:41:29.463164 [info ] [MainThread]: python version: 3.10.11
[0m01:41:29.463164 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m01:41:29.463164 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:41:29.463164 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:41:29.478888 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:41:29.478888 [info ] [MainThread]: Configuration:
[0m01:41:29.620915 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:41:29.643874 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:41:29.643874 [info ] [MainThread]: Required dependencies:
[0m01:41:29.643874 [debug] [MainThread]: Executing "git --help"
[0m01:41:29.698374 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:41:29.699462 [debug] [MainThread]: STDERR: "b''"
[0m01:41:29.700027 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:41:29.700615 [info ] [MainThread]: Connection:
[0m01:41:29.701388 [info ] [MainThread]:   host: localhost
[0m01:41:29.702727 [info ] [MainThread]:   port: 10000
[0m01:41:29.703256 [info ] [MainThread]:   cluster: None
[0m01:41:29.704346 [info ] [MainThread]:   endpoint: None
[0m01:41:29.704986 [info ] [MainThread]:   schema: kdb
[0m01:41:29.705834 [info ] [MainThread]:   organization: 0
[0m01:41:29.705834 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m01:41:29.708861 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:41:29.708861 [debug] [MainThread]: Using spark connection "debug"
[0m01:41:29.708861 [debug] [MainThread]: On debug: select 1 as id
[0m01:41:29.708861 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:41:29.904497 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:41:29.906161 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m01:41:29.908488 [debug] [MainThread]: On debug: Close
[0m01:41:29.919433 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:41:29.921734 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:41:29.922169 [debug] [MainThread]: Command `dbt debug` succeeded at 01:41:29.922169 after 0.48 seconds
[0m01:41:29.922169 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:41:29.922169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F9926DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F983F7AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017F983F7B20>]}
[0m01:41:29.922169 [debug] [MainThread]: Flushing usage events
[0m01:41:41.910257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE1DD84990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE1D5FD910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE1D5E1810>]}


============================== 01:41:41.912767 | c8f6569b-fb56-4c66-8249-adf7853c4af7 ==============================
[0m01:41:41.912767 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:41:41.912767 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:41:41.912767 [info ] [MainThread]: dbt version: 1.6.9
[0m01:41:41.912767 [info ] [MainThread]: python version: 3.11.4
[0m01:41:41.912767 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:41:41.912767 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:41:41.988140 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:41:41.988140 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:41:41.988140 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:41:42.020563 [info ] [MainThread]: Configuration:
[0m01:41:42.020563 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:41:42.020563 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:41:42.020563 [info ] [MainThread]: Required dependencies:
[0m01:41:42.024628 [debug] [MainThread]: Executing "git --help"
[0m01:41:42.088548 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:41:42.089054 [debug] [MainThread]: STDERR: "b''"
[0m01:41:42.090938 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:41:42.092778 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:41:42.093609 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:41:42.094216 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:41:42.096868 [debug] [MainThread]: Command `dbt debug` failed at 01:41:42.096868 after 0.21 seconds
[0m01:41:42.097935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE16AC69D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE16AC7590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE1D980550>]}
[0m01:41:42.099043 [debug] [MainThread]: Flushing usage events
[0m01:43:58.143641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3EFF90CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3EFF91D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3F0086250>]}


============================== 01:43:58.150773 | 5865dfd1-49f2-45eb-9288-82d5657bf606 ==============================
[0m01:43:58.150773 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:43:58.151881 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m01:43:58.151881 [info ] [MainThread]: dbt version: 1.6.9
[0m01:43:58.151881 [info ] [MainThread]: python version: 3.11.4
[0m01:43:58.151881 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:43:58.151881 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:43:58.225167 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:43:58.225167 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:43:58.225167 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:43:58.257253 [info ] [MainThread]: Configuration:
[0m01:43:58.257253 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:43:58.257253 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:43:58.257253 [info ] [MainThread]: Required dependencies:
[0m01:43:58.265267 [debug] [MainThread]: Executing "git --help"
[0m01:43:58.327452 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:43:58.327452 [debug] [MainThread]: STDERR: "b''"
[0m01:43:58.327452 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:43:58.327452 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:43:58.327452 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:43:58.327452 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:43:58.331656 [debug] [MainThread]: Command `dbt debug` failed at 01:43:58.331656 after 0.21 seconds
[0m01:43:58.331656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3E9167590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3F0028290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3F0256910>]}
[0m01:43:58.331656 [debug] [MainThread]: Flushing usage events
[0m01:45:30.803093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A9BC07450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A9BF4AF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A9BC3A850>]}


============================== 01:45:30.803093 | ceb5918e-67bb-4828-886a-e59780f4e006 ==============================
[0m01:45:30.803093 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:45:30.811185 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:45:30.812242 [info ] [MainThread]: dbt version: 1.6.9
[0m01:45:30.812242 [info ] [MainThread]: python version: 3.11.4
[0m01:45:30.812242 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:45:30.812242 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:45:30.915726 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:45:30.916522 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:45:30.916522 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:45:30.949269 [info ] [MainThread]: Configuration:
[0m01:45:30.949269 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:45:30.949269 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:45:30.949269 [info ] [MainThread]: Required dependencies:
[0m01:45:30.949269 [debug] [MainThread]: Executing "git --help"
[0m01:45:31.020872 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:45:31.020872 [debug] [MainThread]: STDERR: "b''"
[0m01:45:31.023261 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:45:31.023261 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:45:31.023261 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:45:31.023261 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:45:31.032335 [debug] [MainThread]: Command `dbt debug` failed at 01:45:31.031694 after 0.25 seconds
[0m01:45:31.032335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A9BE06650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A9B56D550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026A950876D0>]}
[0m01:45:31.032335 [debug] [MainThread]: Flushing usage events
[0m01:48:00.088056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F88720B990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F8869C5390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F887717D90>]}


============================== 01:48:00.088056 | 8b13275a-b8b8-44be-bd2f-9ed1dc8d5a20 ==============================
[0m01:48:00.088056 [info ] [MainThread]: Running with dbt=1.6.9
[0m01:48:00.088056 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:48:00.088056 [info ] [MainThread]: dbt version: 1.6.9
[0m01:48:00.096311 [info ] [MainThread]: python version: 3.11.4
[0m01:48:00.097775 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m01:48:00.097775 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m01:48:00.228409 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m01:48:00.230761 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m01:48:00.231800 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m01:48:00.281455 [info ] [MainThread]: Configuration:
[0m01:48:00.283020 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m01:48:00.286647 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:48:00.289355 [info ] [MainThread]: Required dependencies:
[0m01:48:00.290493 [debug] [MainThread]: Executing "git --help"
[0m01:48:00.348870 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:48:00.350815 [debug] [MainThread]: STDERR: "b''"
[0m01:48:00.354813 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:48:00.356472 [info ] [MainThread]: Connection test skipped since no profile was found
[0m01:48:00.358130 [info ] [MainThread]: [31m1 check failed:[0m
[0m01:48:00.359185 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m01:48:00.361850 [debug] [MainThread]: Command `dbt debug` failed at 01:48:00.361850 after 0.30 seconds
[0m01:48:00.362939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F886F9FE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F88718B690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F88776FA90>]}
[0m01:48:00.363483 [debug] [MainThread]: Flushing usage events
[0m14:24:31.584444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A897D2B790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A89A730890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A89A9DA850>]}


============================== 14:24:31.584444 | 1124f603-bd3a-439e-8fed-28593bef0fd7 ==============================
[0m14:24:31.584444 [info ] [MainThread]: Running with dbt=1.6.9
[0m14:24:31.584444 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:24:31.584444 [info ] [MainThread]: dbt version: 1.6.9
[0m14:24:31.584444 [info ] [MainThread]: python version: 3.11.4
[0m14:24:31.584444 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m14:24:31.594987 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:24:31.676888 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m14:24:31.677890 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:24:31.678889 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m14:24:31.712057 [info ] [MainThread]: Configuration:
[0m14:24:31.713679 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m14:24:31.714322 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:24:31.715383 [info ] [MainThread]: Required dependencies:
[0m14:24:31.716439 [debug] [MainThread]: Executing "git --help"
[0m14:24:31.759186 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:24:31.760204 [debug] [MainThread]: STDERR: "b''"
[0m14:24:31.760204 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:24:31.761186 [info ] [MainThread]: Connection test skipped since no profile was found
[0m14:24:31.762184 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:24:31.762184 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m14:24:31.764187 [debug] [MainThread]: Command `dbt debug` failed at 14:24:31.764187 after 0.22 seconds
[0m14:24:31.764187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A89A747710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A893BE76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A89A949D50>]}
[0m14:24:31.765188 [debug] [MainThread]: Flushing usage events
[0m14:24:36.939334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DFFD1E390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D82E06090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D82FDBA50>]}


============================== 14:24:36.944332 | 631241b7-9d31-4c33-aac8-6d71c5457ae7 ==============================
[0m14:24:36.944332 [info ] [MainThread]: Running with dbt=1.6.9
[0m14:24:36.945330 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m14:24:36.945330 [info ] [MainThread]: dbt version: 1.6.9
[0m14:24:36.946991 [info ] [MainThread]: python version: 3.11.4
[0m14:24:36.947997 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m14:24:36.947997 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:24:37.026792 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m14:24:37.027795 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:24:37.028792 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m14:24:37.062049 [info ] [MainThread]: Configuration:
[0m14:24:37.063242 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m14:24:37.064244 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:24:37.065244 [info ] [MainThread]: Required dependencies:
[0m14:24:37.066243 [debug] [MainThread]: Executing "git --help"
[0m14:24:37.101539 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:24:37.102535 [debug] [MainThread]: STDERR: "b''"
[0m14:24:37.103555 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:24:37.104542 [info ] [MainThread]: Connection test skipped since no profile was found
[0m14:24:37.104542 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:24:37.105547 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m14:24:37.106552 [debug] [MainThread]: Command `dbt debug` failed at 14:24:37.106552 after 0.19 seconds
[0m14:24:37.106552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D827216D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DFC450C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DFC450B50>]}
[0m14:24:37.107550 [debug] [MainThread]: Flushing usage events
[0m14:24:43.493005 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.
[0m14:24:52.576311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25B13DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25D852C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25D852A10>]}


============================== 14:24:52.580267 | 8f163d3f-a12c-4bc3-b379-fcc8349d8f0a ==============================
[0m14:24:52.580267 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:24:52.582270 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:24:52.583274 [info ] [MainThread]: dbt version: 1.5.2
[0m14:24:52.584267 [info ] [MainThread]: python version: 3.10.11
[0m14:24:52.584267 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m14:24:52.585271 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:24:52.585271 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:24:52.586269 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m14:24:52.586269 [info ] [MainThread]: Configuration:
[0m14:24:52.787696 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:24:52.806671 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:24:52.807670 [info ] [MainThread]: Required dependencies:
[0m14:24:52.808673 [debug] [MainThread]: Executing "git --help"
[0m14:24:52.842002 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:24:52.842002 [debug] [MainThread]: STDERR: "b''"
[0m14:24:52.843002 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:24:52.844005 [info ] [MainThread]: Connection:
[0m14:24:52.845002 [info ] [MainThread]:   host: localhost
[0m14:24:52.845002 [info ] [MainThread]:   port: 10000
[0m14:24:52.846002 [info ] [MainThread]:   cluster: None
[0m14:24:52.846817 [info ] [MainThread]:   endpoint: None
[0m14:24:52.849157 [info ] [MainThread]:   schema: kdb
[0m14:24:52.849157 [info ] [MainThread]:   organization: 0
[0m14:24:52.850156 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m14:24:52.852166 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m14:24:52.853159 [debug] [MainThread]: Using spark connection "debug"
[0m14:24:52.853159 [debug] [MainThread]: On debug: select 1 as id
[0m14:24:52.853159 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:57.985438 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m14:24:57.985438 [debug] [MainThread]: SQL status: OK in 5.0 seconds
[0m14:24:57.987244 [debug] [MainThread]: On debug: Close
[0m14:24:58.025245 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:24:58.026248 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:24:58.029276 [debug] [MainThread]: Command `dbt debug` succeeded at 14:24:58.027755 after 5.47 seconds
[0m14:24:58.029794 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:24:58.029794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25B13DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25DBEFD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A25DBEFD30>]}
[0m14:24:58.030805 [debug] [MainThread]: Flushing usage events
[0m14:25:41.984684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CBF1DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CE632E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CE632BF0>]}


============================== 14:25:42.002277 | ec789600-f018-4517-a181-f355b78566e0 ==============================
[0m14:25:42.002277 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:25:42.003274 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:25:42.117282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CE632F20>]}
[0m14:25:42.126281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CE632EC0>]}
[0m14:25:42.127282 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m14:25:42.185064 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:25:43.731806 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:43.732806 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:43.733806 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.poc_demo.example
[0m14:25:43.734655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CED180D0>]}
[0m14:25:43.763914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CECD0250>]}
[0m14:25:43.764912 [info ] [MainThread]: Found 19 models, 4 tests, 0 snapshots, 0 analyses, 722 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m14:25:43.765922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CECD02B0>]}
[0m14:25:43.766925 [info ] [MainThread]: 
[0m14:25:43.768919 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:43.770920 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:43.783023 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:43.784023 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:43.784023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:44.039549 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:25:44.039549 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m14:25:44.134743 [debug] [ThreadPool]: On list_schemas: Close
[0m14:25:44.153314 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_spark_catalog.ndb'
[0m14:25:44.160785 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:44.161293 [debug] [ThreadPool]: Using spark connection "list_None_spark_catalog.ndb"
[0m14:25:44.161293 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_spark_catalog.ndb"} */
show table extended in spark_catalog.ndb like '*'
  
[0m14:25:44.161293 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:45.522039 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:25:45.523038 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m14:25:45.539466 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: ROLLBACK
[0m14:25:45.540466 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:25:45.540466 [debug] [ThreadPool]: On list_None_spark_catalog.ndb: Close
[0m14:25:45.555967 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_spark_catalog.ndb, now list_None_kdb)
[0m14:25:45.564514 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:45.565509 [debug] [ThreadPool]: Using spark connection "list_None_kdb"
[0m14:25:45.565509 [debug] [ThreadPool]: On list_None_kdb: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "connection_name": "list_None_kdb"} */
show table extended in kdb like '*'
  
[0m14:25:45.566508 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:45.801219 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:25:45.801219 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m14:25:45.812128 [debug] [ThreadPool]: On list_None_kdb: ROLLBACK
[0m14:25:45.812128 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:25:45.813127 [debug] [ThreadPool]: On list_None_kdb: Close
[0m14:25:45.827276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CECCFB20>]}
[0m14:25:45.827276 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:45.828275 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:25:45.829275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:45.830274 [info ] [MainThread]: 
[0m14:25:45.839018 [debug] [Thread-1 (]: Began running node model.poc_demo2.hudi_cow
[0m14:25:45.840018 [info ] [Thread-1 (]: 1 of 1 START sql incremental model kdb.hudi_cow ................................ [RUN]
[0m14:25:45.841017 [debug] [Thread-1 (]: Acquiring new spark connection 'model.poc_demo2.hudi_cow'
[0m14:25:45.842018 [debug] [Thread-1 (]: Began compiling node model.poc_demo2.hudi_cow
[0m14:25:45.842319 [debug] [Thread-1 (]: Writing injected SQL for node "model.poc_demo2.hudi_cow"
[0m14:25:45.842319 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_cow (compile): 14:25:45.842319 => 14:25:45.842319
[0m14:25:45.842319 [debug] [Thread-1 (]: Began executing node model.poc_demo2.hudi_cow
[0m14:25:45.907552 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:45.908551 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_cow"
[0m14:25:45.908551 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_cow"} */

  
    create temporary view hudi_cow__dbt_tmp as
      

select *
from ndb.ntable
  
[0m14:25:45.909551 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:25:46.121097 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:25:46.122094 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:25:46.134875 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_cow"
[0m14:25:46.134875 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_cow"} */

      describe extended kdb.hudi_cow
  
[0m14:25:46.361960 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:25:46.362975 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:25:46.383102 [debug] [Thread-1 (]: Writing runtime sql for node "model.poc_demo2.hudi_cow"
[0m14:25:46.385116 [debug] [Thread-1 (]: Using spark connection "model.poc_demo2.hudi_cow"
[0m14:25:46.386631 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: /* {"app": "dbt", "dbt_version": "1.5.2", "profile_name": "poc_demo2", "target_name": "dev", "node_id": "model.poc_demo2.hudi_cow"} */

    -- back compat for old kwarg name
  
  
  
      
          
          
      
  

  

  merge into kdb.hudi_cow as DBT_INTERNAL_DEST
      using hudi_cow__dbt_tmp as DBT_INTERNAL_SOURCE
      on 
              DBT_INTERNAL_SOURCE.application_id = DBT_INTERNAL_DEST.application_id
          

      when matched then update set
         * 

--      when not matched then insert *
      when not matched then insert
         * 

[0m14:25:51.386091 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m14:25:56.398706 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m14:25:58.798156 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:25:58.798156 [debug] [Thread-1 (]: SQL status: OK in 12.0 seconds
[0m14:25:58.822334 [debug] [Thread-1 (]: Timing info for model.poc_demo2.hudi_cow (execute): 14:25:45.842319 => 14:25:58.822334
[0m14:25:58.823334 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: ROLLBACK
[0m14:25:58.823334 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:25:58.824339 [debug] [Thread-1 (]: On model.poc_demo2.hudi_cow: Close
[0m14:25:58.905430 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec789600-f018-4517-a181-f355b78566e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CEDC1690>]}
[0m14:25:58.907428 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model kdb.hudi_cow ........................... [[32mOK[0m in 13.06s]
[0m14:25:58.909429 [debug] [Thread-1 (]: Finished running node model.poc_demo2.hudi_cow
[0m14:25:58.910934 [debug] [MainThread]: On master: ROLLBACK
[0m14:25:58.911942 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:58.976980 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:25:58.978142 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:25:58.978979 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:25:58.978979 [debug] [MainThread]: On master: ROLLBACK
[0m14:25:58.979976 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:25:58.979976 [debug] [MainThread]: On master: Close
[0m14:25:58.993169 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:58.993169 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:58.993169 [debug] [MainThread]: Connection 'list_None_kdb' was properly closed.
[0m14:25:58.993169 [debug] [MainThread]: Connection 'model.poc_demo2.hudi_cow' was properly closed.
[0m14:25:58.993169 [info ] [MainThread]: 
[0m14:25:58.993169 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 15.23 seconds (15.23s).
[0m14:25:58.993169 [debug] [MainThread]: Command end result
[0m14:25:59.017126 [info ] [MainThread]: 
[0m14:25:59.019131 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:25:59.020122 [info ] [MainThread]: 
[0m14:25:59.021126 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:25:59.022123 [debug] [MainThread]: Command `dbt run` succeeded at 14:25:59.022123 after 17.04 seconds
[0m14:25:59.023128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CBF1DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CE632F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286CC216FB0>]}
[0m14:25:59.025153 [debug] [MainThread]: Flushing usage events
[0m14:28:59.061203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544D8BC2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544DC2FA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544E02BCD0>]}


============================== 14:28:59.065203 | 997d8702-570b-4c6d-8d3f-c7d1144a015c ==============================
[0m14:28:59.065203 [info ] [MainThread]: Running with dbt=1.6.9
[0m14:28:59.067203 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:28:59.067203 [info ] [MainThread]: dbt version: 1.6.9
[0m14:28:59.068205 [info ] [MainThread]: python version: 3.11.4
[0m14:28:59.069205 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerror\Scripts\python.exe
[0m14:28:59.070203 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:28:59.161350 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m14:28:59.162351 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:28:59.162351 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m14:28:59.187443 [info ] [MainThread]: Configuration:
[0m14:28:59.187443 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m14:28:59.187443 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:28:59.187443 [info ] [MainThread]: Required dependencies:
[0m14:28:59.187443 [debug] [MainThread]: Executing "git --help"
[0m14:28:59.233531 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:28:59.237552 [debug] [MainThread]: STDERR: "b''"
[0m14:28:59.237552 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:28:59.237552 [info ] [MainThread]: Connection test skipped since no profile was found
[0m14:28:59.237552 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:28:59.237552 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m14:28:59.237552 [debug] [MainThread]: Command `dbt debug` failed at 14:28:59.237552 after 0.20 seconds
[0m14:28:59.237552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544E02B850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544E02BD90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002544D94D750>]}
[0m14:28:59.237552 [debug] [MainThread]: Flushing usage events
[0m14:29:16.383758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D1DDDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D44F2C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D44F29E0>]}


============================== 14:29:16.383758 | d8c15a02-cc58-4843-8e91-bbb304321662 ==============================
[0m14:29:16.383758 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:29:16.383758 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:29:16.383758 [info ] [MainThread]: dbt version: 1.5.2
[0m14:29:16.383758 [info ] [MainThread]: python version: 3.10.11
[0m14:29:16.383758 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m14:29:16.383758 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m14:29:16.383758 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m14:29:16.383758 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m14:29:16.383758 [info ] [MainThread]: Configuration:
[0m14:29:16.495424 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:29:16.513397 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:29:16.514396 [info ] [MainThread]: Required dependencies:
[0m14:29:16.515395 [debug] [MainThread]: Executing "git --help"
[0m14:29:16.583032 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:29:16.583032 [debug] [MainThread]: STDERR: "b''"
[0m14:29:16.584315 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:29:16.584921 [info ] [MainThread]: Connection:
[0m14:29:16.584921 [info ] [MainThread]:   host: localhost
[0m14:29:16.584921 [info ] [MainThread]:   port: 10000
[0m14:29:16.584921 [info ] [MainThread]:   cluster: None
[0m14:29:16.584921 [info ] [MainThread]:   endpoint: None
[0m14:29:16.584921 [info ] [MainThread]:   schema: kdb
[0m14:29:16.584921 [info ] [MainThread]:   organization: 0
[0m14:29:16.584921 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m14:29:16.584921 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m14:29:16.584921 [debug] [MainThread]: Using spark connection "debug"
[0m14:29:16.584921 [debug] [MainThread]: On debug: select 1 as id
[0m14:29:16.584921 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:29:16.739479 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m14:29:16.740483 [debug] [MainThread]: SQL status: OK in 0.0 seconds
[0m14:29:16.741480 [debug] [MainThread]: On debug: Close
[0m14:29:16.750000 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:29:16.752518 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:29:16.754301 [debug] [MainThread]: Command `dbt debug` succeeded at 14:29:16.752518 after 0.38 seconds
[0m14:29:16.754301 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:29:16.755170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D1DDDB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D488FA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000165D488DF00>]}
[0m14:29:16.755170 [debug] [MainThread]: Flushing usage events
[0m15:21:44.823532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD0362560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD334EF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD334ECB0>]}


============================== 15:21:44.827532 | 108da424-157e-40cc-a6fa-41a353f8c6d9 ==============================
[0m15:21:44.827532 [info ] [MainThread]: Running with dbt=1.6.9
[0m15:21:44.828532 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m15:21:44.829534 [info ] [MainThread]: dbt version: 1.6.9
[0m15:21:44.830531 [info ] [MainThread]: python version: 3.10.11
[0m15:21:44.831533 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\spark-error-fixed\penv\sparkerrorfix\Scripts\python.exe
[0m15:21:44.831533 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m15:21:44.940812 [info ] [MainThread]: Using profiles dir at C:\Users\tanma\.dbt
[0m15:21:44.942812 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m15:21:44.942812 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m15:21:44.986631 [info ] [MainThread]: Configuration:
[0m15:21:44.988631 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m15:21:44.989632 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:21:44.989632 [info ] [MainThread]: Required dependencies:
[0m15:21:44.990632 [debug] [MainThread]: Executing "git --help"
[0m15:21:45.029939 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:21:45.030937 [debug] [MainThread]: STDERR: "b''"
[0m15:21:45.031939 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:21:45.032939 [info ] [MainThread]: Connection test skipped since no profile was found
[0m15:21:45.033938 [info ] [MainThread]: [31m1 check failed:[0m
[0m15:21:45.034945 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "poc_demo2", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m15:21:45.037938 [debug] [MainThread]: Command `dbt debug` failed at 15:21:45.037938 after 0.23 seconds
[0m15:21:45.038939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD0362560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD33FD360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015CD33FCEB0>]}
[0m15:21:45.038939 [debug] [MainThread]: Flushing usage events
[0m16:13:18.400117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57daaf9450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57db1812a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57db181240>]}


============================== 16:13:18.411940 | bb5e9e53-b018-4cbc-a26f-1ed5637223d9 ==============================
[0m16:13:18.411940 [info ] [MainThread]: Running with dbt=1.6.9
[0m16:13:18.413193 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/mnt/d/home/Tanmay/Documents/poc_demo/logs', 'fail_fast': 'False', 'profiles_dir': '/home/tanmay/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:13:18.415082 [info ] [MainThread]: dbt version: 1.6.9
[0m16:13:18.416117 [info ] [MainThread]: python version: 3.10.12
[0m16:13:18.417206 [info ] [MainThread]: python path: /mnt/d/home/Tanmay/Documents/spark-error-fixed/sparkerrorlnx/bin/python3
[0m16:13:18.418278 [info ] [MainThread]: os info: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m16:13:18.419438 [info ] [MainThread]: Using profiles dir at /home/tanmay/.dbt
[0m16:13:18.420539 [info ] [MainThread]: Using profiles.yml file at /home/tanmay/.dbt/profiles.yml
[0m16:13:18.421681 [info ] [MainThread]: Using dbt_project.yml file at /mnt/d/home/Tanmay/Documents/poc_demo/dbt_project.yml
[0m16:13:18.468563 [info ] [MainThread]: Configuration:
[0m16:13:18.470267 [info ] [MainThread]:   profiles.yml file [[31mERROR not found[0m]
[0m16:13:18.471361 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:13:18.472379 [info ] [MainThread]: Required dependencies:
[0m16:13:18.473349 [debug] [MainThread]: Executing "git --help"
[0m16:13:18.494721 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:13:18.496615 [debug] [MainThread]: STDERR: "b''"
[0m16:13:18.498007 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:13:18.499554 [info ] [MainThread]: Connection test skipped since no profile was found
[0m16:13:18.501029 [info ] [MainThread]: [31m1 check failed:[0m
[0m16:13:18.502684 [info ] [MainThread]: dbt looked for a profiles.yml file in /home/tanmay/.dbt/profiles.yml, but did
not find one. For more information on configuring your profile, consult the
documentation:

https://docs.getdbt.com/docs/configure-your-profile


[0m16:13:18.504585 [debug] [MainThread]: Command `dbt debug` failed at 16:13:18.504384 after 0.12 seconds
[0m16:13:18.506279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57daaf9450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57db182f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57db182e00>]}
[0m16:13:18.508063 [debug] [MainThread]: Flushing usage events
[0m16:15:09.903035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683461420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683ba9270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683ba9210>]}


============================== 16:15:09.913579 | 13c271f1-4367-41b1-99ae-abfeaab12811 ==============================
[0m16:15:09.913579 [info ] [MainThread]: Running with dbt=1.6.9
[0m16:15:09.914740 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/mnt/d/home/Tanmay/Documents/poc_demo/logs', 'profiles_dir': '/home/tanmay/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:15:09.916566 [info ] [MainThread]: dbt version: 1.6.9
[0m16:15:09.917611 [info ] [MainThread]: python version: 3.10.12
[0m16:15:09.919013 [info ] [MainThread]: python path: /mnt/d/home/Tanmay/Documents/spark-error-fixed/sparkerrorlnx/bin/python3
[0m16:15:09.920710 [info ] [MainThread]: os info: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m16:15:09.922015 [info ] [MainThread]: Using profiles dir at /home/tanmay/.dbt
[0m16:15:09.923026 [info ] [MainThread]: Using profiles.yml file at /home/tanmay/.dbt/profiles.yml
[0m16:15:09.924084 [info ] [MainThread]: Using dbt_project.yml file at /mnt/d/home/Tanmay/Documents/poc_demo/dbt_project.yml
[0m16:15:09.977008 [info ] [MainThread]: Configuration:
[0m16:15:09.978433 [info ] [MainThread]:   profiles.yml file [[31mERROR not found[0m]
[0m16:15:09.979605 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:15:09.980650 [info ] [MainThread]: Required dependencies:
[0m16:15:09.981607 [debug] [MainThread]: Executing "git --help"
[0m16:15:09.987382 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:15:09.990071 [debug] [MainThread]: STDERR: "b''"
[0m16:15:09.991202 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:15:09.992300 [info ] [MainThread]: Connection test skipped since no profile was found
[0m16:15:09.993390 [info ] [MainThread]: [31m1 check failed:[0m
[0m16:15:09.994390 [info ] [MainThread]: dbt looked for a profiles.yml file in /home/tanmay/.dbt/profiles.yml, but did
not find one. For more information on configuring your profile, consult the
documentation:

https://docs.getdbt.com/docs/configure-your-profile


[0m16:15:09.995760 [debug] [MainThread]: Command `dbt debug` failed at 16:15:09.995616 after 0.11 seconds
[0m16:15:09.996971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683461420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683bab0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe683baae00>]}
[0m16:15:09.998133 [debug] [MainThread]: Flushing usage events
[0m16:17:51.576459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020880B0DAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208831D2CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208831D2A40>]}


============================== 16:17:51.580633 | 7d63c9e0-2db6-4cdf-8aed-c69d17ea36ca ==============================
[0m16:17:51.580633 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:17:51.580633 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\tanma\\.dbt', 'log_path': 'D:\\home\\Tanmay\\Documents\\poc_demo\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:17:51.580633 [info ] [MainThread]: dbt version: 1.5.2
[0m16:17:51.580633 [info ] [MainThread]: python version: 3.10.11
[0m16:17:51.580633 [info ] [MainThread]: python path: D:\home\Tanmay\Documents\iceberg_adapter\dbti\Scripts\python.exe
[0m16:17:51.580633 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m16:17:51.580633 [info ] [MainThread]: Using profiles.yml file at C:\Users\tanma\.dbt\profiles.yml
[0m16:17:51.580633 [info ] [MainThread]: Using dbt_project.yml file at D:\home\Tanmay\Documents\poc_demo\dbt_project.yml
[0m16:17:51.580633 [info ] [MainThread]: Configuration:
[0m16:17:51.735249 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m16:17:51.759248 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:17:51.759248 [info ] [MainThread]: Required dependencies:
[0m16:17:51.759248 [debug] [MainThread]: Executing "git --help"
[0m16:17:51.815360 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:17:51.815360 [debug] [MainThread]: STDERR: "b''"
[0m16:17:51.815360 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:17:51.815360 [info ] [MainThread]: Connection:
[0m16:17:51.815360 [info ] [MainThread]:   host: localhost
[0m16:17:51.815360 [info ] [MainThread]:   port: 10000
[0m16:17:51.815360 [info ] [MainThread]:   cluster: None
[0m16:17:51.815360 [info ] [MainThread]:   endpoint: None
[0m16:17:51.815360 [info ] [MainThread]:   schema: kdb
[0m16:17:51.823355 [info ] [MainThread]:   organization: 0
[0m16:17:51.823355 [info ] [MainThread]: Registered adapter: spark=1.5.0
[0m16:17:51.823355 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m16:17:51.823355 [debug] [MainThread]: Using spark connection "debug"
[0m16:17:51.823355 [debug] [MainThread]: On debug: select 1 as id
[0m16:17:51.823355 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:17:52.509966 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m16:17:52.512485 [debug] [MainThread]: SQL status: OK in 1.0 seconds
[0m16:17:52.513501 [debug] [MainThread]: On debug: Close
[0m16:17:52.528407 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m16:17:52.529979 [info ] [MainThread]: [32mAll checks passed![0m
[0m16:17:52.533003 [debug] [MainThread]: Command `dbt debug` succeeded at 16:17:52.533003 after 0.97 seconds
[0m16:17:52.533003 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m16:17:52.533003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208831D2980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088366E3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088366F610>]}
[0m16:17:52.533003 [debug] [MainThread]: Flushing usage events
[0m16:19:56.037807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b75d9510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b7c61360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b7c61300>]}


============================== 16:19:56.054240 | 8e1d451a-6886-4803-89ce-a7e52c438887 ==============================
[0m16:19:56.054240 [info ] [MainThread]: Running with dbt=1.6.9
[0m16:19:56.056020 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/tanmay/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/mnt/d/home/Tanmay/Documents/poc_demo/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:19:56.058756 [info ] [MainThread]: dbt version: 1.6.9
[0m16:19:56.060263 [info ] [MainThread]: python version: 3.10.12
[0m16:19:56.061587 [info ] [MainThread]: python path: /mnt/d/home/Tanmay/Documents/spark-error-fixed/sparkerrorlnx/bin/python3
[0m16:19:56.063102 [info ] [MainThread]: os info: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m16:19:56.064650 [info ] [MainThread]: Using profiles dir at /home/tanmay/.dbt
[0m16:19:56.066659 [info ] [MainThread]: Using profiles.yml file at /home/tanmay/.dbt/profiles.yml
[0m16:19:56.069092 [info ] [MainThread]: Using dbt_project.yml file at /mnt/d/home/Tanmay/Documents/poc_demo/dbt_project.yml
[0m16:19:56.117685 [info ] [MainThread]: Configuration:
[0m16:19:56.119287 [info ] [MainThread]:   profiles.yml file [[31mERROR not found[0m]
[0m16:19:56.120723 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:19:56.123672 [info ] [MainThread]: Required dependencies:
[0m16:19:56.126257 [debug] [MainThread]: Executing "git --help"
[0m16:19:56.131924 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:19:56.133270 [debug] [MainThread]: STDERR: "b''"
[0m16:19:56.134377 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:19:56.135573 [info ] [MainThread]: Connection test skipped since no profile was found
[0m16:19:56.136766 [info ] [MainThread]: [31m1 check failed:[0m
[0m16:19:56.138076 [info ] [MainThread]: dbt looked for a profiles.yml file in /home/tanmay/.dbt/profiles.yml, but did
not find one. For more information on configuring your profile, consult the
documentation:

https://docs.getdbt.com/docs/configure-your-profile


[0m16:19:56.140563 [debug] [MainThread]: Command `dbt debug` failed at 16:19:56.140268 after 0.12 seconds
[0m16:19:56.142721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b75d9510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b7c62f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10b7c62530>]}
[0m16:19:56.145284 [debug] [MainThread]: Flushing usage events
